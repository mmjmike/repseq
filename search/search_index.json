{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Repseq Library for Immune repertoire postanalysis","text":"<p>The purpose of this Python library is to make the postanalysis of TCR/BCR repertoire sequencing data easy and modular.</p>"},{"location":"#modules","title":"Modules","text":""},{"location":"#clonosets","title":"Clonosets","text":"<p>Manipulations with groups of clonosets:</p> <ul> <li>Creates clonosets DataFrames by searching for typical filenames in directories</li> <li>Detects clonoset formats</li> <li>Filters out non-target clonosets (same sample_id but low % of reads)</li> <li>Pools clonosets into one</li> </ul>"},{"location":"#stats","title":"Stats","text":"<ul> <li>Basic clonoset properties, like clone/read/umi counts functional or with OOF/Stops</li> <li>CDR3 amino acid properties: N-counts, physico-chemical properties, Kidera Factors</li> <li>Diversity statistics: observed diversity, (normalized) Shannon-Wiener, chao1</li> <li>Convergence estimate</li> <li>V/D/J/C-gene frequencies or VJ-combinations</li> <li>All calculations are parallelized</li> </ul>"},{"location":"#intersections","title":"Intersections","text":"<ul> <li>Finds parwise intersecting clonotypes between clonosets</li> <li>Intersection metrics: F, F2, D</li> <li>Count tables for clonotypes (similarity groups of clonotypes)</li> <li>Intersect clusters with clonosets</li> <li>TCRnet integration</li> </ul>"},{"location":"#clustering","title":"Clustering","text":"<p>This module implements different immune repertoire clustering analyses:</p> <ul> <li>basic clustering: using hamming distance similarity in CDR3 regions and/or same V/J-segments</li> <li>tcr-dist clustering: using distance metrics from tcrdist software</li> <li>ALICE: find expanded clonotypes by analyzing the probability of neighbour generation with OLGA algrorithm</li> <li>split clusters with community detection algorithms (Louvain, Leiden)</li> <li>easy and modular customisation of cluster analysis</li> <li>output graphs and metadata to Cytoscape format</li> </ul> <p>Graph representation with NetworkX library.</p>"},{"location":"#diffexp","title":"Diffexp","text":"<p>Finds differentially expressing clonotypes/clusters of clonotypes in CFSE-assays or similar experiments.</p>"},{"location":"#clone-filter","title":"Clone Filter","text":"<p>Easy filtering of clonosets by one Filter object, integrated with other analysis procedures. Filtering includes following features:</p> <ul> <li>counting by reads/UMIs/clonotypes</li> <li>use top N clonotypes (tails mixing included for randomly mixing clonotypes with same counts)</li> <li>randomly downsample to N UMIs/reads (you can specify seed, highly recommended for reproducibility)</li> <li>remove low count clonotypes</li> <li>filter out non-functional(OOF,Stop in CDR3)/functional clonotypes</li> <li>white/black list of clonotypes</li> <li>recount frequencies (by reads/UMIs)</li> <li>convert to vdjtools-like format</li> <li>combine (pool) clonotypes with similar features: CDR3/V/J</li> </ul>"},{"location":"#io-module","title":"IO module","text":"<ul> <li>reads and understands clonosets of following formats: MiXCR 3/4, vdjtools, Adaptive Biosciences</li> <li>tsv, .gz, .zip</li> </ul>"},{"location":"#mixcr-module","title":"MiXCR module","text":"<p>As MiXCR is the leading software for generating clonoset tables from raw FastQ files this module helps to run MiXCR 4.3+ batch analyses with SLURM queue manager. Easy accumulation of most sensible processing data from json-reports of MiXCR into one table.</p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#mixcr-analyze","title":"MiXCR analyze","text":"<pre><code># Will be made executable with markdown-exec (although there should be other options)\n# Not ready yet :(\nimport os\nimport pandas as pd\nfrom IPython.display import Image, display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>REPSEQ_PATH = '/home/epepeliaeva/soft/repseq/'\n\nimport sys\n\nsys.path.append(REPSEQ_PATH)\nfrom repseq import io as repseqio\nfrom repseq import mixcr as mx\nfrom repseq import slurm\nfrom repseq import clonosets as cl\nfrom repseq import stats\nfrom repseq import clone_filter as clf\nfrom repseq import intersections\nfrom repseq import clustering\nfrom repseq import logo\nfrom repseq import vdjtools\n</code></pre> <p><pre><code>MIXCR = \"/projects/cdr3_software/bin/mixcr\"\n\nWORKING_DIR = \"/projects/cdr3_common/repseq_demo/\"\nMIXCR_DIR = os.path.join(WORKING_DIR, \"mixcr\")\n\nRAW_DATA_DIR = \"/projects/cdr3_ngs/2023/11_room555_MiSeq_13112023/\"\n\nSAMPLE_LIST_FILENAME = os.path.join(WORKING_DIR, \"sample_table.csv\")\nTABLE_REPORT_FILENAME = os.path.join(WORKING_DIR, \"table_report.csv\")\n\nos.makedirs(WORKING_DIR, exist_ok=True)\n</code></pre> <pre><code>WORKING_DIR = \"/projects/cdr3_common/repseq_demo/\"\nMIXCR_DIR = os.path.join(WORKING_DIR, \"mixcr\")\n\nRAW_DATA_DIR = \"/projects/cdr3_ngs/2023/11_room555_MiSeq_13112023/\"\n\nSAMPLE_LIST_FILENAME = os.path.join(WORKING_DIR, \"sample_table.csv\")\nTABLE_REPORT_FILENAME = os.path.join(WORKING_DIR, \"table_report.csv\")\n\nsample_df = repseqio.read_yaml_metadata(RAW_DATA_DIR)[[\"sample_id\", \"R1\", \"R2\"]].query('sample_id.str.contains(\"Rev05\")')\n\nmx.mixcr4_analyze_batch(sample_df=sample_df, \n                        output_folder = MIXCR_DIR, \n                        command_template=None,\n                        mixcr_path=MIXCR, \n                        memory=32, \n                        time_estimate=1.5)\n</code></pre></p> <pre><code>slurm.check_slurm_progress(os.path.join(MIXCR_DIR, \"mixcr_analyze_slurm_batch.log\"), loop=True)\n</code></pre> <pre><code>mx.show_report_images(MIXCR_DIR)\n</code></pre> <pre><code>proc_table = mx.get_processing_table(MIXCR_DIR)\nproc_table.to_csv(TABLE_REPORT_FILENAME, index=False)\nprint(f\"Report table saved to: {TABLE_REPORT_FILENAME}\")\nproc_table\n</code></pre>"},{"location":"filter/","title":"Filter","text":"<p>Clonoset filter. May be used to filter clonosets:     - by clone functionality     - randomly downsample them to particular number of reads of UMIs     - take top clonotypes by size (number of reads of UMIs) with or without random mixing</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the filter. Will be displayed in print</p> <code>'default_filter'</code> <code>functionality</code> <code>str</code> <p>Possible values: - \"a\" - any (default). No clones are filtered out - \"f\" - only functional. Those, not having stop codons and      frame shifts in CDR3 regions, or having non-empty values in CDR3 amino-acid     sequence - \"n\" - only-nonfunctional - opposite to \"f\" - functional</p> <code>'a'</code> <code>downsample</code> <code>int</code> <p>the number of reads/UMIs to randomly downsample the clonoset to. default value 'None' - means not to apply downsampling</p> <code>None</code> <code>top</code> <code>int</code> <p>the number of top biggest by reads/UMIs clonotypes to take from the clonoset. default value 'None' - means not to apply top</p> <code>None</code> <code>by_umi</code> <code>bool</code> <p>default=False. Which column to take for clonotype count - reads or UMIs  (if UMI count column exists).</p> <code>False</code> <code>mix_tails</code> <code>bool</code> <p>default=False. Defines whether to randomly mix-up the order of clonotypes before sorting by size and taking the top clonotypes. Basically mix_tails=True mixes up  clonotypes with the same size in read or UMIs.</p> <code>False</code> <code>count_threshold</code> <code>int</code> <p>limits [0:100000], all clonotypes with count less than this value will be filtered out</p> <code>None</code> <code>seed</code> <code>any hashable type</code> <p>better to use int - seed for reproducibility of random events  (downsampling or top with mix-tails). Default=None.</p> <code>None</code> <code>unweight</code> <code>bool</code> <p>each clonotype counts (either reads or UMIs) are set to 1</p> <code>False</code> <code>recount_fractions</code> <code>bool</code> <p>if <code>True</code>, clonotype fractions are recalculated after filtration</p> <code>True</code> <code>white_list</code> <code>list of tuples</code> <p>If specified, only clonotypes matching those listed will be retained. Either <code>aa</code>, <code>aaV</code> or <code>aaVJ</code>  formats can be used to list clonotypes, e.g. [(\u201cCASSS..\u201d)], [(\u201cCASSS..\u201d, \u201cTRBV2\u201d)] or [(\u201cCASSS..\u201d, \u201cTRBV2\u201d, \u201cTRBJ1\u201d)].  It is applied before <code>black_list</code></p> <code>[]</code> <code>black_list</code> <code>list of tuples</code> <p>If specified, only clonotypes not listed will be retained. Either <code>aa</code>, <code>aaV</code> or <code>aaVJ</code>  formats can be used to list clonotypes, e.g. [(\u201cCASSS..\u201d)], [(\u201cCASSS..\u201d, \u201cTRBV2\u201d)] or [(\u201cCASSS..\u201d, \u201cTRBV2\u201d, \u201cTRBJ1\u201d)]</p> <code>[]</code> <code>pool_clonoset_by</code> <code>str</code> <p>possible values are [\"\", \"aa\", \"aaV\", \"aaVJ, \"nt\", \"ntV\", \"ntVJ\"]. Clones with identical parameters are merged,  keeping the largest one, while their counts are summed.</p> <code>''</code> <code>convert</code> <code>bool</code> <p>By default, columns are added to the clonotype set to convert  it to the VDJtools format, and the original columns are removed. If set to  <code>False</code>, all original columns are preserved.</p> <code>True</code> <code>ignore_small_clonosets</code> <code>bool</code> <p>If the top or downsample threshold exceeds the counts for a clonoset, the set is kept intact</p> <code>False</code> Source code in <code>repseq/clone_filter.py</code> <pre><code>class Filter:\n\n    \"\"\"\n    Clonoset filter.\n    May be used to filter clonosets:\n        - by clone functionality\n        - randomly downsample them to particular number of reads of UMIs\n        - take top clonotypes by size (number of reads of UMIs) with or without random mixing\n\n    Args:\n        name (str): the name of the filter. Will be displayed in print\n        functionality (str): Possible values:\n            - \"a\" - any (default). No clones are filtered out\n            - \"f\" - only functional. Those, not having stop codons and \n                frame shifts in CDR3 regions, or having non-empty values in CDR3 amino-acid\n                sequence\n            - \"n\" - only-nonfunctional - opposite to \"f\" - functional\n        downsample (int): the number of reads/UMIs to randomly downsample the clonoset to.\n            default value 'None' - means not to apply downsampling\n        top (int): the number of top biggest by reads/UMIs clonotypes to take from the clonoset.\n            default value 'None' - means not to apply top\n        by_umi (bool): default=False. Which column to take for clonotype count - reads or UMIs \n            (if UMI count column exists).\n        mix_tails (bool): default=False. Defines whether to randomly mix-up the order of clonotypes\n            before sorting by size and taking the top clonotypes. Basically mix_tails=True mixes up \n            clonotypes with the same size in read or UMIs.\n        count_threshold (int): limits [0:100000], all clonotypes with count less than this value will\n            be filtered out\n        seed (any hashable type): better to use int - seed for reproducibility of random events \n            (downsampling or top with mix-tails). Default=None.\n        unweight (bool): each clonotype counts (either reads or UMIs) are set to 1\n        recount_fractions (bool): if `True`, clonotype fractions are recalculated after filtration\n        white_list (list of tuples): If specified, only clonotypes matching those listed will be retained. Either `aa`, `aaV` or `aaVJ` \n            formats can be used to list clonotypes, e.g. [(\u201cCASSS..\u201d)], [(\u201cCASSS..\u201d, \u201cTRBV2\u201d)] or [(\u201cCASSS..\u201d, \u201cTRBV2\u201d, \u201cTRBJ1\u201d)]. \n            It is applied before `black_list`\n        black_list (list of tuples): If specified, only clonotypes not listed will be retained. Either `aa`, `aaV` or `aaVJ` \n            formats can be used to list clonotypes, e.g. [(\u201cCASSS..\u201d)], [(\u201cCASSS..\u201d, \u201cTRBV2\u201d)] or [(\u201cCASSS..\u201d, \u201cTRBV2\u201d, \u201cTRBJ1\u201d)]\n        pool_clonoset_by (str): possible values are [\"\", \"aa\", \"aaV\", \"aaVJ, \"nt\", \"ntV\", \"ntVJ\"]. Clones with identical parameters are merged, \n            keeping the largest one, while their counts are summed.\n        convert (bool): By default, columns are added to the clonotype set to convert \n            it to the VDJtools format, and the original columns are removed. If set to  `False`, all original columns are preserved.\n        ignore_small_clonosets (bool): If the top or downsample threshold exceeds the counts for a clonoset, the set is kept intact\n    \"\"\"\n\n    def __init__(self, name=\"default_filter\", functionality=\"a\", downsample=None,\n                 top=None, by_umi=False, mix_tails=False, count_threshold=None, \n                 unweight=False, seed=None, recount_fractions=True,\n                 white_list=[], black_list=[], pool_clonoset_by=\"\", convert=True, \n                 ignore_small_clonosets=False):\n        self.name = name\n        self.functionality = functionality\n        self.downsample_size = downsample\n        self.top = top\n        self.by_umi = by_umi\n        self.mix_tails = mix_tails\n        self.seed = seed\n        self.count_threshold = count_threshold\n        self.unweight = unweight\n        self.recount_fractions = recount_fractions\n        self.white_list = white_list\n        self.black_list = black_list\n        self.pool_by = pool_clonoset_by\n        self.convert = convert\n        self.ignore_small_clonosets = ignore_small_clonosets\n        self._check_input()\n\n    def spawn(self):\n        \"\"\"\n\n        Returns:\n            the copy of the filter. Necessary for parallel computing\n\n        \"\"\"\n        return Filter(name=self.name, functionality=self.functionality,\n                      downsample=self.downsample_size, top=self.top,\n                      by_umi=self.by_umi, mix_tails=self.mix_tails,\n                      count_threshold=self.count_threshold, seed=self.seed,\n                      unweight=self.unweight,\n                      recount_fractions=self.recount_fractions,\n                      white_list = self.white_list,\n                      black_list = self.black_list,\n                      ignore_small_clonosets=self.ignore_small_clonosets\n                      )\n\n    def apply(self, input_clonoset, colnames=None):\n        \"\"\"\n        Main method of the Filter object - application of it to a clonoset\n\n        Args:\n            input_clonoset (pd.DataFrame): clonoset in the form of Pandas DataFrame in\n                MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.\n            colnames (dict, optional): Dictionary of available specific column names.\n                Defaults to None - colnames imputed automatically.\n\n        Returns:\n            clonoset (pd.DataFrame): clonoset after converting to common (VDJtools-like)\n                format and applying functionality filtration and downsampling or taking top\n        \"\"\"\n\n        # copy clonoset for not changing the original one\n        clonoset = input_clonoset.copy()\n        if colnames is None:\n            colnames = get_column_names_from_clonoset(clonoset)\n\n        # create common columns: vdj-refPoints and VDJC-segments in common state\n        clonoset = self._make_common_columns(clonoset, colnames)\n        colnames = get_column_names_from_clonoset(clonoset)\n\n        # converting to common VDJtools-like format and obtaining new colnames\n        if self.convert:\n            clonoset = self._convert_clonoset(clonoset, colnames)\n            colnames = get_column_names_from_clonoset(clonoset)\n\n        # application of main filters\n        if self.functionality != \"a\":\n            clonoset = self._filter_by_functionality(clonoset, colnames)\n\n        clonoset = self._filter_by_count(clonoset, colnames)\n\n        clonoset = self._downsample(clonoset, colnames)\n        clonoset = self._get_top(clonoset, colnames)\n\n        if self.unweight:\n            clonoset = self._unweight(clonoset, colnames)\n        # the fraction columns need to be recounted after filtering, as they\n        # remain the same as in the original clonoset before filtration\n        if self.recount_fractions:\n            clonoset = self._recount_fractions_for_clonoset(clonoset, colnames)\n        if self.pool_by:\n            clonoset = self._pool_clonoset(clonoset, colnames)\n        if len(self.white_list) &gt; 0:\n            clonoset = self._filter_clonotypes(clonoset, list_type=\"white\")\n        if len(self.black_list) &gt; 0:\n            clonoset = self._filter_clonotypes(clonoset, list_type=\"black\")\n        return clonoset\n\n    def _convert_clonoset(self, clonoset, colnames):\n        # copy clonoset for not changing the original one\n        c_clonoset = clonoset.copy()\n\n        # basic column name in clonoset DF\n\n        count_column, fraction_column = decide_count_and_frac_columns(colnames, self.by_umi, suppress_warnings=True)\n\n        rename_dict = {count_column: \"count\",\n                       fraction_column: \"freq\",\n                       colnames[\"cdr3aa_column\"]: \"cdr3aa\",\n                       colnames[\"cdr3nt_column\"]: \"cdr3nt\",\n                       colnames[\"v_column\"]: \"v\",\n                       colnames[\"d_column\"]: \"d\",\n                       colnames[\"j_column\"]: \"j\"}\n        c_clonoset = c_clonoset.rename(columns=rename_dict)\n\n        result_columns = [\"count\", \"freq\"]\n        if \"cdr3nt\" in c_clonoset.columns:\n            result_columns.append(\"cdr3nt\")\n        result_columns += [\"cdr3aa\", \"v\"]\n        segment_borders_columns = [\"VEnd\", \"DStart\", \"DEnd\", \"JStart\"]\n\n\n        clonoset[\"cdr3aa\"] = clonoset[\"cdr3aa\"].fillna(\"\")\n        clonoset[\"cdr3nt\"] = clonoset[\"cdr3nt\"].fillna(\"\")\n\n\n        # In the case of MiXCR and Bioadaptive format the segment type columns\n        # usually show several segment variants with particular allele and score.\n        # Here we extract only the name of the best hit without allele ane score\n        c_clonoset[\"v\"] = c_clonoset[\"v\"].apply(lambda x: extract_segment(x))\n        if \"d\" in c_clonoset.columns:\n            c_clonoset[\"d\"] = c_clonoset[\"d\"].apply(lambda x: extract_segment(x))\n            result_columns.append(\"d\")\n        if \"j\" in c_clonoset.columns:\n            c_clonoset[\"j\"] = c_clonoset[\"j\"].apply(lambda x: extract_segment(x))\n            result_columns.append(\"j\")\n\n        # add the column for Constant segment if it exists in the original clonoset\n        if colnames[\"c_column\"] is not None:\n            c_clonoset = c_clonoset.rename(columns={colnames[\"c_column\"]: \"c\"})\n            c_clonoset[\"c\"] = c_clonoset[\"c\"].apply(lambda x: extract_segment(x))\n            result_columns += [\"c\"]\n\n        # obtain the borders of the segments within CDR3 region, if possible and add them to\n        # resulting clonoset\n        if \"refPoints\" in c_clonoset.columns:\n            c_clonoset[\"VEnd\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 11, minus=True))\n            c_clonoset[\"DStart\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 12, minus=False))\n            c_clonoset[\"DEnd\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 15, minus=True))\n            c_clonoset[\"JStart\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 16, minus=False))\n\n        result_columns += [col for col in segment_borders_columns if col in c_clonoset.columns]    \n\n        # save \"sample_id\" column if it is present in clonoset\n        if \"sample_id\" in c_clonoset.columns:\n            result_columns.append(\"sample_id\")\n        c_clonoset = c_clonoset.sort_values(by=\"count\", ascending=False).reset_index(drop=True)\n        return c_clonoset[result_columns]\n\n    def _make_common_columns(self, clonoset_in, colnames):\n        clonoset = clonoset_in.copy()\n\n        # treat refPoints\n        refpoints_columns = [\"VEnd\", \"DStart\", \"DEnd\", \"JStart\"]\n        if len(set(clonoset.columns).intersection(set(refpoints_columns))) &lt; 4: # check if not all the columns present\n            if \"refPoints\" in clonoset.columns: # if refPoints is present, create new columns\n                clonoset[\"VEnd\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 11, minus=True))\n                clonoset[\"DStart\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 12, minus=False))\n                clonoset[\"DEnd\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 15, minus=True))\n                clonoset[\"JStart\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 16, minus=False))\n\n        # treat v,d,j segments\n\n        # V\n        if \"v\" not in clonoset.columns:\n            if colnames[\"v_column\"] is not None:\n                clonoset[\"v\"] = clonoset[colnames[\"v_column\"]]\n        if \"v\" in clonoset.columns:\n            clonoset[\"v\"] = clonoset[\"v\"].apply(lambda x: extract_segment(x))\n\n        # D\n        if \"d\" not in clonoset.columns:\n            if colnames[\"d_column\"] is not None:\n                clonoset[\"d\"] = clonoset[colnames[\"d_column\"]]\n        if \"d\" in clonoset.columns:\n            clonoset[\"d\"] = clonoset[\"d\"].apply(lambda x: extract_segment(x))\n\n        # J\n        if \"j\" not in clonoset.columns:\n            if colnames[\"j_column\"] is not None:\n                clonoset[\"j\"] = clonoset[colnames[\"j_column\"]]\n        if \"j\" in clonoset.columns:\n            clonoset[\"j\"] = clonoset[\"j\"].apply(lambda x: extract_segment(x))\n\n        # C\n        if \"c\" not in clonoset.columns:\n            if colnames[\"c_column\"] is not None:\n                clonoset[\"c\"] = clonoset[colnames[\"c_column\"]]\n        if \"c\" in clonoset.columns:\n            clonoset[\"c\"] = clonoset[\"c\"].apply(lambda x: extract_segment(x))\n\n        if \"cdr3aa\" not in clonoset.columns:\n            if colnames[\"cdr3aa_column\"] is not None:\n                clonoset[\"cdr3aa\"] = clonoset[colnames[\"cdr3aa_column\"]].fillna(\"\")\n        if \"cdr3nt\" not in clonoset.columns:\n            if colnames[\"cdr3nt_column\"] is not None:\n                clonoset[\"cdr3nt\"] = clonoset[colnames[\"cdr3nt_column\"]].fillna(\"\")\n\n        return clonoset\n\n\n\n    def _unweight(self, clonoset_in, colnames):\n        clonoset = clonoset_in.copy()\n        clonoset[colnames[\"count_column\"]] = 1\n        return clonoset\n\n    def _recount_fractions_for_clonoset(self, clonoset_in, colnames):\n        if self.is_empty():\n            return clonoset_in\n        clonoset = clonoset_in.copy()\n\n        count_column = colnames[\"count_column\"]\n        fraction_column = colnames[\"fraction_column\"]\n        umi_column = colnames[\"umi_column\"]\n        umi_fraction_column = colnames[\"umi_fraction_column\"]\n\n        clonoset[fraction_column] = clonoset[count_column]/clonoset[count_column].sum()\n        if colnames[\"umi\"]:\n            clonoset[umi_fraction_column] = clonoset[umi_column]/clonoset[umi_column].sum()\n        return clonoset\n\n    def _filter_by_count(self, clonoset_in, colnames):\n        if self.count_threshold is None:\n            return clonoset_in\n        clonoset = clonoset_in.copy()\n        count_column = colnames[\"count_column\"]\n        clonoset = clonoset.loc[clonoset[count_column] &gt;= self.count_threshold]\n\n        return clonoset\n\n    def _check_input(self):\n\n        \"\"\"\n        Check if the object was created properly\n\n        Raises:\n            ValueError: in case of incorrect parameter values\n        \"\"\"\n        functionality_options = [\"a\", \"f\", \"n\"]\n        count_threshold_limits = [0, 100000]\n        if self.functionality not in functionality_options:\n            raise ValueError(f\"Incorrect value '{self.functionality}' for functionality. Possible values: {', '.join(functionality_options)}\")\n        if self.count_threshold is not None:\n            if not isinstance(self.count_threshold, int):\n                raise TypeError(\"Count threshold must be an 'int' or 'None'\")\n            if (self.count_threshold &lt; count_threshold_limits[0]\n                  or self.count_threshold &gt; count_threshold_limits[1]):\n                raise ValueError(f\"Incorrect value '{self.functionality}' for count_threshold. Possible values: {count_threshold_limits}\")                \n        if self.downsample_size is not None:\n            if not isinstance(self.downsample_size, int):\n                raise ValueError(f\"Incorrect value '{self.downsample_size}' for downsample_size. Only int or None possible\")\n            elif self.downsample_size &lt; 1:\n                raise ValueError(f\"Incorrect value '{self.downsample_size}' for downsample_size. Value too low\")\n        if self.top is not None:\n            if not isinstance(self.top, int):\n                raise ValueError(f\"Incorrect value '{self.top}' for top. Only int or None possible\")\n            elif self.top &lt; 1:\n                raise ValueError(f\"Incorrect value '{self.top}' for top. Value too low\")\n        if not isinstance(self.seed, Hashable):\n            raise ValueError(f\"Incorrect value '{self.seed}' for seed. Must be hashable\")\n        pool_by_options = [\"\", \"aa\", \"aaV\", \"aaVJ\", \"nt\", \"ntV\", \"ntVJ\"]\n        if self.pool_by not in pool_by_options:\n            raise ValueError(f\"Incorrect value '{self.pool_by}' for clonoset pool. Possible values: {', '.join(pool_by_options)}\")\n\n    def _downsample(self, clonoset_in, colnames):\n        \"\"\"\n        Downsample clonoset.\n\n        This function takes the total number of reads or UMIs of the clonoset.\n        Then randomly samples the downsample_size from 0 to this total number of reads/UMIs. \n        This random sample is mapped to the clonotype sizes and \n        the new downsampled clonoset is created\n        \"\"\"\n\n        if self.downsample_size is None:\n            return clonoset_in\n\n        clonoset = clonoset_in.copy()\n        count_column = colnames[\"count_column\"]\n        total_count = int(clonoset[count_column].sum())\n\n        # raise ValueError if UMI/read count is less then downsample_size\n\n        if total_count &lt; self.downsample_size:\n            if self.ignore_small_clonosets:\n                return clonoset\n            else:\n                raise ValueError(f\"total count {total_count} is less than downsample size {self.downsample_size}\")\n        elif total_count == self.downsample_size:\n            return clonoset\n\n        # set seed if given and take the sample of total_count\n        if self.seed is not None:\n            random.seed(self.seed)\n        sample = sorted(random.sample(range(total_count), self.downsample_size))\n\n        # map the sample to the clone counts in the clonoset\n        curr_sum = 0\n        i = 0\n        new_counts_dict = {}\n        for index,r in clonoset.iterrows():\n            curr_sum+=r[count_column]\n            new_count = 0\n            if i == self.downsample_size:\n                break\n            while(sample[i]&lt;curr_sum):\n                new_count+=1\n                i+=1\n                if i == self.downsample_size:\n                    break\n            if new_count &gt; 0:\n                new_counts_dict[index]=new_count\n\n        # filter clonoset for missed clones and set new clone counts\n        (indices,counts) = zip(*new_counts_dict.items())\n        clonoset = clonoset.loc[clonoset.index.isin(indices)]\n        clonoset[count_column] = counts    \n        return clonoset.reset_index(drop=True)\n\n    def _get_top(self, clonoset_in, colnames):\n        \"\"\"\n        Takes top N biggest clones from the clonoset.\n\n        Mix-tails is recommended for use, because the order of the clonotypes\n        with equal count may not be independent from their other properties.\n        This option mixes up the order of all clonotypes in clonoset and then\n        sorts them by count in decreasing order, so that clonotypes with the same\n        count not have completely random order. Also use seed option for reproducibility\n        of the results.\n\n        Raises:\n            ValueError: if clone count is less then required top\n        \"\"\"\n\n        if self.top is None:\n            return clonoset_in\n\n        clonoset = clonoset_in.copy()\n        count_column = colnames[\"count_column\"]\n\n        #shuffle the order of clonotypes if required\n        if self.seed is not None:\n            random.seed(self.seed)\n        if self.mix_tails:\n            index_order = random.sample(list(clonoset.index), len(clonoset))\n            clonoset = clonoset.iloc[index_order] \n            clonoset = clonoset.sort_values(by=count_column, ascending=False)\n\n        if self.top &gt; len(clonoset):\n            if self.ignore_small_clonosets:\n                return clonoset\n            else:\n                raise ValueError(f\"Warning! Clonoset size - {len(clonoset)} - is less than required top - {self.top}\")\n\n        # take top\n        if self.top &gt; 0:\n            clonoset=clonoset.iloc[:self.top]\n\n        return clonoset.reset_index(drop=True)\n\n    def _filter_by_functionality(self, clonoset_in, colnames):\n        clonoset = clonoset_in.copy()\n\n        if self.functionality == \"f\":\n            clonoset = filter_by_functionality(clonoset, colnames=colnames, functional=True)\n        if self.functionality == \"n\":\n            clonoset = filter_by_functionality(clonoset, colnames=colnames, functional=False)\n\n        return clonoset.reset_index(drop=True)\n\n    def __str__(self):\n\n        functionality = {\"a\": \"any\",\n                         \"f\": \"only functional clones (no frameshifts and Stops)\",\n                         \"n\": \"only non-functional\"}\n        output  = f\"Filter name:\\t{self.name}\\n\"\n        output += f\"Functionality:\\t{functionality[self.functionality]}\\n\"\n        random = False\n        change_size = False\n        if self.functionality != \"a\":\n            change_size = True\n        if isinstance(self.count_threshold, int):\n            output += f\"Count threshold:\\t{self.count_threshold}\\n\"\n            change_size = True\n        if isinstance(self.downsample_size, int):\n            output += f\"Downsample size:\\t{self.downsample_size}\\n\"\n            random = True\n            change_size = True\n        if isinstance(self.top, int):\n            output += f\"Take top:\\t{self.top}\\n\"\n            change_size = True\n            if self.mix_tails:\n                random = True\n        if self.unweight:\n            output += f\"Clone size unweighted (all clone counts = 1)\"\n\n        if change_size:\n            if self.by_umi:\n                output += f\"Count by:\\t UMI (if exist)\\n\"\n            else:\n                output += f\"Count by:\\t reads/counts\\n\"\n        if random:\n            if isinstance(self.seed, int):\n                output += f\"Seed for random:\\t{self.seed}\\n\"\n            else:\n                output += f\"Seed for random:\\tunset\\n\"\n                output += f\"Warning: filter contains random events.\\nTo obtain reproducible results, set seed in calcutations or manually (random.seed(some_int))\\nprior to applying the filter.\\nNote only seed that is set as this object parameter will work for mix_tails\\n\"\n\n        return output\n\n    def _pool_clonoset(self, clonoset_in, colnames):\n        # copy clonoset and sort by clone counts and reset index for order\n        clonoset = clonoset_in.copy().sort_values(by=colnames[\"count_column\"], ascending=False).reset_index(drop=True)\n\n        # create list of pool columns\n        aa, check_v, check_j = overlap_type_to_flags(self.pool_by)\n        columns_for_pool = []\n        if aa:\n            columns_for_pool.append(colnames[\"cdr3aa_column\"])\n        else:\n            columns_for_pool.append(colnames[\"cdr3nt_column\"])\n        if check_v:\n            columns_for_pool.append(colnames[\"v_column\"])\n        if check_j:\n            columns_for_pool.append(colnames[\"j_column\"])\n\n        # create column combining all pool columns\n        clonoset[\"pool_id\"] = clonoset.apply(lambda x: \"|\".join([x[colname] for colname in columns_for_pool]), axis=1)\n\n        indices_to_retain = []\n\n        for pool_id in clonoset[\"pool_id\"].unique():\n            pool_clonoset = clonoset.loc[clonoset[\"pool_id\"] == pool_id]\n\n            # select the clone with biggest count - it will represent pooled clonotypes by\n            # columns other that count and freq\n            top_index = pool_clonoset.index[0]\n            indices_to_retain.append(top_index)\n\n            # sum counts and fractions for pooled clonotypes\n            clonoset.loc[top_index,colnames[\"count_column\"]] = pool_clonoset[colnames[\"count_column\"]].sum()\n            clonoset.loc[top_index,colnames[\"fraction_column\"]] = pool_clonoset[colnames[\"fraction_column\"]].sum()\n\n        # retain only rows with representative clonotypes and remove technical column\n        clonoset = clonoset.loc[indices_to_retain].drop(columns=[\"pool_id\"])\n\n        return clonoset\n\n    def _filter_clonotypes(self, clonoset_in, list_type):\n        if list_type == \"white\":\n            clonotypes_list = self.white_list\n        elif list_type == \"black\":\n            clonotypes_list = self.black_list\n        else:\n            raise ValueError(\"list_type must be 'white' or 'black'\")\n\n        clonoset = clonoset_in.copy()\n\n        clonoset[\"filter_pass\"] = clonoset.apply(lambda x: self._compare_clonoset_list_row_with_clonotype(x, clonotypes_list), axis=1)\n        if list_type == \"white\":\n            clonoset = clonoset[clonoset[\"filter_pass\"]].drop(columns=[\"filter_pass\"]).reset_index(drop=True)\n        else:\n            clonoset = clonoset[~clonoset[\"filter_pass\"]].drop(columns=[\"filter_pass\"]).reset_index(drop=True)\n        return clonoset\n\n\n# def convert_clonoset_to_clonotype_filter_list(clonoset_df, overlap_type=\"aaVJ\"):\n#     clonotypes_list = []\n#     aa, include_v, include_j = intersections.overlap_type_to_flags(overlap_type)\n#     for i,r in clonoset_df.iterrows():\n#         clonotype = []\n#         if aa:\n#             clonotype.append(row[\"cdr3aa\"])\n#         else:\n#             clonotype.append(row[\"cdr3nt\"])\n#         if include_v:\n#             clonotype.append(row[\"v\"])\n#         if include_j:\n#             clonotype.append(row[\"j\"])\n\n#         clonotype = tuple(clonotype)\n#         clonotypes_list.append(clonotype)\n#     return clonotypes_list\n\n    def _compare_clonoset_row_with_clonotype(self, row, clonotype):\n        c_len = len(clonotype)\n        if c_len == 1:\n            if row[\"cdr3aa\"] == clonotype[0]:\n                return True\n        elif c_len == 2:\n            if row[\"cdr3aa\"] == clonotype[0] and row[\"v\"] == clonotype[1]:\n                return True\n        elif c_len == 3:\n            if row[\"cdr3aa\"] == clonotype[0] and row[\"v\"] == clonotype[1] and row[\"j\"] == clonotype[2]:\n                return True\n        else:\n            # need to write better explanation for error\n            raise ValueError(\"clonotypes must contain from 1 to 3 values\")\n\n        return False\n\n    def _compare_clonoset_list_row_with_clonotype(self, row, clonotypes_list):\n        for clonotype in clonotypes_list:\n            if self._compare_clonoset_row_with_clonotype(row, clonotype):\n                return True\n        return False\n\n    def is_empty(self):\n        return self.functionality == \"a\" and self.downsample_size is None and self.top is None and self.count_threshold is None and not self.unweight\n\n    def _repr_html_(self):\n        \"\"\"\n        function for printing the Filter properties to Jupyter output\n        \"\"\"\n\n        functionality = {\"a\": \"any\",\n                         \"f\": \"only functional clones (no frameshifts and Stops)\",\n                         \"n\": \"only non-functional\"}\n        output  = f\"&lt;p&gt;Filter name: {self.name}&lt;/p&gt;\"\n        output += f\"&lt;p&gt;Functionality:\\t{functionality[self.functionality]}&lt;/p&gt;\"\n        random = False\n        change_size = False\n        if self.functionality != \"a\":\n            change_size = True\n        if isinstance(self.count_threshold, int):\n            output += f\"&lt;p&gt;Count threshold: {self.count_threshold}&lt;/p&gt;\"\n            change_size = True\n        if isinstance(self.downsample_size, int):\n            output += f\"&lt;p&gt;Downsample size: {self.downsample_size}&lt;/p&gt;\"\n            random = True\n            change_size = True\n        if isinstance(self.top, int):\n            output += f\"&lt;p&gt;Take top: {self.top}&lt;/p&gt;\"\n            change_size = True\n            if self.mix_tails:\n                random = True\n        if self.unweight:\n            output += f\"&lt;p&gt;Clone size unweighted: (all clone counts = 1)&lt;/p&gt;\"\n\n        if change_size:\n            if self.by_umi:\n                output += f\"&lt;p&gt;Count by:  UMI (if exist)&lt;/p&gt;\"\n            else:\n                output += f\"&lt;p&gt;Count by: reads/counts&lt;/p&gt;\"\n        if random:\n            if isinstance(self.seed, int):\n                output += f\"&lt;p&gt;Seed for random: {self.seed}&lt;/p&gt;\"\n            else:\n                output += f\"&lt;p&gt;Seed for random:\\tunset&lt;/p&gt;\"\n                output += f\"&lt;p&gt;Warning: filter contains random events. To obtain reproducible results, set seed in calcutations or manually (random.seed(some_int)) prior to applying the filter. Note only seed that is set as this object parameter will work for mix_tails&lt;/p&gt;\"\n\n        return output\n</code></pre>"},{"location":"filter/#clone_filter.Filter.apply","title":"<code>apply(input_clonoset, colnames=None)</code>","text":"<p>Main method of the Filter object - application of it to a clonoset</p> <p>Parameters:</p> Name Type Description Default <code>input_clonoset</code> <code>DataFrame</code> <p>clonoset in the form of Pandas DataFrame in MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.</p> required <code>colnames</code> <code>dict</code> <p>Dictionary of available specific column names. Defaults to None - colnames imputed automatically.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>clonoset</code> <code>DataFrame</code> <p>clonoset after converting to common (VDJtools-like) format and applying functionality filtration and downsampling or taking top</p> Source code in <code>repseq/clone_filter.py</code> <pre><code>def apply(self, input_clonoset, colnames=None):\n    \"\"\"\n    Main method of the Filter object - application of it to a clonoset\n\n    Args:\n        input_clonoset (pd.DataFrame): clonoset in the form of Pandas DataFrame in\n            MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.\n        colnames (dict, optional): Dictionary of available specific column names.\n            Defaults to None - colnames imputed automatically.\n\n    Returns:\n        clonoset (pd.DataFrame): clonoset after converting to common (VDJtools-like)\n            format and applying functionality filtration and downsampling or taking top\n    \"\"\"\n\n    # copy clonoset for not changing the original one\n    clonoset = input_clonoset.copy()\n    if colnames is None:\n        colnames = get_column_names_from_clonoset(clonoset)\n\n    # create common columns: vdj-refPoints and VDJC-segments in common state\n    clonoset = self._make_common_columns(clonoset, colnames)\n    colnames = get_column_names_from_clonoset(clonoset)\n\n    # converting to common VDJtools-like format and obtaining new colnames\n    if self.convert:\n        clonoset = self._convert_clonoset(clonoset, colnames)\n        colnames = get_column_names_from_clonoset(clonoset)\n\n    # application of main filters\n    if self.functionality != \"a\":\n        clonoset = self._filter_by_functionality(clonoset, colnames)\n\n    clonoset = self._filter_by_count(clonoset, colnames)\n\n    clonoset = self._downsample(clonoset, colnames)\n    clonoset = self._get_top(clonoset, colnames)\n\n    if self.unweight:\n        clonoset = self._unweight(clonoset, colnames)\n    # the fraction columns need to be recounted after filtering, as they\n    # remain the same as in the original clonoset before filtration\n    if self.recount_fractions:\n        clonoset = self._recount_fractions_for_clonoset(clonoset, colnames)\n    if self.pool_by:\n        clonoset = self._pool_clonoset(clonoset, colnames)\n    if len(self.white_list) &gt; 0:\n        clonoset = self._filter_clonotypes(clonoset, list_type=\"white\")\n    if len(self.black_list) &gt; 0:\n        clonoset = self._filter_clonotypes(clonoset, list_type=\"black\")\n    return clonoset\n</code></pre>"},{"location":"filter/#clone_filter.Filter.spawn","title":"<code>spawn()</code>","text":"<p>Returns:</p> Type Description <p>the copy of the filter. Necessary for parallel computing</p> Source code in <code>repseq/clone_filter.py</code> <pre><code>def spawn(self):\n    \"\"\"\n\n    Returns:\n        the copy of the filter. Necessary for parallel computing\n\n    \"\"\"\n    return Filter(name=self.name, functionality=self.functionality,\n                  downsample=self.downsample_size, top=self.top,\n                  by_umi=self.by_umi, mix_tails=self.mix_tails,\n                  count_threshold=self.count_threshold, seed=self.seed,\n                  unweight=self.unweight,\n                  recount_fractions=self.recount_fractions,\n                  white_list = self.white_list,\n                  black_list = self.black_list,\n                  ignore_small_clonosets=self.ignore_small_clonosets\n                  )\n</code></pre>"},{"location":"functions/","title":"All functions","text":""},{"location":"functions/#clone_filter","title":"clone_filter","text":""},{"location":"functions/#clone_filter.Filter","title":"<code>Filter</code>","text":"<p>Clonoset filter. May be used to filter clonosets:     - by clone functionality     - randomly downsample them to particular number of reads of UMIs     - take top clonotypes by size (number of reads of UMIs) with or without random mixing</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the filter. Will be displayed in print</p> <code>'default_filter'</code> <code>functionality</code> <code>str</code> <p>Possible values: - \"a\" - any (default). No clones are filtered out - \"f\" - only functional. Those, not having stop codons and      frame shifts in CDR3 regions, or having non-empty values in CDR3 amino-acid     sequence - \"n\" - only-nonfunctional - opposite to \"f\" - functional</p> <code>'a'</code> <code>downsample</code> <code>int</code> <p>the number of reads/UMIs to randomly downsample the clonoset to. default value 'None' - means not to apply downsampling</p> <code>None</code> <code>top</code> <code>int</code> <p>the number of top biggest by reads/UMIs clonotypes to take from the clonoset. default value 'None' - means not to apply top</p> <code>None</code> <code>by_umi</code> <code>bool</code> <p>default=False. Which column to take for clonotype count - reads or UMIs  (if UMI count column exists).</p> <code>False</code> <code>mix_tails</code> <code>bool</code> <p>default=False. Defines whether to randomly mix-up the order of clonotypes before sorting by size and taking the top clonotypes. Basically mix_tails=True mixes up  clonotypes with the same size in read or UMIs.</p> <code>False</code> <code>count_threshold</code> <code>int</code> <p>limits [0:100000], all clonotypes with count less than this value will be filtered out</p> <code>None</code> <code>seed</code> <code>any hashable type</code> <p>better to use int - seed for reproducibility of random events  (downsampling or top with mix-tails). Default=None.</p> <code>None</code> <code>unweight</code> <code>bool</code> <p>each clonotype counts (either reads or UMIs) are set to 1</p> <code>False</code> <code>recount_fractions</code> <code>bool</code> <p>if <code>True</code>, clonotype fractions are recalculated after filtration</p> <code>True</code> <code>white_list</code> <code>list of tuples</code> <p>If specified, only clonotypes matching those listed will be retained. Either <code>aa</code>, <code>aaV</code> or <code>aaVJ</code>  formats can be used to list clonotypes, e.g. [(\u201cCASSS..\u201d)], [(\u201cCASSS..\u201d, \u201cTRBV2\u201d)] or [(\u201cCASSS..\u201d, \u201cTRBV2\u201d, \u201cTRBJ1\u201d)].  It is applied before <code>black_list</code></p> <code>[]</code> <code>black_list</code> <code>list of tuples</code> <p>If specified, only clonotypes not listed will be retained. Either <code>aa</code>, <code>aaV</code> or <code>aaVJ</code>  formats can be used to list clonotypes, e.g. [(\u201cCASSS..\u201d)], [(\u201cCASSS..\u201d, \u201cTRBV2\u201d)] or [(\u201cCASSS..\u201d, \u201cTRBV2\u201d, \u201cTRBJ1\u201d)]</p> <code>[]</code> <code>pool_clonoset_by</code> <code>str</code> <p>possible values are [\"\", \"aa\", \"aaV\", \"aaVJ, \"nt\", \"ntV\", \"ntVJ\"]. Clones with identical parameters are merged,  keeping the largest one, while their counts are summed.</p> <code>''</code> <code>convert</code> <code>bool</code> <p>By default, columns are added to the clonotype set to convert  it to the VDJtools format, and the original columns are removed. If set to  <code>False</code>, all original columns are preserved.</p> <code>True</code> <code>ignore_small_clonosets</code> <code>bool</code> <p>If the top or downsample threshold exceeds the counts for a clonoset, the set is kept intact</p> <code>False</code> Source code in <code>repseq/clone_filter.py</code> <pre><code>class Filter:\n\n    \"\"\"\n    Clonoset filter.\n    May be used to filter clonosets:\n        - by clone functionality\n        - randomly downsample them to particular number of reads of UMIs\n        - take top clonotypes by size (number of reads of UMIs) with or without random mixing\n\n    Args:\n        name (str): the name of the filter. Will be displayed in print\n        functionality (str): Possible values:\n            - \"a\" - any (default). No clones are filtered out\n            - \"f\" - only functional. Those, not having stop codons and \n                frame shifts in CDR3 regions, or having non-empty values in CDR3 amino-acid\n                sequence\n            - \"n\" - only-nonfunctional - opposite to \"f\" - functional\n        downsample (int): the number of reads/UMIs to randomly downsample the clonoset to.\n            default value 'None' - means not to apply downsampling\n        top (int): the number of top biggest by reads/UMIs clonotypes to take from the clonoset.\n            default value 'None' - means not to apply top\n        by_umi (bool): default=False. Which column to take for clonotype count - reads or UMIs \n            (if UMI count column exists).\n        mix_tails (bool): default=False. Defines whether to randomly mix-up the order of clonotypes\n            before sorting by size and taking the top clonotypes. Basically mix_tails=True mixes up \n            clonotypes with the same size in read or UMIs.\n        count_threshold (int): limits [0:100000], all clonotypes with count less than this value will\n            be filtered out\n        seed (any hashable type): better to use int - seed for reproducibility of random events \n            (downsampling or top with mix-tails). Default=None.\n        unweight (bool): each clonotype counts (either reads or UMIs) are set to 1\n        recount_fractions (bool): if `True`, clonotype fractions are recalculated after filtration\n        white_list (list of tuples): If specified, only clonotypes matching those listed will be retained. Either `aa`, `aaV` or `aaVJ` \n            formats can be used to list clonotypes, e.g. [(\u201cCASSS..\u201d)], [(\u201cCASSS..\u201d, \u201cTRBV2\u201d)] or [(\u201cCASSS..\u201d, \u201cTRBV2\u201d, \u201cTRBJ1\u201d)]. \n            It is applied before `black_list`\n        black_list (list of tuples): If specified, only clonotypes not listed will be retained. Either `aa`, `aaV` or `aaVJ` \n            formats can be used to list clonotypes, e.g. [(\u201cCASSS..\u201d)], [(\u201cCASSS..\u201d, \u201cTRBV2\u201d)] or [(\u201cCASSS..\u201d, \u201cTRBV2\u201d, \u201cTRBJ1\u201d)]\n        pool_clonoset_by (str): possible values are [\"\", \"aa\", \"aaV\", \"aaVJ, \"nt\", \"ntV\", \"ntVJ\"]. Clones with identical parameters are merged, \n            keeping the largest one, while their counts are summed.\n        convert (bool): By default, columns are added to the clonotype set to convert \n            it to the VDJtools format, and the original columns are removed. If set to  `False`, all original columns are preserved.\n        ignore_small_clonosets (bool): If the top or downsample threshold exceeds the counts for a clonoset, the set is kept intact\n    \"\"\"\n\n    def __init__(self, name=\"default_filter\", functionality=\"a\", downsample=None,\n                 top=None, by_umi=False, mix_tails=False, count_threshold=None, \n                 unweight=False, seed=None, recount_fractions=True,\n                 white_list=[], black_list=[], pool_clonoset_by=\"\", convert=True, \n                 ignore_small_clonosets=False):\n        self.name = name\n        self.functionality = functionality\n        self.downsample_size = downsample\n        self.top = top\n        self.by_umi = by_umi\n        self.mix_tails = mix_tails\n        self.seed = seed\n        self.count_threshold = count_threshold\n        self.unweight = unweight\n        self.recount_fractions = recount_fractions\n        self.white_list = white_list\n        self.black_list = black_list\n        self.pool_by = pool_clonoset_by\n        self.convert = convert\n        self.ignore_small_clonosets = ignore_small_clonosets\n        self._check_input()\n\n    def spawn(self):\n        \"\"\"\n\n        Returns:\n            the copy of the filter. Necessary for parallel computing\n\n        \"\"\"\n        return Filter(name=self.name, functionality=self.functionality,\n                      downsample=self.downsample_size, top=self.top,\n                      by_umi=self.by_umi, mix_tails=self.mix_tails,\n                      count_threshold=self.count_threshold, seed=self.seed,\n                      unweight=self.unweight,\n                      recount_fractions=self.recount_fractions,\n                      white_list = self.white_list,\n                      black_list = self.black_list,\n                      ignore_small_clonosets=self.ignore_small_clonosets\n                      )\n\n    def apply(self, input_clonoset, colnames=None):\n        \"\"\"\n        Main method of the Filter object - application of it to a clonoset\n\n        Args:\n            input_clonoset (pd.DataFrame): clonoset in the form of Pandas DataFrame in\n                MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.\n            colnames (dict, optional): Dictionary of available specific column names.\n                Defaults to None - colnames imputed automatically.\n\n        Returns:\n            clonoset (pd.DataFrame): clonoset after converting to common (VDJtools-like)\n                format and applying functionality filtration and downsampling or taking top\n        \"\"\"\n\n        # copy clonoset for not changing the original one\n        clonoset = input_clonoset.copy()\n        if colnames is None:\n            colnames = get_column_names_from_clonoset(clonoset)\n\n        # create common columns: vdj-refPoints and VDJC-segments in common state\n        clonoset = self._make_common_columns(clonoset, colnames)\n        colnames = get_column_names_from_clonoset(clonoset)\n\n        # converting to common VDJtools-like format and obtaining new colnames\n        if self.convert:\n            clonoset = self._convert_clonoset(clonoset, colnames)\n            colnames = get_column_names_from_clonoset(clonoset)\n\n        # application of main filters\n        if self.functionality != \"a\":\n            clonoset = self._filter_by_functionality(clonoset, colnames)\n\n        clonoset = self._filter_by_count(clonoset, colnames)\n\n        clonoset = self._downsample(clonoset, colnames)\n        clonoset = self._get_top(clonoset, colnames)\n\n        if self.unweight:\n            clonoset = self._unweight(clonoset, colnames)\n        # the fraction columns need to be recounted after filtering, as they\n        # remain the same as in the original clonoset before filtration\n        if self.recount_fractions:\n            clonoset = self._recount_fractions_for_clonoset(clonoset, colnames)\n        if self.pool_by:\n            clonoset = self._pool_clonoset(clonoset, colnames)\n        if len(self.white_list) &gt; 0:\n            clonoset = self._filter_clonotypes(clonoset, list_type=\"white\")\n        if len(self.black_list) &gt; 0:\n            clonoset = self._filter_clonotypes(clonoset, list_type=\"black\")\n        return clonoset\n\n    def _convert_clonoset(self, clonoset, colnames):\n        # copy clonoset for not changing the original one\n        c_clonoset = clonoset.copy()\n\n        # basic column name in clonoset DF\n\n        count_column, fraction_column = decide_count_and_frac_columns(colnames, self.by_umi, suppress_warnings=True)\n\n        rename_dict = {count_column: \"count\",\n                       fraction_column: \"freq\",\n                       colnames[\"cdr3aa_column\"]: \"cdr3aa\",\n                       colnames[\"cdr3nt_column\"]: \"cdr3nt\",\n                       colnames[\"v_column\"]: \"v\",\n                       colnames[\"d_column\"]: \"d\",\n                       colnames[\"j_column\"]: \"j\"}\n        c_clonoset = c_clonoset.rename(columns=rename_dict)\n\n        result_columns = [\"count\", \"freq\"]\n        if \"cdr3nt\" in c_clonoset.columns:\n            result_columns.append(\"cdr3nt\")\n        result_columns += [\"cdr3aa\", \"v\"]\n        segment_borders_columns = [\"VEnd\", \"DStart\", \"DEnd\", \"JStart\"]\n\n\n        clonoset[\"cdr3aa\"] = clonoset[\"cdr3aa\"].fillna(\"\")\n        clonoset[\"cdr3nt\"] = clonoset[\"cdr3nt\"].fillna(\"\")\n\n\n        # In the case of MiXCR and Bioadaptive format the segment type columns\n        # usually show several segment variants with particular allele and score.\n        # Here we extract only the name of the best hit without allele ane score\n        c_clonoset[\"v\"] = c_clonoset[\"v\"].apply(lambda x: extract_segment(x))\n        if \"d\" in c_clonoset.columns:\n            c_clonoset[\"d\"] = c_clonoset[\"d\"].apply(lambda x: extract_segment(x))\n            result_columns.append(\"d\")\n        if \"j\" in c_clonoset.columns:\n            c_clonoset[\"j\"] = c_clonoset[\"j\"].apply(lambda x: extract_segment(x))\n            result_columns.append(\"j\")\n\n        # add the column for Constant segment if it exists in the original clonoset\n        if colnames[\"c_column\"] is not None:\n            c_clonoset = c_clonoset.rename(columns={colnames[\"c_column\"]: \"c\"})\n            c_clonoset[\"c\"] = c_clonoset[\"c\"].apply(lambda x: extract_segment(x))\n            result_columns += [\"c\"]\n\n        # obtain the borders of the segments within CDR3 region, if possible and add them to\n        # resulting clonoset\n        if \"refPoints\" in c_clonoset.columns:\n            c_clonoset[\"VEnd\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 11, minus=True))\n            c_clonoset[\"DStart\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 12, minus=False))\n            c_clonoset[\"DEnd\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 15, minus=True))\n            c_clonoset[\"JStart\"] = c_clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 16, minus=False))\n\n        result_columns += [col for col in segment_borders_columns if col in c_clonoset.columns]    \n\n        # save \"sample_id\" column if it is present in clonoset\n        if \"sample_id\" in c_clonoset.columns:\n            result_columns.append(\"sample_id\")\n        c_clonoset = c_clonoset.sort_values(by=\"count\", ascending=False).reset_index(drop=True)\n        return c_clonoset[result_columns]\n\n    def _make_common_columns(self, clonoset_in, colnames):\n        clonoset = clonoset_in.copy()\n\n        # treat refPoints\n        refpoints_columns = [\"VEnd\", \"DStart\", \"DEnd\", \"JStart\"]\n        if len(set(clonoset.columns).intersection(set(refpoints_columns))) &lt; 4: # check if not all the columns present\n            if \"refPoints\" in clonoset.columns: # if refPoints is present, create new columns\n                clonoset[\"VEnd\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 11, minus=True))\n                clonoset[\"DStart\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 12, minus=False))\n                clonoset[\"DEnd\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 15, minus=True))\n                clonoset[\"JStart\"] = clonoset[\"refPoints\"].apply(lambda x: extract_refpoint_position(x, 16, minus=False))\n\n        # treat v,d,j segments\n\n        # V\n        if \"v\" not in clonoset.columns:\n            if colnames[\"v_column\"] is not None:\n                clonoset[\"v\"] = clonoset[colnames[\"v_column\"]]\n        if \"v\" in clonoset.columns:\n            clonoset[\"v\"] = clonoset[\"v\"].apply(lambda x: extract_segment(x))\n\n        # D\n        if \"d\" not in clonoset.columns:\n            if colnames[\"d_column\"] is not None:\n                clonoset[\"d\"] = clonoset[colnames[\"d_column\"]]\n        if \"d\" in clonoset.columns:\n            clonoset[\"d\"] = clonoset[\"d\"].apply(lambda x: extract_segment(x))\n\n        # J\n        if \"j\" not in clonoset.columns:\n            if colnames[\"j_column\"] is not None:\n                clonoset[\"j\"] = clonoset[colnames[\"j_column\"]]\n        if \"j\" in clonoset.columns:\n            clonoset[\"j\"] = clonoset[\"j\"].apply(lambda x: extract_segment(x))\n\n        # C\n        if \"c\" not in clonoset.columns:\n            if colnames[\"c_column\"] is not None:\n                clonoset[\"c\"] = clonoset[colnames[\"c_column\"]]\n        if \"c\" in clonoset.columns:\n            clonoset[\"c\"] = clonoset[\"c\"].apply(lambda x: extract_segment(x))\n\n        if \"cdr3aa\" not in clonoset.columns:\n            if colnames[\"cdr3aa_column\"] is not None:\n                clonoset[\"cdr3aa\"] = clonoset[colnames[\"cdr3aa_column\"]].fillna(\"\")\n        if \"cdr3nt\" not in clonoset.columns:\n            if colnames[\"cdr3nt_column\"] is not None:\n                clonoset[\"cdr3nt\"] = clonoset[colnames[\"cdr3nt_column\"]].fillna(\"\")\n\n        return clonoset\n\n\n\n    def _unweight(self, clonoset_in, colnames):\n        clonoset = clonoset_in.copy()\n        clonoset[colnames[\"count_column\"]] = 1\n        return clonoset\n\n    def _recount_fractions_for_clonoset(self, clonoset_in, colnames):\n        if self.is_empty():\n            return clonoset_in\n        clonoset = clonoset_in.copy()\n\n        count_column = colnames[\"count_column\"]\n        fraction_column = colnames[\"fraction_column\"]\n        umi_column = colnames[\"umi_column\"]\n        umi_fraction_column = colnames[\"umi_fraction_column\"]\n\n        clonoset[fraction_column] = clonoset[count_column]/clonoset[count_column].sum()\n        if colnames[\"umi\"]:\n            clonoset[umi_fraction_column] = clonoset[umi_column]/clonoset[umi_column].sum()\n        return clonoset\n\n    def _filter_by_count(self, clonoset_in, colnames):\n        if self.count_threshold is None:\n            return clonoset_in\n        clonoset = clonoset_in.copy()\n        count_column = colnames[\"count_column\"]\n        clonoset = clonoset.loc[clonoset[count_column] &gt;= self.count_threshold]\n\n        return clonoset\n\n    def _check_input(self):\n\n        \"\"\"\n        Check if the object was created properly\n\n        Raises:\n            ValueError: in case of incorrect parameter values\n        \"\"\"\n        functionality_options = [\"a\", \"f\", \"n\"]\n        count_threshold_limits = [0, 100000]\n        if self.functionality not in functionality_options:\n            raise ValueError(f\"Incorrect value '{self.functionality}' for functionality. Possible values: {', '.join(functionality_options)}\")\n        if self.count_threshold is not None:\n            if not isinstance(self.count_threshold, int):\n                raise TypeError(\"Count threshold must be an 'int' or 'None'\")\n            if (self.count_threshold &lt; count_threshold_limits[0]\n                  or self.count_threshold &gt; count_threshold_limits[1]):\n                raise ValueError(f\"Incorrect value '{self.functionality}' for count_threshold. Possible values: {count_threshold_limits}\")                \n        if self.downsample_size is not None:\n            if not isinstance(self.downsample_size, int):\n                raise ValueError(f\"Incorrect value '{self.downsample_size}' for downsample_size. Only int or None possible\")\n            elif self.downsample_size &lt; 1:\n                raise ValueError(f\"Incorrect value '{self.downsample_size}' for downsample_size. Value too low\")\n        if self.top is not None:\n            if not isinstance(self.top, int):\n                raise ValueError(f\"Incorrect value '{self.top}' for top. Only int or None possible\")\n            elif self.top &lt; 1:\n                raise ValueError(f\"Incorrect value '{self.top}' for top. Value too low\")\n        if not isinstance(self.seed, Hashable):\n            raise ValueError(f\"Incorrect value '{self.seed}' for seed. Must be hashable\")\n        pool_by_options = [\"\", \"aa\", \"aaV\", \"aaVJ\", \"nt\", \"ntV\", \"ntVJ\"]\n        if self.pool_by not in pool_by_options:\n            raise ValueError(f\"Incorrect value '{self.pool_by}' for clonoset pool. Possible values: {', '.join(pool_by_options)}\")\n\n    def _downsample(self, clonoset_in, colnames):\n        \"\"\"\n        Downsample clonoset.\n\n        This function takes the total number of reads or UMIs of the clonoset.\n        Then randomly samples the downsample_size from 0 to this total number of reads/UMIs. \n        This random sample is mapped to the clonotype sizes and \n        the new downsampled clonoset is created\n        \"\"\"\n\n        if self.downsample_size is None:\n            return clonoset_in\n\n        clonoset = clonoset_in.copy()\n        count_column = colnames[\"count_column\"]\n        total_count = int(clonoset[count_column].sum())\n\n        # raise ValueError if UMI/read count is less then downsample_size\n\n        if total_count &lt; self.downsample_size:\n            if self.ignore_small_clonosets:\n                return clonoset\n            else:\n                raise ValueError(f\"total count {total_count} is less than downsample size {self.downsample_size}\")\n        elif total_count == self.downsample_size:\n            return clonoset\n\n        # set seed if given and take the sample of total_count\n        if self.seed is not None:\n            random.seed(self.seed)\n        sample = sorted(random.sample(range(total_count), self.downsample_size))\n\n        # map the sample to the clone counts in the clonoset\n        curr_sum = 0\n        i = 0\n        new_counts_dict = {}\n        for index,r in clonoset.iterrows():\n            curr_sum+=r[count_column]\n            new_count = 0\n            if i == self.downsample_size:\n                break\n            while(sample[i]&lt;curr_sum):\n                new_count+=1\n                i+=1\n                if i == self.downsample_size:\n                    break\n            if new_count &gt; 0:\n                new_counts_dict[index]=new_count\n\n        # filter clonoset for missed clones and set new clone counts\n        (indices,counts) = zip(*new_counts_dict.items())\n        clonoset = clonoset.loc[clonoset.index.isin(indices)]\n        clonoset[count_column] = counts    \n        return clonoset.reset_index(drop=True)\n\n    def _get_top(self, clonoset_in, colnames):\n        \"\"\"\n        Takes top N biggest clones from the clonoset.\n\n        Mix-tails is recommended for use, because the order of the clonotypes\n        with equal count may not be independent from their other properties.\n        This option mixes up the order of all clonotypes in clonoset and then\n        sorts them by count in decreasing order, so that clonotypes with the same\n        count not have completely random order. Also use seed option for reproducibility\n        of the results.\n\n        Raises:\n            ValueError: if clone count is less then required top\n        \"\"\"\n\n        if self.top is None:\n            return clonoset_in\n\n        clonoset = clonoset_in.copy()\n        count_column = colnames[\"count_column\"]\n\n        #shuffle the order of clonotypes if required\n        if self.seed is not None:\n            random.seed(self.seed)\n        if self.mix_tails:\n            index_order = random.sample(list(clonoset.index), len(clonoset))\n            clonoset = clonoset.iloc[index_order] \n            clonoset = clonoset.sort_values(by=count_column, ascending=False)\n\n        if self.top &gt; len(clonoset):\n            if self.ignore_small_clonosets:\n                return clonoset\n            else:\n                raise ValueError(f\"Warning! Clonoset size - {len(clonoset)} - is less than required top - {self.top}\")\n\n        # take top\n        if self.top &gt; 0:\n            clonoset=clonoset.iloc[:self.top]\n\n        return clonoset.reset_index(drop=True)\n\n    def _filter_by_functionality(self, clonoset_in, colnames):\n        clonoset = clonoset_in.copy()\n\n        if self.functionality == \"f\":\n            clonoset = filter_by_functionality(clonoset, colnames=colnames, functional=True)\n        if self.functionality == \"n\":\n            clonoset = filter_by_functionality(clonoset, colnames=colnames, functional=False)\n\n        return clonoset.reset_index(drop=True)\n\n    def __str__(self):\n\n        functionality = {\"a\": \"any\",\n                         \"f\": \"only functional clones (no frameshifts and Stops)\",\n                         \"n\": \"only non-functional\"}\n        output  = f\"Filter name:\\t{self.name}\\n\"\n        output += f\"Functionality:\\t{functionality[self.functionality]}\\n\"\n        random = False\n        change_size = False\n        if self.functionality != \"a\":\n            change_size = True\n        if isinstance(self.count_threshold, int):\n            output += f\"Count threshold:\\t{self.count_threshold}\\n\"\n            change_size = True\n        if isinstance(self.downsample_size, int):\n            output += f\"Downsample size:\\t{self.downsample_size}\\n\"\n            random = True\n            change_size = True\n        if isinstance(self.top, int):\n            output += f\"Take top:\\t{self.top}\\n\"\n            change_size = True\n            if self.mix_tails:\n                random = True\n        if self.unweight:\n            output += f\"Clone size unweighted (all clone counts = 1)\"\n\n        if change_size:\n            if self.by_umi:\n                output += f\"Count by:\\t UMI (if exist)\\n\"\n            else:\n                output += f\"Count by:\\t reads/counts\\n\"\n        if random:\n            if isinstance(self.seed, int):\n                output += f\"Seed for random:\\t{self.seed}\\n\"\n            else:\n                output += f\"Seed for random:\\tunset\\n\"\n                output += f\"Warning: filter contains random events.\\nTo obtain reproducible results, set seed in calcutations or manually (random.seed(some_int))\\nprior to applying the filter.\\nNote only seed that is set as this object parameter will work for mix_tails\\n\"\n\n        return output\n\n    def _pool_clonoset(self, clonoset_in, colnames):\n        # copy clonoset and sort by clone counts and reset index for order\n        clonoset = clonoset_in.copy().sort_values(by=colnames[\"count_column\"], ascending=False).reset_index(drop=True)\n\n        # create list of pool columns\n        aa, check_v, check_j = overlap_type_to_flags(self.pool_by)\n        columns_for_pool = []\n        if aa:\n            columns_for_pool.append(colnames[\"cdr3aa_column\"])\n        else:\n            columns_for_pool.append(colnames[\"cdr3nt_column\"])\n        if check_v:\n            columns_for_pool.append(colnames[\"v_column\"])\n        if check_j:\n            columns_for_pool.append(colnames[\"j_column\"])\n\n        # create column combining all pool columns\n        clonoset[\"pool_id\"] = clonoset.apply(lambda x: \"|\".join([x[colname] for colname in columns_for_pool]), axis=1)\n\n        indices_to_retain = []\n\n        for pool_id in clonoset[\"pool_id\"].unique():\n            pool_clonoset = clonoset.loc[clonoset[\"pool_id\"] == pool_id]\n\n            # select the clone with biggest count - it will represent pooled clonotypes by\n            # columns other that count and freq\n            top_index = pool_clonoset.index[0]\n            indices_to_retain.append(top_index)\n\n            # sum counts and fractions for pooled clonotypes\n            clonoset.loc[top_index,colnames[\"count_column\"]] = pool_clonoset[colnames[\"count_column\"]].sum()\n            clonoset.loc[top_index,colnames[\"fraction_column\"]] = pool_clonoset[colnames[\"fraction_column\"]].sum()\n\n        # retain only rows with representative clonotypes and remove technical column\n        clonoset = clonoset.loc[indices_to_retain].drop(columns=[\"pool_id\"])\n\n        return clonoset\n\n    def _filter_clonotypes(self, clonoset_in, list_type):\n        if list_type == \"white\":\n            clonotypes_list = self.white_list\n        elif list_type == \"black\":\n            clonotypes_list = self.black_list\n        else:\n            raise ValueError(\"list_type must be 'white' or 'black'\")\n\n        clonoset = clonoset_in.copy()\n\n        clonoset[\"filter_pass\"] = clonoset.apply(lambda x: self._compare_clonoset_list_row_with_clonotype(x, clonotypes_list), axis=1)\n        if list_type == \"white\":\n            clonoset = clonoset[clonoset[\"filter_pass\"]].drop(columns=[\"filter_pass\"]).reset_index(drop=True)\n        else:\n            clonoset = clonoset[~clonoset[\"filter_pass\"]].drop(columns=[\"filter_pass\"]).reset_index(drop=True)\n        return clonoset\n\n\n# def convert_clonoset_to_clonotype_filter_list(clonoset_df, overlap_type=\"aaVJ\"):\n#     clonotypes_list = []\n#     aa, include_v, include_j = intersections.overlap_type_to_flags(overlap_type)\n#     for i,r in clonoset_df.iterrows():\n#         clonotype = []\n#         if aa:\n#             clonotype.append(row[\"cdr3aa\"])\n#         else:\n#             clonotype.append(row[\"cdr3nt\"])\n#         if include_v:\n#             clonotype.append(row[\"v\"])\n#         if include_j:\n#             clonotype.append(row[\"j\"])\n\n#         clonotype = tuple(clonotype)\n#         clonotypes_list.append(clonotype)\n#     return clonotypes_list\n\n    def _compare_clonoset_row_with_clonotype(self, row, clonotype):\n        c_len = len(clonotype)\n        if c_len == 1:\n            if row[\"cdr3aa\"] == clonotype[0]:\n                return True\n        elif c_len == 2:\n            if row[\"cdr3aa\"] == clonotype[0] and row[\"v\"] == clonotype[1]:\n                return True\n        elif c_len == 3:\n            if row[\"cdr3aa\"] == clonotype[0] and row[\"v\"] == clonotype[1] and row[\"j\"] == clonotype[2]:\n                return True\n        else:\n            # need to write better explanation for error\n            raise ValueError(\"clonotypes must contain from 1 to 3 values\")\n\n        return False\n\n    def _compare_clonoset_list_row_with_clonotype(self, row, clonotypes_list):\n        for clonotype in clonotypes_list:\n            if self._compare_clonoset_row_with_clonotype(row, clonotype):\n                return True\n        return False\n\n    def is_empty(self):\n        return self.functionality == \"a\" and self.downsample_size is None and self.top is None and self.count_threshold is None and not self.unweight\n\n    def _repr_html_(self):\n        \"\"\"\n        function for printing the Filter properties to Jupyter output\n        \"\"\"\n\n        functionality = {\"a\": \"any\",\n                         \"f\": \"only functional clones (no frameshifts and Stops)\",\n                         \"n\": \"only non-functional\"}\n        output  = f\"&lt;p&gt;Filter name: {self.name}&lt;/p&gt;\"\n        output += f\"&lt;p&gt;Functionality:\\t{functionality[self.functionality]}&lt;/p&gt;\"\n        random = False\n        change_size = False\n        if self.functionality != \"a\":\n            change_size = True\n        if isinstance(self.count_threshold, int):\n            output += f\"&lt;p&gt;Count threshold: {self.count_threshold}&lt;/p&gt;\"\n            change_size = True\n        if isinstance(self.downsample_size, int):\n            output += f\"&lt;p&gt;Downsample size: {self.downsample_size}&lt;/p&gt;\"\n            random = True\n            change_size = True\n        if isinstance(self.top, int):\n            output += f\"&lt;p&gt;Take top: {self.top}&lt;/p&gt;\"\n            change_size = True\n            if self.mix_tails:\n                random = True\n        if self.unweight:\n            output += f\"&lt;p&gt;Clone size unweighted: (all clone counts = 1)&lt;/p&gt;\"\n\n        if change_size:\n            if self.by_umi:\n                output += f\"&lt;p&gt;Count by:  UMI (if exist)&lt;/p&gt;\"\n            else:\n                output += f\"&lt;p&gt;Count by: reads/counts&lt;/p&gt;\"\n        if random:\n            if isinstance(self.seed, int):\n                output += f\"&lt;p&gt;Seed for random: {self.seed}&lt;/p&gt;\"\n            else:\n                output += f\"&lt;p&gt;Seed for random:\\tunset&lt;/p&gt;\"\n                output += f\"&lt;p&gt;Warning: filter contains random events. To obtain reproducible results, set seed in calcutations or manually (random.seed(some_int)) prior to applying the filter. Note only seed that is set as this object parameter will work for mix_tails&lt;/p&gt;\"\n\n        return output\n</code></pre>"},{"location":"functions/#clone_filter.Filter.apply","title":"<code>apply(input_clonoset, colnames=None)</code>","text":"<p>Main method of the Filter object - application of it to a clonoset</p> <p>Parameters:</p> Name Type Description Default <code>input_clonoset</code> <code>DataFrame</code> <p>clonoset in the form of Pandas DataFrame in MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.</p> required <code>colnames</code> <code>dict</code> <p>Dictionary of available specific column names. Defaults to None - colnames imputed automatically.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>clonoset</code> <code>DataFrame</code> <p>clonoset after converting to common (VDJtools-like) format and applying functionality filtration and downsampling or taking top</p> Source code in <code>repseq/clone_filter.py</code> <pre><code>def apply(self, input_clonoset, colnames=None):\n    \"\"\"\n    Main method of the Filter object - application of it to a clonoset\n\n    Args:\n        input_clonoset (pd.DataFrame): clonoset in the form of Pandas DataFrame in\n            MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.\n        colnames (dict, optional): Dictionary of available specific column names.\n            Defaults to None - colnames imputed automatically.\n\n    Returns:\n        clonoset (pd.DataFrame): clonoset after converting to common (VDJtools-like)\n            format and applying functionality filtration and downsampling or taking top\n    \"\"\"\n\n    # copy clonoset for not changing the original one\n    clonoset = input_clonoset.copy()\n    if colnames is None:\n        colnames = get_column_names_from_clonoset(clonoset)\n\n    # create common columns: vdj-refPoints and VDJC-segments in common state\n    clonoset = self._make_common_columns(clonoset, colnames)\n    colnames = get_column_names_from_clonoset(clonoset)\n\n    # converting to common VDJtools-like format and obtaining new colnames\n    if self.convert:\n        clonoset = self._convert_clonoset(clonoset, colnames)\n        colnames = get_column_names_from_clonoset(clonoset)\n\n    # application of main filters\n    if self.functionality != \"a\":\n        clonoset = self._filter_by_functionality(clonoset, colnames)\n\n    clonoset = self._filter_by_count(clonoset, colnames)\n\n    clonoset = self._downsample(clonoset, colnames)\n    clonoset = self._get_top(clonoset, colnames)\n\n    if self.unweight:\n        clonoset = self._unweight(clonoset, colnames)\n    # the fraction columns need to be recounted after filtering, as they\n    # remain the same as in the original clonoset before filtration\n    if self.recount_fractions:\n        clonoset = self._recount_fractions_for_clonoset(clonoset, colnames)\n    if self.pool_by:\n        clonoset = self._pool_clonoset(clonoset, colnames)\n    if len(self.white_list) &gt; 0:\n        clonoset = self._filter_clonotypes(clonoset, list_type=\"white\")\n    if len(self.black_list) &gt; 0:\n        clonoset = self._filter_clonotypes(clonoset, list_type=\"black\")\n    return clonoset\n</code></pre>"},{"location":"functions/#clone_filter.Filter.spawn","title":"<code>spawn()</code>","text":"<p>Returns:</p> Type Description <p>the copy of the filter. Necessary for parallel computing</p> Source code in <code>repseq/clone_filter.py</code> <pre><code>def spawn(self):\n    \"\"\"\n\n    Returns:\n        the copy of the filter. Necessary for parallel computing\n\n    \"\"\"\n    return Filter(name=self.name, functionality=self.functionality,\n                  downsample=self.downsample_size, top=self.top,\n                  by_umi=self.by_umi, mix_tails=self.mix_tails,\n                  count_threshold=self.count_threshold, seed=self.seed,\n                  unweight=self.unweight,\n                  recount_fractions=self.recount_fractions,\n                  white_list = self.white_list,\n                  black_list = self.black_list,\n                  ignore_small_clonosets=self.ignore_small_clonosets\n                  )\n</code></pre>"},{"location":"functions/#clonosets","title":"clonosets","text":""},{"location":"functions/#clonosets.annotate_clonotypes_with_vdjdb","title":"<code>annotate_clonotypes_with_vdjdb(clonotypes_df, drop_method=True, drop_meta=True, drop_cdr3fix=True)</code>","text":"<p>Uses entries from VDJdb database to annotate the clonotypes with the same V, J, and CDR3. In case of several matches, the one with the highest vdjdb score is used.</p> <p>Parameters:</p> Name Type Description Default <code>clonotypes_df</code> <code>DataFrame</code> <p>A dataframe containing clonotypes. For instance, it can be made with  <code>pool_clonotypes_from_clonosets_df</code> function from the Clustering module. </p> required <code>drop_method</code> <code>bool</code> <p>if <code>True</code>, <code>Method</code> column is excluded from the annotation</p> <code>True</code> <code>drop_meta</code> <code>bool</code> <p>if <code>True</code>, <code>Meta</code> column is excluded from the annotation</p> <code>True</code> <code>drop_cdr3fix</code> <code>bool</code> <p>if <code>True</code>, <code>CDR3fix</code> column is excluded from the annotation</p> <code>True</code> <p>Returns:</p> Name Type Description <code>annotated_clonotypes</code> <code>DataFrame</code> <p>annotated clonotypes_df</p> Source code in <code>repseq/clonosets.py</code> <pre><code>def annotate_clonotypes_with_vdjdb(clonotypes_df, drop_method=True, drop_meta=True, drop_cdr3fix=True):\n    \"\"\"\n    Uses entries from VDJdb database to annotate the clonotypes with the same V, J, and CDR3. In case of several matches,\n    the one with the highest vdjdb score is used.\n\n    Args:\n        clonotypes_df (pd.DataFrame): A dataframe containing clonotypes. For instance, it can be made with \n            `pool_clonotypes_from_clonosets_df` function from the Clustering module. \n        drop_method (bool): if `True`, `Method` column is excluded from the annotation\n        drop_meta (bool): if `True`, `Meta` column is excluded from the annotation\n        drop_cdr3fix (bool): if `True`, `CDR3fix` column is excluded from the annotation\n\n    Returns:\n        annotated_clonotypes (pd.DataFrame): annotated clonotypes_df\n    \"\"\"\n    clonotypes_df_copy = clonotypes_df.copy()\n    vdjdb_whole = vdjdb(vdjdb_dataset=None, drop_method=drop_method, drop_meta=drop_meta, drop_cdr3fix=drop_cdr3fix)\n    vdjdb_whole = vdjdb_whole.sort_values(by='score', ascending=False).drop_duplicates(subset=['v', 'j', 'cdr3aa', 'epitope', 'epitope_gene', 'epitope_species'])\n    annotated_clonotypes = pd.merge(clonotypes_df_copy, vdjdb_whole, on=['cdr3aa', 'v', 'j'], how='left')\n    return annotated_clonotypes\n</code></pre>"},{"location":"functions/#clonosets.find_all_exported_clonosets","title":"<code>find_all_exported_clonosets(folders, chain=None, show_offtarget=True, offtarget_threshold=0.01)</code>","text":"<p>Main method of the Filter object - application of it to a clonoset</p> <p>Parameters:</p> Name Type Description Default <code>folders</code> <code>path or list of paths</code> <p>clonoset in the form of Pandas DataFrame in MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.</p> required <code>colnames</code> <code>dict</code> <p>Dictionary of available specific column names. Defaults to None - colnames imputed automatically.</p> required <p>Returns:</p> Name Type Description <code>clonoset</code> <code>DataFrame</code> <p>clonoset after converting to common (VDJtools-like) format and applying functionality filtration and downsampling or taking top</p> Source code in <code>repseq/clonosets.py</code> <pre><code>def find_all_exported_clonosets(folders, chain=None, show_offtarget=True, offtarget_threshold=0.01):\n    \"\"\"\n    Main method of the Filter object - application of it to a clonoset\n\n    Args:\n        folders (path or list of paths): clonoset in the form of Pandas DataFrame in\n            MiXCR(3 or 4+ version), VDJtools or Bioadaptive formats.\n        colnames (dict, optional): Dictionary of available specific column names.\n            Defaults to None - colnames imputed automatically.\n\n    Returns:\n        clonoset (pd.DataFrame): clonoset after converting to common (VDJtools-like)\n            format and applying functionality filtration and downsampling or taking top\n\n    \"\"\"\n    if isinstance(folders, str):\n        folders = [folders]\n    clonosets_dfs = []\n    for folder in folders:\n        clonosets_dfs.append(find_all_exported_clonosets_in_folder(folder, chain=chain, show_offtarget=show_offtarget, offtarget_threshold=offtarget_threshold))\n    return pd.concat(clonosets_dfs)\n</code></pre>"},{"location":"functions/#clustering","title":"clustering","text":""},{"location":"functions/#clustering.Node","title":"<code>Node</code>","text":"Source code in <code>repseq/clustering.py</code> <pre><code>class Node:\n\n    def __init__(self, node_id, seq_nt, seq_aa, v, j, sample_id, size=1):\n        self.id = node_id\n        self.v = v\n        self.j = j\n        self.seq_aa = seq_aa\n        self.seq_nt = seq_nt\n        self.sample_id = sample_id\n        self.size = size\n        self.additional_properties = {}\n\n    def is_neighbour_of(self, other, mismatches=1, aa=True, check_v=False, check_j=False):\n        \"\"\"function compare two strings and return\n        True if their are equal\n            or if they have one mismatch and equal length\n        False in all other conditions\n        \"\"\"\n\n        if aa:\n            string1 = self.seq_aa\n            string2 = other.seq_aa\n        else:\n            string1 = self.seq_nt\n            string2 = other.seq_nt\n        if len(string1) != len(string2):\n             return False\n        if check_v and self.v != other.v:\n            return False\n        if check_j and self.j != other.j:\n            return False\n        hamm_dist = sum([a != b for a,b in zip(string1,string2)]) \n        if hamm_dist &gt; mismatches:\n            return False     \n        return True\n\n    def __str__(self):\n        return \"{}_{}\".format(self.seq_aa, self.id)\n\n    def add_properties(self, metadata):\n        if self.sample_id not in metadata:\n            for property in list(metadata[list(metadata)[0]].keys()):\n                self.additional_properties[property] = None\n        else:\n            self.additional_properties.update(metadata[self.sample_id])\n</code></pre>"},{"location":"functions/#clustering.Node.is_neighbour_of","title":"<code>is_neighbour_of(other, mismatches=1, aa=True, check_v=False, check_j=False)</code>","text":"<p>function compare two strings and return True if their are equal     or if they have one mismatch and equal length False in all other conditions</p> Source code in <code>repseq/clustering.py</code> <pre><code>def is_neighbour_of(self, other, mismatches=1, aa=True, check_v=False, check_j=False):\n    \"\"\"function compare two strings and return\n    True if their are equal\n        or if they have one mismatch and equal length\n    False in all other conditions\n    \"\"\"\n\n    if aa:\n        string1 = self.seq_aa\n        string2 = other.seq_aa\n    else:\n        string1 = self.seq_nt\n        string2 = other.seq_nt\n    if len(string1) != len(string2):\n         return False\n    if check_v and self.v != other.v:\n        return False\n    if check_j and self.j != other.j:\n        return False\n    hamm_dist = sum([a != b for a,b in zip(string1,string2)]) \n    if hamm_dist &gt; mismatches:\n        return False     \n    return True\n</code></pre>"},{"location":"functions/#common_functions","title":"common_functions","text":""},{"location":"functions/#common_functions.kl_divergence","title":"<code>kl_divergence(p, q, epsilon=1e-10)</code>","text":"<p>Calculate the KL divergence between two probability distributions p and q.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>list of float</code> <p>First probability distribution.</p> required <code>q</code> <code>list of float</code> <p>Second probability distribution.</p> required <code>epsilon</code> <code>float</code> <p>Small value to avoid log(0) and division by zero.</p> <code>1e-10</code> <p>Returns:</p> Name Type Description <code>float</code> <p>KL divergence D(P || Q)</p> Source code in <code>repseq/common_functions.py</code> <pre><code>def kl_divergence(p, q, epsilon=1e-10):\n    \"\"\"\n    Calculate the KL divergence between two probability distributions p and q.\n\n    Args:\n        p (list of float): First probability distribution.\n        q (list of float): Second probability distribution.\n        epsilon (float): Small value to avoid log(0) and division by zero.\n\n    Returns:\n        float: KL divergence D(P || Q)\n    \"\"\"\n\n    p = np.asarray(p, dtype=np.float64)\n    q = np.asarray(q, dtype=np.float64)\n\n    # normalize\n    p /= np.sum(p)\n    q /= np.sum(q)\n\n    # escape division by zero and log(0) with epsilon value\n    p_safe = np.where(p == 0, epsilon, p)\n    q_safe = np.where(q == 0, epsilon, q)\n\n    result = np.sum(np.where(p != 0, p_safe * np.log(p_safe / q_safe), 0))\n\n    return result\n</code></pre>"},{"location":"functions/#constants","title":"constants","text":""},{"location":"functions/#diffexp","title":"diffexp","text":""},{"location":"functions/#intersections","title":"intersections","text":""},{"location":"functions/#intersections.count_table","title":"<code>count_table(clonosets_df, cl_filter=None, overlap_type='aaV', mismatches=0, strict_presence=False, by_freq=False)</code>","text":"<p>Creates a table that shows how many times each unique clonotype appears across different clonosets. It processes a given dataset of clonotypes (clonosets_df)  and generates a frequency/count table based on a specified overlap type.</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, <code>filename</code> - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>mismatches</code> <code>int</code> <p>Max number of single-letter mismatches in clonotypes sequences  for them to be treated similar, i.e. hamming distance.</p> <code>0</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required <code>strict_presence (bool, default</code> <p>): if set to <code>True</code> and the clonotype is not found in the clonoset, it will not be counted, even when the <code>mismatches</code> option is not set to 0. If <code>False</code>, mismatched sequences are counted even if the exact match does not exist.     by_freq (bool): default is <code>True</code> - this means that the intersect metric is frequency of clonotype,  but not its count</p> required <code>by_freq</code> <code>bool</code> <p>default is <code>True</code> - this means that the intersect metric is frequency of clonotype,  but not its count</p> <code>False</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe containing clonotype counts with sample names as columns and all possible clonotypes (given the overlap_type) as rows.</p> Source code in <code>repseq/intersections.py</code> <pre><code>def count_table(clonosets_df, cl_filter=None, overlap_type=\"aaV\", mismatches=0, strict_presence=False, by_freq=False):\n    \"\"\"\n    Creates a table that shows how many times each unique clonotype appears across different clonosets. It processes a given dataset of clonotypes (clonosets_df) \n    and generates a frequency/count table based on a specified overlap type.\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            `filename` - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): Max number of single-letter mismatches in clonotypes sequences \n            for them to be treated similar, i.e. hamming distance.\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n        strict_presence (bool, default:): if set to `True` and the clonotype is not found in the clonoset, it will not be counted, even when the `mismatches` option is not set to 0.\n            If `False`, mismatched sequences are counted even if the exact match does not exist.\n                by_freq (bool): default is `True` - this means that the intersect metric is frequency of clonotype, \n            but not its count\n        by_freq (bool): default is `True` - this means that the intersect metric is frequency of clonotype, \n            but not its count\n\n    Returns:\n        df (pd.DataFrame): dataframe containing clonotype counts with sample names as columns and all possible clonotypes (given the overlap_type) as rows. \n    \"\"\"\n\n\n    print(\"Creating clonotypes count table\\n\"+\"-\"*50)\n    print(f\"Overlap type: {overlap_type}\")\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    clonoset_dicts = convert_clonosets_to_compact_dicts(clonosets_df, cl_filter=cl_filter,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=not bool(mismatches))\n    unique_clonotypes = find_unique_clonotypes_in_clonoset_dicts(clonoset_dicts)\n\n    tasks = []\n    for sample_id in clonoset_dicts:\n        task = [unique_clonotypes, sample_id, clonoset_dicts[sample_id], mismatches, strict_presence]\n        tasks.append(task)\n\n    results = run_parallel_calculation(count_table_mp, tasks, \"Counting features\", object_name=\"clonosets\")\n    result_dict = dict()\n    for result in results:\n        result_dict.update(result)\n    count_table = pd.DataFrame(result_dict)\n    count_table.index = unique_clonotypes\n    return count_table\n</code></pre>"},{"location":"functions/#intersections.count_table_by_cluster","title":"<code>count_table_by_cluster(clonosets_df, clusters_list, cl_filter=None, overlap_type='aaV', mismatches=0, by_freq=True)</code>","text":"<p>This function creates a table that shows the presence of clonotypes grouped into user-provided clusters across different clonosets. Instead of counting individual clonotypes, it  calculates how many clonotypes from each cluster appear in each clonoset.</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>cluster_list</code> <code>?</code> <p>description</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>mismatches</code> <code>int</code> <p>Max number of single-letter mismatches in clonotypes sequences  for them to be treated similar, i.e. hamming distance.</p> <code>0</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with the following columns: description</p> Source code in <code>repseq/intersections.py</code> <pre><code>def count_table_by_cluster(clonosets_df, clusters_list, cl_filter=None, overlap_type=\"aaV\", mismatches=0, by_freq=True):\n\n    \"\"\"\n    This function creates a table that shows the presence of clonotypes grouped into user-provided clusters across different clonosets. Instead of counting individual clonotypes, it \n    calculates how many clonotypes from each cluster appear in each clonoset.\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        cluster_list (?): description\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): Max number of single-letter mismatches in clonotypes sequences \n            for them to be treated similar, i.e. hamming distance.\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n\n    Returns:\n        df (pd.DataFrame): dataframe with the following columns: description\n    \"\"\"\n\n    print(\"Creating clonotypes count table\\n\"+\"-\"*50)\n    print(f\"Overlap type: {overlap_type}\")\n\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n\n    clonoset_dicts = convert_clonosets_to_compact_dicts(clonosets_df, cl_filter=cl_filter,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=not bool(mismatches))\n\n    clonotypes_by_cluster = convert_clusters_to_clonotype_list(clusters_list, aa, check_v, check_j, mismatches)\n\n    tasks = []\n    for sample_id in clonoset_dicts:\n        task = [clonotypes_by_cluster, sample_id, clonoset_dicts[sample_id], mismatches]\n        tasks.append(task)\n\n    results = run_parallel_calculation(count_table_by_cluster_mp, tasks, \"Counting cluster presence\", object_name=\"clonosets\")\n    result_dict = dict()\n    for result in results:\n        result_dict.update(result)\n    count_table = pd.DataFrame(result_dict).reset_index().rename(columns = {\"index\":\"feature_id\"})\n    count_table[\"feature_id\"] = count_table[\"feature_id\"].apply(lambda x: f\"cluster_{x}\")\n\n    return count_table\n</code></pre>"},{"location":"functions/#intersections.count_table_with_custom_clonotypes","title":"<code>count_table_with_custom_clonotypes(clonosets_df, clones_df=None, cl_filter=None, overlap_type='aaV', mismatches=0, strict_presence=False, by_freq=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>clones_df</code> <code>DataFrame</code> <p>a dataframe with clonotypes to look for in <code>clonosets_df</code>. Should have the following columns:</p> <code>None</code> Source code in <code>repseq/intersections.py</code> <pre><code>def count_table_with_custom_clonotypes(clonosets_df, clones_df=None, cl_filter=None, overlap_type=\"aaV\", mismatches=0, strict_presence=False, by_freq=False):\n    \"\"\"\n    Args:\n        clones_df (pd.DataFrame): a dataframe with clonotypes to look for in `clonosets_df`. Should have the following columns:\n        `cdr3aa`, `cdr3nt` (at least one of these two), `v`, `j` (could be none of those). For instance, your input might have only `cdr3aa` column.\n        If `overlap_type` isn't specified, it is determined individually for each clonotype in `clones_df`. In this case, if a row corresponding to\n        a particular clonotype has non-empty values in both `cdr3aa` and `cdr3nt` columns, `cdr3aa` is chosen.\n    \"\"\"\n\n    print(\"Creating clonotypes count table\\n\"+\"-\"*50)\n    print(f\"Overlap type: {overlap_type}\")\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    clonoset_dicts = convert_clonosets_to_compact_dicts(clonosets_df, cl_filter=cl_filter,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=not bool(mismatches))\n\n    if clones_df is not None:\n        # creating dummy counts\n        clones_df_copy = clones_df.copy()\n        clones_df_copy['count'] = [0 for _ in range(clones_df.shape[0])]\n        clones_df_copy['freq'] = [0 for _ in range(clones_df.shape[0])]\n        clonotype_list_dict = {'clonotype_list': None}\n\n        clonotype_list_dict['clonotype_list'] = prepare_clonoset_for_intersection(clones_df_copy, \n                                                                                overlap_type=overlap_type,\n                                                                                len_vj_format=bool(mismatches))  \n\n        unique_clonotypes = find_unique_clonotypes_in_clonoset_dicts(clonotype_list_dict)\n    else: \n        unique_clonotypes = find_unique_clonotypes_in_clonoset_dicts(clonoset_dicts)\n\n    tasks = []\n    for sample_id in clonoset_dicts:\n        task = [unique_clonotypes, sample_id, clonoset_dicts[sample_id], mismatches, strict_presence]\n        tasks.append(task)\n\n    results = run_parallel_calculation(count_table_mp, tasks, \"Counting features\", object_name=\"clonosets\")\n    result_dict = dict()\n    for result in results:\n        result_dict.update(result)\n    count_table = pd.DataFrame(result_dict)\n    if aa:\n        count_table.insert(0, 'cdr3aa', [ct[0] for ct in unique_clonotypes])\n    else:\n        count_table.insert(0, 'cdr3nt', [ct[0] for ct in unique_clonotypes])\n    if check_v:\n        count_table.insert(1, 'v', [ct[1] for ct in unique_clonotypes])\n    if check_j:\n        count_table.insert(1, 'j', [ct[2] for ct in unique_clonotypes])\n    count_table.index = unique_clonotypes\n    return count_table\n</code></pre>"},{"location":"functions/#intersections.find_intersecting_clonotypes","title":"<code>find_intersecting_clonotypes(clonosets_df, cl_filter=None, overlap_type='aaV', mismatches=0, metric='F2', clonosets_df2=None, cl_filter2=None)</code>","text":"<p>Calculating overlap distances between multiple repseq samples using F2 of F metrics The result of this function may be used for heatmap+clusterization of samples or for MDS plots</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>cl_filter</code> <code>Filter</code> <p>clonoset filter - object from <code>clone_filter.py</code> module. It is applied to clonosets in <code>clonosets_df</code>.</p> <code>None</code> <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>mismatches</code> <code>int</code> <p>Max number of single-letter mismatches in clonotypes sequences  for them to be treated similar, i.e. hamming distance.</p> <code>0</code> <code>by_umi</code> <code>bool</code> <p>set =True for MiXCR4 clonosets to select count/frequency of clonotypes  in UMI's if they exist in implemented protocol</p> required <code>metric</code> <code>str</code> <p>possible values - <code>F</code>, <code>F2</code> or <code>C</code>. Default <code>F2</code>. <code>F2</code> - sum of sqrt of product of  similar clonotype frequencies in two clonosets. <code>F</code> - sqrt of the sum of frequency products. <code>C</code> - total frequency of clonotypes in <code>sample1</code>, that are similar to clonotypes in <code>sample2</code></p> <code>'F2'</code> <code>clonosets_df2</code> <code>DataFrame</code> <p>If <code>clonosets_df2</code> is None (default), samples within <code>clonosets_df</code> are compared with each other;  Otherwise, the comparison is performed exclusively between samples from <code>clonosets_df</code> and <code>clonosets_df2</code>. </p> <code>None</code> <code>cl_filter2</code> <code>Filter</code> <p>clonoset filter - object from <code>clone_filter.py</code> module. It is applied to clonosets in <code>clonosets_df2</code>.  If there are samples with non-unique sample_ids between the two dataframes, both filters will be applied to those samples.</p> <code>None</code> <p>Important: similar clonotypes by <code>overlap_type</code> in one particular clonoset are NOT combined into one and are treated as different clonotypes.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>clone</code>, <code>sample1_count</code>, <code>sample2_count</code>, <code>sample1</code>, <code>sample2</code>, <code>pair</code>.  <code>clone</code> - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric count columns contain freq/count of the clone in sample pair column is made for easy separation of possibly huge DataFrame into overlapping pairs</p> Source code in <code>repseq/intersections.py</code> <pre><code>def find_intersecting_clonotypes(clonosets_df, cl_filter=None, overlap_type=\"aaV\", mismatches=0, metric=\"F2\", clonosets_df2=None, cl_filter2=None):\n    \"\"\"\n    Calculating overlap distances between multiple repseq samples using F2 of F metrics\n    The result of this function may be used for heatmap+clusterization of samples or for MDS plots\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        cl_filter (Filter): clonoset filter - object from `clone_filter.py` module. It is applied to clonosets in `clonosets_df`.\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): Max number of single-letter mismatches in clonotypes sequences \n            for them to be treated similar, i.e. hamming distance.\n        by_umi (bool): set =True for MiXCR4 clonosets to select count/frequency of clonotypes \n            in UMI's if they exist in implemented protocol\n        metric (str): possible values - `F`, `F2` or `C`. Default `F2`. `F2` - sum of sqrt of product of \n            similar clonotype frequencies in two clonosets. `F` - sqrt of the sum of frequency products.\n            `C` - total frequency of clonotypes in `sample1`, that are similar to clonotypes in `sample2`\n        clonosets_df2 (pd.DataFrame): If `clonosets_df2` is None (default), samples within `clonosets_df` are compared with each other; \n            Otherwise, the comparison is performed exclusively between samples from `clonosets_df` and `clonosets_df2`. \n        cl_filter2 (Filter): clonoset filter - object from `clone_filter.py` module. It is applied to clonosets in `clonosets_df2`. \n            If there are samples with non-unique sample_ids between the two dataframes, both filters will be applied to those samples.\n\n\n    Important: similar clonotypes by `overlap_type` in one particular clonoset are NOT combined into one\n    and are treated as different clonotypes.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `clone`, `sample1_count`, `sample2_count`, `sample1`, `sample2`, `pair`. \n            `clone` - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric\n            count columns contain freq/count of the clone in sample\n            pair column is made for easy separation of possibly huge DataFrame into overlapping pairs\n    \"\"\"\n\n\n    print(\"Intersecting clones in clonosets\\n\"+\"-\"*50)\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    print(f\"Overlap type: {overlap_type}\")\n\n    clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2 = prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2,\n                                                                                                                        cl_filter, cl_filter2,\n                                                                                                                        overlap_type, strict=not bool(mismatches))\n\n    # generating a set of tasks\n\n    tasks = []\n\n    if two_dataframes:\n        for sample1 in sample_list:\n            for sample2 in sample_list2:\n                tasks.append((sample1, sample2, clonoset_lists, check_v, check_j, mismatches))        \n    else:\n        for i in range(samples_total):\n            for j in range(samples_total):\n                sample1 = sample_list[i]\n                sample2 = sample_list[j]\n                if sample1 != sample2:\n                    tasks.append((sample1, sample2, clonoset_lists, check_v, check_j, mismatches))\n\n\n    # run calculation in parallel\n    result_list = run_parallel_calculation(find_overlapping_clones_in_two_clone_dicts, tasks, \"Intersecting clonosets\", object_name=\"pairs\")\n\n    return pd.concat(result_list).reset_index(drop=True)\n</code></pre>"},{"location":"functions/#intersections.intersect_clones_in_samples_batch","title":"<code>intersect_clones_in_samples_batch(clonosets_df, cl_filter=None, overlap_type='aaV', by_freq=True, clonosets_df2=None, cl_filter2=None)</code>","text":"<p>Calculating frequencies of intersecting clonotypes between multiple repseq samples. The result of this function may be used for scatterplots of frequencies/counts of  overlapping clonotypes</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, <code>filename</code> - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>by_umi</code> <code>bool</code> <p>set <code>=True</code> for MiXCR4 clonosets to select count/frequency of clonotypes  in UMI's if they exist in implemented protocol</p> required <code>by_freq</code> <code>bool</code> <p>default is <code>True</code> - this means that the intersect metric is frequency of clonotype,  but not its count</p> <code>True</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required <p>Important: when using particular overlap type, similar clonotypes in one particular clonoset are combined into one with summation of counts/frequencies.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>clone</code>, <code>sample1_count</code>, <code>sample2_count</code>, <code>sample1</code>, <code>sample2</code>, <code>pair</code> clone - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric count columns contain freq/count of the clone in sample pair column is made for easy separation of possibly huge DataFrame into overlapping pairs</p> Source code in <code>repseq/intersections.py</code> <pre><code>def intersect_clones_in_samples_batch(clonosets_df, cl_filter=None, overlap_type=\"aaV\", by_freq=True, clonosets_df2=None, cl_filter2=None):\n    \"\"\"\n    Calculating frequencies of intersecting clonotypes between multiple repseq samples.\n    The result of this function may be used for scatterplots of frequencies/counts of \n    overlapping clonotypes\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            `filename` - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        by_umi (bool): set `=True` for MiXCR4 clonosets to select count/frequency of clonotypes \n            in UMI's if they exist in implemented protocol\n        by_freq (bool): default is `True` - this means that the intersect metric is frequency of clonotype, \n            but not its count\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n\n    Important: when using particular overlap type, similar clonotypes in one particular clonoset are\n    combined into one with summation of counts/frequencies.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `clone`, `sample1_count`, `sample2_count`, `sample1`, `sample2`, `pair`\n            clone - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric\n            count columns contain freq/count of the clone in sample\n            pair column is made for easy separation of possibly huge DataFrame into overlapping pairs\n    \"\"\"\n\n\n    print(\"Intersecting clones in clonosets\\n\"+\"-\"*50)\n    print(f\"Overlap type: {overlap_type}\")\n\n    clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2 = prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2,\n                                                                                                                        cl_filter, cl_filter2,\n                                                                                                                        overlap_type, by_freq=by_freq,\n                                                                                                                        strict=True)\n    # generating a set of tasks\n\n    tasks = []\n\n    if two_dataframes:\n        for sample1 in sample_list:\n            for sample2 in sample_list2:\n                tasks.append((sample1, sample2, clonoset_lists))\n    else:\n        for i in range(samples_total):\n            sample1 = sample_list[i]\n            for j in range(samples_total-i-1):\n                sample2 = sample_list[j+i+1]\n                tasks.append((sample1, sample2, clonoset_lists))\n\n    results = run_parallel_calculation(intersect_two_clone_dicts, tasks, \"Intersecting clonosets\", object_name=\"pairs\")\n\n    # df = pd.concat(results).index.set_names()\n    df = pd.concat(results).reset_index(drop=True)\n    df = split_tuple_clone_column(df, overlap_type)\n\n    return df\n</code></pre>"},{"location":"functions/#intersections.overlap_distances","title":"<code>overlap_distances(clonosets_df, cl_filter=None, overlap_type='aaV', mismatches=0, metric='F2', clonosets_df2=None, cl_filter2=None)</code>","text":"<p>Calculating overlap distances between multiple repseq samples using F2 of F metrics The result of this function may be used for heatmap+clusterization of samples or for MDS plots</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>mismatches</code> <code>int</code> <p>Max number of single-letter mismatches in clonotypes sequences  for them to be treated similar, i.e. hamming distance.</p> <code>0</code> <code>by_umi</code> <code>bool</code> <p>set =True for MiXCR4 clonosets to select count/frequency of clonotypes  in UMI's if they exist in implemented protocol</p> required <code>metric</code> <code>str</code> <p>possible values - <code>F</code>, <code>F2</code> or <code>C</code>. Default <code>F2</code>. <code>F2</code> - sum of sqrt of product of  similar clonotype frequencies in two clonosets. <code>F</code> - sqrt of the sum of frequency products. <code>C</code> - total frequency of clonotypes in <code>sample1</code>, that are similar to clonotypes in <code>sample2</code></p> <code>'F2'</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required similar clonotypes by <code>overlap_type</code> in one particular clonoset will be combined into one <p>clonotype with sum for count.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>clone</code>, <code>sample1_count</code>, <code>sample2_count</code>, <code>sample1</code>, <code>sample2</code>, <code>pair</code>.  <code>clone</code> - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric count columns contain freq/count of the clone in sample pair column is made for easy separation of possibly huge DataFrame into overlapping pairs</p> Source code in <code>repseq/intersections.py</code> <pre><code>def overlap_distances(clonosets_df, cl_filter=None, overlap_type=\"aaV\", mismatches=0, metric=\"F2\", clonosets_df2=None, cl_filter2=None):\n    \"\"\"\n    Calculating overlap distances between multiple repseq samples using F2 of F metrics\n    The result of this function may be used for heatmap+clusterization of samples or for MDS plots\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): Max number of single-letter mismatches in clonotypes sequences \n            for them to be treated similar, i.e. hamming distance.\n        by_umi (bool): set =True for MiXCR4 clonosets to select count/frequency of clonotypes \n            in UMI's if they exist in implemented protocol\n        metric (str): possible values - `F`, `F2` or `C`. Default `F2`. `F2` - sum of sqrt of product of \n            similar clonotype frequencies in two clonosets. `F` - sqrt of the sum of frequency products.\n            `C` - total frequency of clonotypes in `sample1`, that are similar to clonotypes in `sample2`\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n\n    Important: similar clonotypes by `overlap_type` in one particular clonoset will be combined into one\n        clonotype with sum for count.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `clone`, `sample1_count`, `sample2_count`, `sample1`, `sample2`, `pair`. \n            `clone` - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric\n            count columns contain freq/count of the clone in sample\n            pair column is made for easy separation of possibly huge DataFrame into overlapping pairs\n    \"\"\"\n\n\n    print(\"Intersecting clones in clonosets\\n\"+\"-\"*50)\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    print(f\"Overlap type: {overlap_type}\")\n\n    metric = metric.upper()\n    metrics = [\"F\", \"F2\", \"C\", \"BC\", \"J\", \"JSD\"]\n    mismatch_metrics = [\"F\", \"C\"]\n    non_symmetry_metrics = [\"C\"]\n    frequency_metrics = [\"F\", \"F2\", \"C\"]\n\n\n    if metric not in metrics:\n        raise ValueError(f\"Metric {metric} is not supported. Possible values: {', '.join(metrics)}\")\n\n    if mismatches and metric not in mismatch_metrics:\n        raise ValueError(f\"Metric {metric} does not allow mismatches. Mismatches only possible for: {', '.join(mismatch_metrics)}\")\n\n    by_freq = metric in frequency_metrics\n\n    clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2 = prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2,\n                                                                                                                        cl_filter, cl_filter2,\n                                                                                                                        overlap_type, by_freq=by_freq)\n\n    # generating a set of tasks\n\n    tasks = []\n\n    if two_dataframes:\n        for sample1 in sample_list:\n            for sample2 in sample_list2:\n                tasks.append((sample1, sample2, clonoset_lists, mismatches, metric))        \n    else:\n        if metric not in non_symmetry_metrics and not two_dataframes:\n            for i in range(samples_total):\n                sample1 = sample_list[i]\n                for j in range(samples_total-i-1):\n                    sample2 = sample_list[j+i+1]\n                    tasks.append((sample1, sample2, clonoset_lists, mismatches, metric))\n                if metric == \"F2\":\n                    tasks.append((sample1, sample1, clonoset_lists, mismatches, metric))\n        else:\n            for i in range(samples_total):\n                for j in range(samples_total):\n                    sample1 = sample_list[i]\n                    sample2 = sample_list[j]\n                    if sample1 != sample2:\n                        tasks.append((sample1, sample2, clonoset_lists, mismatches, metric))\n\n\n    # run calculation in parallel\n    result_list = run_parallel_calculation(overlap_metric_two_clone_dicts, tasks, \"Intersecting clonosets\", object_name=\"pairs\")\n\n    if not two_dataframes and metric != \"C\":\n        result_list = result_list + [(result[1], result[0], result[2]) for result in result_list]\n    overlap_df = pd.DataFrame(result_list, columns=[\"sample1\", \"sample2\", metric.lower()]).pivot_table(index=\"sample1\", columns=[\"sample2\"], values=metric.lower()).reset_index().set_index(\"sample1\").fillna(1)\n    return overlap_df\n</code></pre>"},{"location":"functions/#intersections.prepare_clonotypes_dfs_for_intersections","title":"<code>prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2, cl_filter, cl_filter2, overlap_type, by_freq=True, strict=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>description</p> required <code>clonosets_df2</code> <code>DataFrame</code> <p>description</p> required <code>cl_filter</code> <code>Filter</code> <p>description</p> required <code>cl_filter2</code> <code>Filter</code> <p>description</p> required <code>overlap_type</code> <code>str</code> <p>description</p> required <code>by_freq</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> <code>ValueError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>clonoset_lists</code> <code>dict</code> <p>dict of </p> <code>samples_total</code> <code>int</code> <code>two_dataframes</code> <code>bool</code> <code>sample_list</code> <code>list</code> <code>sample_list2</code> <code>list</code> Source code in <code>repseq/intersections.py</code> <pre><code>def prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2, cl_filter, cl_filter2, overlap_type, by_freq=True, strict=False):\n    \"\"\"\n    Args:\n        clonosets_df (pd.DataFrame): _description_\n        clonosets_df2 (pd.DataFrame): _description_\n        cl_filter (Filter): _description_\n        cl_filter2 (Filter): _description_\n        overlap_type (str): _description_\n        by_freq (bool, optional): _description_. Defaults to True.\n\n    Raises:\n        ValueError: _description_\n        ValueError: _description_\n\n    Returns:\n        clonoset_lists (dict): dict of \n        samples_total (int): \n        two_dataframes (bool):\n        sample_list (list):\n        sample_list2 (list):\n    \"\"\"\n    # output:\n    ### clonoset_lists\n\n    if len(clonosets_df.sample_id.unique()) &lt; len(clonosets_df):\n        raise ValueError(\"Input clonosets in DataFrame have non-unique sample_id's\")\n    clonosets_df_1 = clonosets_df[[\"sample_id\", \"filename\"]]\n    two_dataframes = False\n    if isinstance(clonosets_df2, pd.DataFrame):\n        two_dataframes = True\n        if len(clonosets_df2.sample_id.unique()) &lt; len(clonosets_df2):\n            raise ValueError(\"Input clonosets in DataFrame2 have non-unique sample_id's\")\n        clonosets_df_2 = clonosets_df2[[\"sample_id\", \"filename\"]]\n        intersecting_sample_ids = set(clonosets_df2.sample_id.unique()).intersection(set(clonosets_df.sample_id.unique()))\n        if len(intersecting_sample_ids) &gt; 0 and cl_filter2 is not None:\n            print(\"WARNING! Some samples have the same sample_id in two sample_df's. The second filter will be applied to common samples\")\n\n\n    # converting clonosets to compact lists of clonotypes separated by CDR3 lengths to dictionary based on overlap type and count/freq/umi\n    clonoset_lists = convert_clonosets_to_compact_dicts(clonosets_df_1, cl_filter=cl_filter,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=strict)\n    if two_dataframes:\n        if cl_filter2 is None:\n            cl_filter2 = cl_filter\n        clonoset_lists_2 = convert_clonosets_to_compact_dicts(clonosets_df_2, cl_filter=cl_filter2,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=strict)\n        clonoset_lists.update(clonoset_lists_2)\n\n    samples_total = len(clonosets_df_1)\n    if two_dataframes:\n        samples_total = len(pd.concat([clonosets_df_1, clonosets_df_2]))\n\n    sample_list = list(clonosets_df_1.sort_values(by=\"sample_id\").sample_id)\n    sample_list2 = None\n    if two_dataframes:\n        sample_list2 = list(clonosets_df_2.sort_values(by=\"sample_id\").sample_id)\n\n    return clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2\n</code></pre>"},{"location":"functions/#intersections.tcrnet","title":"<code>tcrnet(clonosets_df_exp, clonosets_df_control, cl_filter=None, cl_filter_c=None, overlap_type='aaVJ', mismatches=1)</code>","text":"<p>This is an implementation of TCRnet (TCR neighbour enrichment test) algorithm.  It identifies similar clonotypes for the experimental dataset and the control one based on sequence  similarity (allowing up to <code>mismatches</code> differences).   Args:     clonosets_df_exp (pd.DataFrame): a DataFrame with experimental clonosets containing three columns - <code>sample_id</code> and <code>filename</code> columns,         filename - full path to a clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format     clonosets_df_control (pd.DataFrame): a DataFrame with control clonosets containing three columns - <code>sample_id</code> and <code>filename</code> columns,         filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format     cl_filter (repseq.clone_filter.Filter): A filter applied to the experimental dataset before processing     cl_filter_c (repseq.clone_filter.Filter): A filter applied to the control dataset before processing     overlap_type (str): possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence         to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments         to decide if clonotypes are equal     mismatches (int): Max number of single-letter mismatches in clonotype sequences          for them to be treated similar, i.e. hamming distance.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>fold</code>, <code>p_value_b</code>, <code>p_value_p</code>, <code>p_value_b_adj</code>, <code>p_value_p_adj</code>, <code>log10_b_adj</code>, <code>log10_p_adj</code>, <code>log2_fc</code>. <code>p</code> in <code>p_value</code> </p> <p>stands for <code>poisson</code>, <code>b</code> for <code>binomial</code>, <code>adj</code> for multiple testing correction, and <code>log2_fc</code>for log2 fold change</p> Source code in <code>repseq/intersections.py</code> <pre><code>def tcrnet(clonosets_df_exp, clonosets_df_control, cl_filter=None, cl_filter_c=None, overlap_type=\"aaVJ\", mismatches=1):\n\n    \"\"\"\n    This is an implementation of TCRnet (TCR neighbour enrichment test) algorithm.  It identifies similar clonotypes for the experimental dataset and the control one based on sequence \n    similarity (allowing up to `mismatches` differences).    \n    Args:\n        clonosets_df_exp (pd.DataFrame): a DataFrame with experimental clonosets containing three columns - `sample_id` and `filename` columns,\n            filename - full path to a clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n        clonosets_df_control (pd.DataFrame): a DataFrame with control clonosets containing three columns - `sample_id` and `filename` columns,\n            filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n        cl_filter (repseq.clone_filter.Filter): A filter applied to the experimental dataset before processing\n        cl_filter_c (repseq.clone_filter.Filter): A filter applied to the control dataset before processing\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): Max number of single-letter mismatches in clonotype sequences \n            for them to be treated similar, i.e. hamming distance.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `fold`, `p_value_b`, `p_value_p`, `p_value_b_adj`, `p_value_p_adj`, `log10_b_adj`, `log10_p_adj`, `log2_fc`. `p` in `p_value` \n        stands for `poisson`, `b` for `binomial`, `adj` for multiple testing correction, and `log2_fc`for log2 fold change \n    \"\"\"\n\n    print(\"Running TCRnet neighbour count\\n\"+\"-\"*50)\n    print(f\"Overlap type: {overlap_type}\")\n\n    clonoset_exp = pool_clonotypes_from_clonosets_df(clonosets_df_exp, cl_filter=cl_filter)\n    clonoset_exp_dict = prepare_clonoset_for_intersection(clonoset_exp, overlap_type=overlap_type, by_freq=False, len_vj_format=True)\n\n    unique_clonotypes = [(seq_count[0], *len_vj[1:]) for len_vj, seq_counts in clonoset_exp_dict.items() for seq_count in seq_counts]\n\n    clonoset_control = pool_clonotypes_from_clonosets_df(clonosets_df_control, cl_filter=cl_filter_c)\n    clonoset_control_dict = prepare_clonoset_for_intersection(clonoset_control, overlap_type=overlap_type, by_freq=False, len_vj_format=True)\n\n\n    tasks = []\n    chunks = 40\n    chunk_size = len(unique_clonotypes)//chunks+1\n    for i in range(chunks):\n        first = i*chunk_size\n        last = (i+1)*chunk_size\n        task = (unique_clonotypes[first:last], clonoset_exp_dict, clonoset_control_dict, mismatches)\n        tasks.append(task)\n\n    results = run_parallel_calculation(tcrnet_mp, tasks, \"Calc neighbours (TCRnet)\", object_name=\"parts\")\n    results = list(itertools.chain.from_iterable(results)) # unpack results from several workers\n\n    df = pd.DataFrame(results, columns=[\"clone\", \"count_exp\", \"count_control\", \"group_count_exp\", \"group_count_control\"])\n    df = tcrnet_stats_calc(df)\n    return df\n</code></pre>"},{"location":"functions/#io","title":"io","text":""},{"location":"functions/#io.open_json_report","title":"<code>open_json_report(filename)</code>","text":"<p>Supporting function for <code>read_json_report</code>. Reads the last record from json file.</p> Source code in <code>repseq/io.py</code> <pre><code>def open_json_report(filename):\n    \"\"\"\n    Supporting function for `read_json_report`. Reads the last record from json file.\n    \"\"\"\n\n    with open(filename) as data_file:    \n        for jsonObj in data_file:\n            report = json.loads(jsonObj)\n    return report\n</code></pre>"},{"location":"functions/#io.read_clonoset","title":"<code>read_clonoset(filename)</code>","text":"<p>Reads generic clonoset files.  Easyly reads <code>csv</code>, <code>tsv</code>, <code>txt</code> or <code>gz</code> files. Reads first found file inside <code>zip</code> files. Clonosets should be in tab-separated format: MiXCR (v3 or v4), vdjtools, Bioadaptive</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to clonoset file</p> required <p>Returns:</p> Name Type Description <code>clonoset</code> <code>DataFrame</code> <p>DataFrame representation of clonoset in given file. Bioadaptive clonosets are converted to vdjtools-like format.</p> Source code in <code>repseq/io.py</code> <pre><code>def read_clonoset(filename):\n    \"\"\"\n    Reads generic clonoset files. \n    Easyly reads `csv`, `tsv`, `txt` or `gz` files.\n    Reads first found file inside `zip` files.\n    Clonosets should be in tab-separated format: MiXCR (v3 or v4), vdjtools, Bioadaptive\n\n    Args:\n        filename (str): path to clonoset file\n\n    Returns:\n        clonoset (pd.DataFrame): DataFrame representation of clonoset in given file.\n            Bioadaptive clonosets are converted to vdjtools-like format.\n    \"\"\"\n\n\n    file_name, file_extension = os.path.splitext(filename)\n\n    d_types_mixcr = {'cloneId': int, 'readCount': int, 'readFraction': float,\n                    'uniqueUMICount': int, 'uniqueUMIFraction': float,\n                    'uniqueMoleculeCount': int, 'uniqueMoleculeFraction': float,\n                    'cloneCount': int, 'cloneFraction': float,\n                    'targetSequences': str, 'targetQualities': str,\n                    'allVHitsWithScore': str, 'allDHitsWithScore': str,\n                    'allJHitsWithScore': str, 'allCHitsWithScore': str,\n                    'allVAlignments': str, 'allDAlignments': str,\n                    'allJAlignments': str, 'allCAlignments': str,\n                    'nSeqFR1': str, 'minQualFR1': str,\n                    'nSeqCDR1': str, 'minQualCDR1': str,\n                    'nSeqFR2': str, 'minQualFR2': str,\n                    'nSeqCDR2': str, 'minQualCDR2': str,\n                    'nSeqFR3': str, 'minQualFR3': str,\n                    'nSeqCDR3': str, 'minQualCDR3': str,\n                    'nSeqFR4': str, 'minQualFR4': str,\n                    'aaSeqFR1': str, 'aaSeqCDR1': str,\n                    'aaSeqFR2': str, 'aaSeqCDR2': str,\n                    'aaSeqFR3': str, 'aaSeqCDR3': str,\n                    'aaSeqFR4': str, 'refPoints': str\n                    }\n\n    d_types_vdjtools = {'cdr3aa': str, 'cdr3nt': str,\n                        'v': str, 'd': str, 'j': str,\n                        'CDR3aa': str, 'CDR3nt': str,\n                        'V': str, 'D': str, 'J': str,\n                        'C': str, \"frequency\": float#,\n                        #'count': int, 'freq': float#,\n                        #'VEnd':int, 'DStart':int, 'DEnd':int, \"JStart\":int\n                        }\n\n    d_types_bioadaptive = {'nucleotide': str, 'aminoAcid': str,\n                            'count (templates/reads)': int,\n                            'frequencyCount (%)': float,\n                            'count': int,\n                            'frequencyCount': float,\n                            'vGeneName': str, 'dGeneName': str,\n                            'jGeneName': str, 'cdr3Length': int,\n                            'n1Index': int,'dIndex': int,\n                            'n2Index': int,'jIndex': int\n                            }\n\n\n    datatypes = {**d_types_mixcr,**d_types_vdjtools, **d_types_bioadaptive}\n    if file_extension == \".zip\":\n        archive = zipfile.ZipFile(filename, 'r')\n        inner_filename = zipfile.ZipFile.namelist(archive)[0]\n        filename = archive.open(inner_filename)\n    clonoset = pd.read_csv(filename, sep=\"\\t\", dtype=datatypes)\n    if \"nucleotide\" in clonoset.columns and \"aminoAcid\" in clonoset.columns:\n        clonoset = convert_bioadaptive_clonoset(clonoset)\n    return clonoset\n</code></pre>"},{"location":"functions/#io.read_json_report","title":"<code>read_json_report(sample_id, folder, report_type)</code>","text":"<p>Reads MiXCR4 json reports into a Python mixed data structure. This function takes the last json record, if for example MiXCR adds up several records  to json file (it happens, when the program is rerun several times on the same data). Program also includes cases when Sample-barcodes are used.</p> <p>Parameters:</p> Name Type Description Default <code>sample_id</code> <code>str</code> <p>sample_id used when running the MiXCR program</p> required <code>folder</code> <code>str</code> <p>folder in which the MiXCR output is stored</p> required <code>report_type</code> <code>str</code> <p>align, refine, assemble</p> required <p>Returns:</p> Name Type Description <code>report</code> <code>dict</code> <p>mixed dict/list python structure, representing the json report</p> Source code in <code>repseq/io.py</code> <pre><code>def read_json_report(sample_id, folder, report_type):\n    \"\"\"\n    Reads MiXCR4 json reports into a Python mixed data structure.\n    This function takes the last json record, if for example MiXCR adds up several records \n    to json file (it happens, when the program is rerun several times on the same data).\n    Program also includes cases when Sample-barcodes are used.\n\n    Args:\n        sample_id (str): sample_id used when running the MiXCR program\n        folder (str): folder in which the MiXCR output is stored\n        report_type (str): align, refine, assemble\n\n    Returns:\n        report (dict): mixed dict/list python structure, representing the json report\n    \"\"\"\n\n\n    filename = os.path.join(folder, f\"{sample_id}.{report_type}.report.json\")\n    if \".\" in sample_id:\n        sample_id2 = \".\".join(sample_id.split(\".\")[:-1])\n        filename2 = os.path.join(folder, f\"{sample_id2}.{report_type}.report.json\")\n        try:\n            report = open_json_report(filename)\n        except FileNotFoundError:\n            report = open_json_report(filename2)\n    else:\n        report = open_json_report(filename)\n    return report\n</code></pre>"},{"location":"functions/#io.read_yaml_metadata","title":"<code>read_yaml_metadata(folder, filename='metadata.yaml', verbose=True)</code>","text":"<p>Reads NGSiK metadata from a given folder and converts to <code>pd.DataFrame</code>. By default  it searches for <code>metadata.yaml</code> file in this folder and extracts the table.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>path to NGSiK folder</p> required <code>filename</code> <code>str</code> <p>NGSiK metadata filename</p> <code>'metadata.yaml'</code> <code>verbose</code> <code>bool</code> <p>verbosity for </p> <code>True</code> <p>Returns:</p> Name Type Description <code>sample_df</code> <code>DataFrame</code> <p>extracted DataFrame from metadata</p> Source code in <code>repseq/io.py</code> <pre><code>def read_yaml_metadata(folder, filename=\"metadata.yaml\", verbose=True):\n\n    \"\"\"\n    Reads NGSiK metadata from a given folder and converts to `pd.DataFrame`. By default \n    it searches for `metadata.yaml` file in this folder and extracts the table.\n\n    Args:\n        folder (str): path to NGSiK folder\n        filename (str): NGSiK metadata filename\n        verbose (bool): verbosity for \n\n    Returns:\n        sample_df (pd.DataFrame): extracted DataFrame from metadata\n\n    \"\"\"\n\n\n    most_important_columns = [\"sample_id\", \"R1\", \"R2\",\"libraryPerson\", \"projectPerson\", \"projectName\", \"species\", \"miNNNPattern\", \"SMPL\", \"mix_id\", \"preset\", \"startingMaterial\", \"libraryType\"]\n    yaml_filename = os.path.join(folder, filename)\n\n    if os.path.isfile(yaml_filename):\n        with open(yaml_filename, \"r\") as stream:\n            try:\n                metadata_dict =yaml.safe_load(stream)\n        #         pd.io.json.json_normalize(metadata_dict, \"file\", \"samples\", errors='ignore')\n            except yaml.YAMLError as exc:\n                print(exc)\n\n        df = pd.json_normalize(metadata_dict)\n        df = df.explode(\"file\")\n        df = pd.concat([df.drop(['file'], axis=1), df['file'].apply(pd.Series)], axis=1)\n        df = df.explode(\"samples\")\n        df = pd.concat([df.drop(['samples'], axis=1), df['samples'].apply(pd.Series)], axis=1)\n        if 'patternGroupValues' in df.columns:\n            df = pd.concat([df.drop(['patternGroupValues'], axis=1), df['patternGroupValues'].apply(pd.Series)], axis=1)\n        df[\"R1\"] = df[\"R1\"].apply(lambda x: os.path.join(folder, x))\n        df[\"R2\"] = df[\"R2\"].apply(lambda x: os.path.join(folder, x))\n        df = df.rename(columns={\"name\": \"sample_id\"})\n\n        for col_name in most_important_columns[::-1]:\n            if col_name in df.columns:\n                first_column = df.pop(col_name) \n                df.insert(0, col_name, first_column)\n\n        return df.reset_index(drop=True)\n    else:\n        if verbose:\n            print(f\"Metadata file '{yaml_filename}' not found. Nothing to return\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"functions/#io.vdjdb","title":"<code>vdjdb(vdjdb_dataset=None, drop_method=True, drop_meta=True, drop_cdr3fix=True)</code>","text":"<p>Reads VDJdb datasets and adjusts column names. If <code>vdjdb_dataset</code> is <code>None</code>, the latest version of the whole database is downloaded.</p> <p>Parameters:</p> Name Type Description Default <code>vdjdb_dataset</code> <code>str</code> <p>path to a file exported from VDJdb database in .tsv format.  If set to <code>None</code> (default), all entries from VDJdb will be downloaded.</p> <code>None</code> <code>drop_method</code> <code>bool</code> <p>if <code>True</code>, <code>Method</code> column is excluded</p> <code>True</code> <code>drop_meta</code> <code>bool</code> <p>if <code>True</code>, <code>Meta</code> column is excluded</p> <code>True</code> <code>drop_cdr3fix</code> <code>bool</code> <p>if <code>True</code>, <code>CDR3fix</code> column is excluded</p> <code>True</code> <p>Returns:</p> Name Type Description <code>vdjdb_df</code> <code>DataFrame</code> <p>DataFrame representation of VDJdb entries</p> Source code in <code>repseq/io.py</code> <pre><code>def vdjdb(vdjdb_dataset=None, drop_method=True, drop_meta=True, drop_cdr3fix=True):\n    \"\"\"\n    Reads VDJdb datasets and adjusts column names. If `vdjdb_dataset` is `None`, the latest version of the whole database is downloaded.\n\n    Args:\n        vdjdb_dataset (str): path to a file exported from VDJdb database in .tsv format. \n            If set to `None` (default), all entries from VDJdb will be downloaded.\n        drop_method (bool): if `True`, `Method` column is excluded\n        drop_meta (bool): if `True`, `Meta` column is excluded\n        drop_cdr3fix (bool): if `True`, `CDR3fix` column is excluded\n\n    Returns:\n        vdjdb_df (pd.DataFrame): DataFrame representation of VDJdb entries\n    \"\"\"\n    if vdjdb_dataset:\n        vdjdb_df = pd.read_csv(vdjdb_dataset, delimiter='\\t')\n        renaming_dict = {'complex.id': 'complex_id',\n                    'Gene': 'chain',\n                    'CDR3': 'cdr3aa',\n                    'V': 'v',\n                    'J': 'j',\n                    'Species': 'species',\n                    'MHC A': 'mhc_a',\n                    'MHC B': 'mhc_b',\n                    'MHC class': 'mhc_class',  \n                    'Epitope': 'epitope',\n                    'Epitope gene': 'epitope_gene',\n                    'Epitope species': 'epitope_species',\n                    'Reference': 'reference',\n                    'Score': 'score',\n                    'Method': 'method',\n                    'Meta': 'meta',\n                    'CDR3fix': 'cdr3fix'}\n    else:\n        renaming_dict = {'complex.id': 'complex_id',\n                    'gene': 'gene',\n                    'cdr3': 'cdr3aa',\n                    'v.segm': 'v',\n                    'j.segm': 'j',\n                    'species': 'species',\n                    'mhc.a': 'mhc_a',\n                    'mhc.b': 'mhc_b',\n                    'mhc.class': 'mhc_class',\n                    'antigen.epitope': 'epitope',\n                    'antigen.gene': 'epitope_gene',\n                    'antigen.species': 'epitope_species',\n                    'reference.id': 'reference_id',\n                    'method': 'method',\n                    'meta': 'meta',\n                    'cdr3fix': 'cdr3fix',\n                    'vdjdb.score': 'score'}\n        api_url = \"https://api.github.com/repos/antigenomics/vdjdb-db/releases/latest\"\n        response = requests.get(api_url)\n        vdjdb_release = requests.get(response.json()['assets'][0]['browser_download_url']).content\n        with zipfile.ZipFile(io.BytesIO(vdjdb_release)) as zf:\n            filename = [name for name in zf.namelist() if name.endswith('vdjdb.txt')][0]\n            with zf.open(filename) as f:\n                vdjdb_df = pd.read_csv(f, delimiter='\\t').drop(columns=[\n                                            'web.method', \n                                            'web.method.seq', \n                                            'web.cdr3fix.unmp', \n                                            'web.cdr3fix.unmp',\n                                            'web.cdr3fix.nc'])\n    vdjdb_df = vdjdb_df.rename(columns=renaming_dict)\n    if not drop_method:\n        method = vdjdb_df.pop('method')\n        method_df = pd.DataFrame.from_dict([json.loads(d) for d in method])\n        vdjdb_df = pd.concat([vdjdb_df, method_df], axis=1)\n        # since entries are added manually, frequency isn't uniform\n        freqs = vdjdb_df['frequency']\n\n        is_fraction = freqs.str.contains(r'^\\d+\\s*/\\s*\\d+$', na=False)\n        fractions = freqs[is_fraction].str.extract(r'(\\d+)\\s*/\\s*(\\d+)').astype(float)\n        fraction_values = fractions[0] / fractions[1]\n\n        is_percent = freqs.str.endswith('%', na=False)\n        percent_values = freqs[is_percent].str.rstrip('%').astype(float) / 100\n\n        is_number = freqs.str.match(r'^\\d*\\.?\\d+$', na=False)\n        number_values = freqs[is_number].astype(float)\n\n        frequencies = pd.Series(pd.NA, index=freqs.index)\n        frequencies[is_fraction] = fraction_values.values\n        frequencies[is_percent] = percent_values.values\n        frequencies[is_number] = number_values.values\n        vdjdb_df['frequency'] = frequencies\n    else:\n        vdjdb_df.pop('method')\n\n    if not drop_meta:\n        meta = vdjdb_df.pop('meta')\n        meta_df = pd.DataFrame.from_dict([json.loads(d) for d in meta])\n        meta_df = meta_df.rename(columns=\n                                                                {'study.id': 'study_id',\n                                                                'cell.subset': 'cell_subset',\n                                                                'subject.cohort': 'subject_cohort',\n                                                                'subject.id': 'subject_id',\n                                                                'replica.id': 'replica_id',\n                                                                'clone.id': 'clone_id',\n                                                                'epitope.id': 'epitope_id',\n                                                                'tissue': 'tissue',\n                                                                'donor.MHC': 'donor_mhc',\n                                                                'donor.MHC.method': 'donor_mhc_method',\n                                                                'structure.id': 'structure_id',\n                                                                'samples.found': 'samples_found',\n                                                                'studies.found': 'studies_found'})\n        vdjdb_df = pd.concat([vdjdb_df, meta_df], axis=1)\n    else:\n        vdjdb_df.pop('meta')\n\n    if not drop_cdr3fix:\n        cdr3fix = vdjdb_df.pop('cdr3fix')\n        cdr3fix_df = pd.DataFrame.from_dict([json.loads(d) for d in cdr3fix])\n        cdr3fix_df = cdr3fix_df.rename(columns={'cdr3': 'cdr3aa_new',\n                                                                         'cdr3_old': 'cdr3aa_old'})\n        vdjdb_df = pd.concat([vdjdb_df, cdr3fix_df], axis=1)\n    else:\n        vdjdb_df.pop('cdr3fix')\n    order = ['complex_id', 'chain', 'cdr3aa', 'v', 'j', 'species', 'mhc_a', 'mhc_b',\n       'mhc_class', 'epitope', 'epitope_gene', 'epitope_species', 'reference',\n       'score', 'identification', 'frequency', 'singlecell', 'sequencing',\n       'verification', 'study_id', 'cell_subset', 'subject_cohort',\n       'subject_id', 'replica_id', 'clone_id', 'epitope_id', 'tissue',\n       'donor_mhc', 'donor_mhc_method', 'structure_id', 'samples_found',\n       'studies_found', 'cdr3aa_new', 'cdr3aa_old', 'fixNeeded', 'good',\n       'jCanonical', 'jFixType', 'jId', 'jStart', 'vCanonical', 'vEnd',\n       'vFixType', 'vId']\n    existing_columns = [col for col in order if col in vdjdb_df.columns]\n    vdjdb_df['v'] = vdjdb_df['v'].apply(lambda x: extract_segment(x))\n    vdjdb_df['j'] = vdjdb_df['j'].apply(lambda x: extract_segment(x))\n    def to_numeric(s):\n        try:\n            return pd.to_numeric(s)\n        except ValueError:\n            return s\n    vdjdb_df = vdjdb_df.apply(to_numeric)\n    return vdjdb_df[existing_columns]\n</code></pre>"},{"location":"functions/#logo","title":"logo","text":""},{"location":"functions/#migec","title":"migec","text":""},{"location":"functions/#minnn","title":"minnn","text":""},{"location":"functions/#mixcr","title":"mixcr","text":""},{"location":"functions/#mixcr.get_processing_table","title":"<code>get_processing_table(folder, show_offtarget=False, offtarget_chain_threshold=0.01)</code>","text":"<p>Searches for clonosets in the the folder, extracts their sample_id's and shows main processing stats in a table format. By default does not show \"off-target\" clonosets -  those having less than 1% (default, may be overriden) of reads for the sample_id. For example, you have sequenced TRB sample, but there is found 0.5% (by read count)  of TRA chains for the same sample_id, then the clonoset will not be shown in the table. You can specify <code>show_offtarget=True</code> to display all found chains in the table or  outherwise set a higher value for <code>offtarget_chain_threshold</code> (<code>0.01</code> by default).</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str or list</code> <p>folder or list of folders in which to look for clonosets and processing stats</p> required <code>show_offtarget</code> <code>bool</code> <p>add offtarget chains to the stats</p> <code>False</code> <code>offtarget_chain_threshold</code> <code>float</code> <p>threshold for off-target chains</p> <code>0.01</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe, containing <code>sample_id</code>, <code>extracted_chain</code> and  different processing stats columns. There may be several rows with the same  <code>sample_id</code>, with each found <code>extracted_chain</code></p> Source code in <code>repseq/mixcr.py</code> <pre><code>def get_processing_table(folder, show_offtarget=False, offtarget_chain_threshold=0.01):\n    \"\"\"\n    Searches for clonosets in the the folder, extracts their sample_id's and shows main\n    processing stats in a table format. By default does not show \"off-target\" clonosets - \n    those having less than 1% (default, may be overriden) of reads for the sample_id.\n    For example, you have sequenced TRB sample, but there is found 0.5% (by read count) \n    of TRA chains for the same sample_id, then the clonoset will not be shown in the table.\n    You can specify `show_offtarget=True` to display all found chains in the table or \n    outherwise set a higher value for `offtarget_chain_threshold` (`0.01` by default).\n\n    Args:\n        folder (str or list): folder or list of folders in which to look for clonosets and\n            processing stats\n        show_offtarget (bool): add offtarget chains to the stats\n        offtarget_chain_threshold (float): threshold for off-target chains\n\n    Returns:\n        df (pd.DataFrame): dataframe, containing `sample_id`, `extracted_chain` and \n            different processing stats columns. There may be several rows with the same \n            `sample_id`, with each found `extracted_chain`\n    \"\"\"\n\n    if isinstance(folder, list):\n        tables = []\n        for f in folder:\n            table = get_processing_table(f, show_offtarget=show_offtarget)\n            tables.append(table)\n        return pd.concat(tables).sort_values(by=\"sample_id\").reset_index(drop=True)\n\n    results = []\n    clonosets = find_all_exported_clonosets_in_folder(folder, chain=None)\n\n    for i, r in clonosets.iterrows():\n        sample_id = r[\"sample_id\"]\n        chain = r[\"chain\"]\n        align_report = read_json_report(sample_id, folder, \"align\")\n\n        try:\n            refine_report = read_json_report(sample_id, folder, \"refine\")\n            umi = True\n        except FileNotFoundError:\n            umi = False\n\n        assemble_report = read_json_report(sample_id, folder, \"assemble\")\n\n        # print(sample_id, chain)\n        clonoset = read_clonoset(r.filename)\n        clonoset_f = filter_nonfunctional_clones(clonoset)\n\n        # align report\n        Rt=align_report[\"totalReadsProcessed\"]\n        Ru=align_report[\"totalReadsProcessed\"]-align_report[\"notAlignedReasons\"][\"NoBarcode\"]\n        Ru_pc = round(Ru/Rt*100, 2)\n        Ra=align_report[\"aligned\"]\n        Ra_pc = round(Ra/Rt*100, 2)\n        Roa = align_report[\"overlappedAligned\"]\n        Roa_pc = round(Roa/Ra*100, 2)\n\n        if umi:\n        #Ra2=refine_report[\"correctionReport\"][\"inputRecords\"] ##### differs from Ra, but D.Bolotin did not explain why\n\n            UMIa=refine_report[\"correctionReport\"][\"steps\"][0][\"inputDiversity\"]\n            UMIc=refine_report[\"correctionReport\"][\"steps\"][0][\"outputDiversity\"]\n            try:\n                UMIf=refine_report[\"correctionReport\"][\"filterReport\"][\"numberOfGroupsAccepted\"]\n            except TypeError:\n                UMIf=UMIc\n            Rf=refine_report[\"correctionReport\"][\"outputRecords\"]\n            try:\n                overseq_threshold = int(refine_report[\"correctionReport\"][\"filterReport\"][\"operatorReports\"][0][\"operatorReport\"][\"threshold\"])\n            except TypeError:\n                overseq_threshold = None\n            reads_per_umi = round(Rf/UMIf, 2)\n        else:\n            UMIa = np.nan\n            UMIc = np.nan\n            UMIf = np.nan\n            Rf = np.nan\n            overseq_threshold = np.nan\n            reads_per_umi = np.nan\n\n        Ct=assemble_report[\"clones\"]\n        Rcl=assemble_report[\"readsInClones\"]\n\n        Ctc=len(clonoset)\n        Rclc=int(clonoset.readCount.sum())\n\n        Cfunc=len(clonoset_f)\n        Rfunc=int(clonoset_f.readCount.sum())\n        if umi:\n            UMIcl=clonoset.uniqueMoleculeCount.sum()\n            UMIfunc=clonoset_f.uniqueMoleculeCount.sum()\n        else:\n            UMIcl=np.nan\n            UMIfunc=np.nan\n        if umi and overseq_threshold is None:\n            reads_per_umi = round(Rclc/UMIcl, 2)\n\n        results.append([sample_id, chain, Rt, Ru_pc, Ra_pc, Roa_pc, UMIa, UMIc, overseq_threshold, Rf, UMIf, reads_per_umi, Ct, Rcl, Ctc, Rclc, Cfunc, Rfunc, UMIcl, UMIfunc])\n    result_df = pd.DataFrame(results, columns=[\"sample_id\", \"extracted_chain\", \"reads_total\", \"reads_with_umi_pc\", \"reads_aligned_pc\", \"reads_overlapped_aln_pc\",\n                                               \"total_umi\", \"umi_after_correction\", \"overseq_threshold\", \"reads_after_filter\", \"umi_after_filter\",\n                                               \"reads_per_umi\", \"clones_total\", \"reads_in_clones_total\", \"clones\", \"reads_in_clones\", \"clones_func\", \"reads_in_func_clones\", \"umi_in_clones\", \"umi_in_func_clones\"])\n    if not show_offtarget:\n        result_df = result_df.loc[result_df.reads_in_clones/result_df.reads_in_clones_total &gt; offtarget_chain_threshold]\n    return result_df.sort_values(by=\"sample_id\").reset_index(drop=True)\n</code></pre>"},{"location":"functions/#mixcr.mixcr4_analyze_batch","title":"<code>mixcr4_analyze_batch(sample_df, output_folder, command_template=None, mixcr_path='mixcr', memory=32, time_estimate=1.5, custom_tag_pattern_column=None)</code>","text":"<p>Function for batch runs of MiXCR software using SLURM. For each record in the given <code>sample_df</code> this function creates a SLURM-script in <code>~/temp/slurm</code> folder and adds them to SLURM-queue. All the <code>stdout</code> logs are also  put to <code>~/temp/slurm</code> folder. In case of troubles check the latest logs in this folder.  By default this function uses <code>mixcr analyze</code> command for MiLab Hum RNA TCR Kit (with UMI).  To change the command template use <code>command_template</code> parameter</p> <p>Parameters:</p> Name Type Description Default <code>sample_df</code> <code>DataFrame</code> <p>DataFrame, containing 'sample_id' column and  'R1' and 'R2' columns, containing paths (recommended full paths) to raw read files</p> required <code>output_folder</code> <code>str</code> <p>path to output folder</p> required <code>command_template</code> <code>str</code> <p>MiXCR command template  (default: 'mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix'). May be used as an example. Note that <code>mixcr analyze</code> and <code>r1 r2 output_prefix</code> are  \"magical\" parts of the template that should be kept as-is in the template, so change  only the part in-between these parts.</p> <code>None</code> <code>mixcr_path</code> <code>str</code> <p>path to MiXCR binary</p> <code>'mixcr'</code> <code>memory</code> <code>int</code> <p>required OOM in GB</p> <code>32</code> <code>time_estimate</code> <code>numeric</code> <p>time estimate in hours for the calculation. It is the limit for SLURM task</p> <code>1.5</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def mixcr4_analyze_batch(sample_df, output_folder, command_template=None,\n                         mixcr_path=\"mixcr\", memory=32, time_estimate=1.5, custom_tag_pattern_column=None):\n\n    \"\"\"\n    Function for batch runs of MiXCR software using SLURM.\n    For each record in the given `sample_df` this function creates a SLURM-script in\n    `~/temp/slurm` folder and adds them to SLURM-queue. All the `stdout` logs are also \n    put to `~/temp/slurm` folder. In case of troubles check the latest logs in this folder. \n    By default this function uses `mixcr analyze` command for MiLab Hum RNA TCR Kit (with UMI). \n    To change the command template use `command_template` parameter\n\n    Args:\n        sample_df (pd.DataFrame): DataFrame, containing 'sample_id' column and \n            'R1' and 'R2' columns, containing paths (recommended full paths) to raw read files\n        output_folder (str): path to output folder\n        command_template (str): MiXCR command template \n            (default: 'mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix').\n            May be used as an example. Note that `mixcr analyze` and `r1 r2 output_prefix` are \n            \"magical\" parts of the template that should be kept as-is in the template, so change \n            only the part in-between these parts.\n        mixcr_path (str): path to MiXCR binary\n        memory (int): required OOM in GB\n        time_estimate (numeric): time estimate in hours for the calculation. It\n            is the limit for SLURM task\n\n    Returns:\n        None\n    \"\"\"\n    max_memory = 1500\n    min_memory = 16\n\n    program_name=\"MIXCR4 Analyze Batch\"\n    samples_num = sample_df.shape[0]\n\n    # by default use the most popular preset for MiLaboratory Human TCR UMI MULTIPLEX Kit\n    default_command_template = \"mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix\"\n    if command_template is None:\n        command_template = default_command_template\n\n    # cut placeholders from command template\n    remove_list = [\"mixcr\", \"r1\", \"r2\", \"output_prefix\"]\n    command_template = ' '.join([w for w in command_template.split() if w not in remove_list])\n\n    # check input for custom tag pattern\n    custom_tag_pattern = False\n    if isinstance(custom_tag_pattern_column, str):\n        if custom_tag_pattern_column not in sample_df.columns:\n            raise ValueError(f\"Specified tag-pattern columns '{custom_tag_pattern_column}' is not present in sample_df\")\n        if \"--tag-pattern\" in command_template.split():\n            raise ValueError(f\"Please, remove '--tag-pattern' option from command_template, when you use custom tag-pattern\")\n        custom_tag_pattern = True\n\n    # Create output dir if does not exist\n    os.makedirs(output_folder, exist_ok=True)\n\n    # default mixcr analyze slurm parameters. They are quite excessive, works fine.\n    # time_estimate=1.5\n    cpus=40\n    if not isinstance(memory, int):\n        raise TypeError(\"memory parameter must be an integer\")\n    if memory &lt; min_memory:\n        print(f\"{memory} &lt; than limit ({min_memory}), using {min_memory} GB\")\n        memory = min_memory\n    if memory &gt; max_memory:\n        print(f\"{memory} &gt; than limit ({max_memory}), using {max_memory} GB\")\n        memory = max_memory\n\n\n    # create slurm batch file for progress tracking\n    slurm_batch_filename = os.path.join(output_folder, \"mixcr_analyze_slurm_batch.log\")\n    create_slurm_batch_file(slurm_batch_filename, program_name, samples_num)\n\n    # main cycle by samples\n    for i,r in sample_df.iterrows():\n        sample_id = r[\"sample_id\"]\n        r1 = r[\"R1\"]\n        r2 = r[\"R2\"]\n    #   output_prefix = os.path.join(output_folder, sample_id)\n        output_prefix = sample_id\n        if custom_tag_pattern:\n            tag_pattern = r[custom_tag_pattern_column]\n            command = f'{mixcr_path} -Xmx{memory}g {command_template} --tag-pattern \"{tag_pattern}\" {r1} {r2} {output_prefix}'\n        else:\n            command = f'{mixcr_path} -Xmx{memory}g {command_template} {r1} {r2} {output_prefix}'\n        command = f\"cd {output_folder}; \" + command\n        jobname = f\"mixcr_analyze_{sample_id}\"\n\n        # for batch task finish tracking:\n        command += f'; echo \"{jobname} finished\" &gt;&gt; {slurm_batch_filename}'\n\n        # create slurm script and add job to queue, print stdout of sbatch\n        stdout, stderr = run_slurm_command_from_jupyter(command, jobname, cpus, time_estimate, memory)\n        print(stdout, stderr)\n\n    print(f\"{samples_num} tasks added to slurm queue\\n\")\n    print(f'To see running progress bar run this function in the next jupyter cell:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\", loop=True)')\n    print(f'To see current progress:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\")')\n</code></pre>"},{"location":"functions/#mixcr.mixcr4_reports","title":"<code>mixcr4_reports(folder, mixcr_path='mixcr')</code>","text":"<p>runs <code>mixcr exportQc</code> commands - <code>align</code>, <code>chainUsage</code> and <code>tags</code> in a given folder  for all <code>.clns</code> filenames. <code>align</code> and <code>chainUsage</code> are run twice to create both  <code>svg</code> and <code>pdf</code> files.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>folder in which to run the <code>mixcr exportQc</code> commands</p> required <code>mixcr_path</code> <code>str</code> <p>path to MiXCR binary</p> <code>'mixcr'</code> <p>Returns:     None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def mixcr4_reports(folder, mixcr_path=\"mixcr\"):\n\n    \"\"\"\n    runs `mixcr exportQc` commands - `align`, `chainUsage` and `tags` in a given folder \n    for all `.clns` filenames. `align` and `chainUsage` are run twice to create both \n    `svg` and `pdf` files.\n\n    Args:\n        folder (str): folder in which to run the `mixcr exportQc` commands\n        mixcr_path (str): path to MiXCR binary\n    Returns:\n        None\n\n    \"\"\"\n\n\n    program_name=\"MIXCR4.3 Reports\"\n    time_estimate=1\n    cpus=40\n    memory=32\n\n    # clns_filenames = os.path.join(folder, \"*.clns\")\n    # align_filename = os.path.join(folder, \"alignQc.png\")\n    # chains_filename = os.path.join(folder, \"chainsQc.png\")\n    # tags_filename = os.path.join(folder, \"tagsQc.pdf\")\n    clns_filenames = \"*.clns\"\n    align_filename = \"alignQc.svg\"\n    chains_filename = \"chainsQc.svg\"\n    align_filename_pdf = \"alignQc.pdf\"\n    chains_filename_pdf = \"chainsQc.pdf\"\n    tags_filename = \"tagsQc.pdf\"\n    #tables_filename = os.path.join(folder, \"tables.tsv\")\n    #preproc_filename = os.path.join(folder, \"preproc_tables.tsv\")\n    #postanalysis_filename = os.path.join(folder, \"postanalysis.json\")\n\n\n\n    commands = {\"alignQc\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc align -f {clns_filenames} {align_filename}\",\n                \"chainUsage\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc chainUsage -f {clns_filenames} {chains_filename}\",\n                \"alignQcPDF\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc align -f {clns_filenames} {align_filename_pdf}\",\n                \"chainUsagePDF\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc chainUsage -f {clns_filenames} {chains_filename_pdf}\",\n                \"tagsQc\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc tags -f {clns_filenames} {tags_filename}\"#,\n                #\"postanalysis\": f\"{MIXCR} -Xmx32g postanalysis individual -f --default-downsampling none --default-weight-function umi --only-productive --tables {tables_filename} --preproc-tables {preproc_filename} {clns_filenames} {postanalysis_filename}\"\n               }\n\n\n    commands_num = len(commands)\n\n    slurm_batch_filename = os.path.join(folder, \"mixcr_reports_slurm_batch.log\")\n    create_slurm_batch_file(slurm_batch_filename, program_name, commands_num)\n\n    for jobname, command in commands.items():\n        command += f'; echo \"{jobname} finished\" &gt;&gt; {slurm_batch_filename}'\n        stdout, stderr = run_slurm_command_from_jupyter(command, jobname, cpus, time_estimate, memory)\n        print(stdout, stderr)\n</code></pre>"},{"location":"functions/#mixcr.mixcr_7genes_run_batch","title":"<code>mixcr_7genes_run_batch(sample_df, output_folder, mixcr_path='mixcr', memory=32, time_estimate=1.5)</code>","text":"<p>Function for batch runs of the MiXCR software using the SLURM <code>mixcr analyze</code> command and the <code>Human 7GENES DNA Multiplex</code> MiXCR built-in preset.  Incomplete rearrangements obtained by this kit are also included. For each incomplete rearrangement, unaligned reads from the previous  step are iteratively processed. Each output is stored in a subdirectory named after the corresponding rearrangement. For each record in the given <code>sample_df</code>, this function creates a SLURM script in the <code>~/temp/slurm</code> folder and adds it to the SLURM queue.  All <code>stdout</code> logs are also saved to the <code>~/temp/slurm</code> folder. In case of troubles, check the latest logs in this folder. </p> <p>Parameters:</p> Name Type Description Default <code>sample_df</code> <code>DataFrame</code> <p>DataFrame containing a 'sample_id' column and  'R1' and 'R2' columns containing paths (recommended full paths) to raw read files.</p> required <code>output_folder</code> <code>str</code> <p>Path to the output folder.</p> required <code>mixcr_path</code> <code>str</code> <p>Path to the MiXCR binary.</p> <code>'mixcr'</code> <code>memory</code> <code>int</code> <p>Required OOM in GB.</p> <code>32</code> <code>time_estimate</code> <code>numeric</code> <p>Time estimate in hours for the calculation; it  is the limit for the SLURM task.</p> <code>1.5</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def mixcr_7genes_run_batch(sample_df, output_folder, mixcr_path=\"mixcr\", memory=32, time_estimate=1.5):\n    \"\"\"\n    Function for batch runs of the MiXCR software using the SLURM `mixcr analyze` command and the `Human 7GENES DNA Multiplex` MiXCR built-in preset. \n    Incomplete rearrangements obtained by this kit are also included. For each incomplete rearrangement, unaligned reads from the previous \n    step are iteratively processed. Each output is stored in a subdirectory named after the corresponding rearrangement.\n    For each record in the given `sample_df`, this function creates a SLURM script in the `~/temp/slurm` folder and adds it to the SLURM queue. \n    All `stdout` logs are also saved to the `~/temp/slurm` folder. In case of troubles, check the latest logs in this folder. \n\n    Args:\n        sample_df (pd.DataFrame): DataFrame containing a 'sample_id' column and \n            'R1' and 'R2' columns containing paths (recommended full paths) to raw read files.\n        output_folder (str): Path to the output folder.\n        mixcr_path (str): Path to the MiXCR binary.\n        memory (int): Required OOM in GB.\n        time_estimate (numeric): Time estimate in hours for the calculation; it \n            is the limit for the SLURM task.\n\n    Returns:\n        None\n    \"\"\"\n    # default mixcr analyze slurm parameters. They are quite excessive, works fine.\n    max_memory = 1500\n    min_memory = 16\n    cpus=40\n\n    program_name=\"MIXCR4 Analyze 7genes Batch\"\n    samples_num = sample_df.shape[0]\n\n    # Create output dir if does not exist\n    os.makedirs(output_folder, exist_ok=True)\n\n\n    if not isinstance(memory, int):\n        raise TypeError(\"memory parameter must be an integer\")\n    if memory &lt; min_memory:\n        print(f\"{memory} &lt; than limit ({min_memory}), using {min_memory} GB\")\n        memory = min_memory\n    if memory &gt; max_memory:\n        print(f\"{memory} &gt; than limit ({max_memory}), using {max_memory} GB\")\n        memory = max_memory\n\n    # create slurm batch file for progress tracking\n    slurm_batch_filename = os.path.join(output_folder, \"mixcr_analyze_slurm_batch.log\")\n    create_slurm_batch_file(slurm_batch_filename, program_name, samples_num)\n\n    list_of_incomplete_rearrangements = [\"DJ_TRB\", \"VDD_TRD\", \"DDJ_TRD\", \"DD_TRD\", \"DJ_IGH\", \"VKDE_IGK\", \"CINTRON_KDE_IGK\"]\n\n    # main cycle by samples\n    for i,r in sample_df.iterrows():\n        sample_id = r[\"sample_id\"]\n        r1 = r[\"R1\"]\n        r2 = r[\"R2\"]\n        output_prefix = sample_id\n\n        R1na = f\"{sample_id}_R1_not_aligned.fastq.gz\"\n        R2na = f\"{sample_id}_R2_not_aligned.fastq.gz\"\n\n        commands = [f\"cd {output_folder}\"]\n\n        first_command = f'{mixcr_path} -Xmx{memory}g analyze milab-human-dna-xcr-7genes-multiplex -f --not-aligned-R1 {R1na} --not-aligned-R2 {R2na} {r1} {r2} {output_prefix}'\n        commands.append(first_command)\n\n        for rearrangement in list_of_incomplete_rearrangements:\n\n            # swap r and Rna so we would not implement copy of R_na\n            r1, R1na = R1na, r1\n            r2, R2na = R2na, r2\n\n            output_prefix = os.path.join(rearrangement, sample_id)\n\n            R1na = f\"{output_prefix}_R1_not_aligned.fastq.gz\"\n            R2na = f\"{output_prefix}_R2_not_aligned.fastq.gz\"\n\n            i_r_command = f'{mixcr_path} -Xmx{memory}g analyze generic-amplicon -f --species hs --library {rearrangement} --assemble-clonotypes-by CDR3 --dna --floating-left-alignment-boundary --floating-right-alignment-boundary J -MexportClones.splitFilesBy=[] --not-aligned-R1 {R1na} --not-aligned-R2 {R2na} {r1} {r2} {output_prefix}'\n            commands.append(i_r_command)\n\n        jobname = f\"mixcr_analyze_{sample_id}\"\n\n        # for batch task finish tracking:\n        commands.append(f'echo \"{jobname} finished\" &gt;&gt; {slurm_batch_filename}')\n\n        # join commands by &amp;&amp; so that next command runs if previous was finished without error and add new lines to the script\n        command = \" &amp;&amp; \\\\ \\n\".join(commands)\n\n        # create slurm script and add job to queue, print stdout of sbatch\n        stdout, stderr = run_slurm_command_from_jupyter(command, jobname, cpus, time_estimate, memory)\n        print(stdout, stderr)\n        # print(command)\n    print(f\"{samples_num} tasks added to slurm queue\\n\")\n    print(f'To see running progress bar run this function in the next jupyter cell:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\", loop=True)')\n    print(f'To see current progress:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\")')\n</code></pre>"},{"location":"functions/#mixcr.show_report_images","title":"<code>show_report_images(folder)</code>","text":"<p>This function displays QC images <code>alignQc.svg</code> and <code>chainsQc.svg</code> in Jupyter Notebook. This pictures may be generated by <code>mixcr4_reports</code> function. In case there are no <code>.svg</code> images, the <code>.png</code> images are shown.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>folder in which to look for QC images.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def show_report_images(folder):\n    \"\"\"\n    This function displays QC images `alignQc.svg` and `chainsQc.svg` in Jupyter Notebook.\n    This pictures may be generated by `mixcr4_reports` function.\n    In case there are no `.svg` images, the `.png` images are shown.\n\n    Args:\n        folder (str): folder in which to look for QC images.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    svg_align_filename = os.path.join(folder, \"alignQc.svg\")\n    svg_chain_filename = os.path.join(folder, \"chainsQc.svg\")\n    png_align_filename = os.path.join(folder, \"alignQc.png\")\n    png_chain_filename = os.path.join(folder, \"chainsQc.png\")\n\n    if os.path.exists(svg_align_filename):\n        print(svg_align_filename)\n        display(SVG(filename=svg_align_filename))\n    elif os.path.exists(png_align_filename):\n        print(png_align_filename)\n        display(Image(filename=png_align_filename))\n    else:\n        print(\"No alignQc image found (svg or png)\")\n\n    if os.path.exists(svg_chain_filename):\n        print(svg_chain_filename)\n        display(SVG(filename=svg_chain_filename))\n    elif os.path.exists(png_chain_filename):\n        print(png_chain_filename)\n        display(Image(filename=png_chain_filename))\n    else:\n        print(\"No chainQc image found (svg or png)\")\n</code></pre>"},{"location":"functions/#mixcr.show_report_images_new","title":"<code>show_report_images_new(folder, chart_type='summary', count_type='percent', output_file=None)</code>","text":"<p>Shows quality control reports in MiXCR-like style</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>folder in which to look for QC images.</p> required <code>chart_type</code> <code>str</code> <p>Possible values are <code>summary</code> (corresponds to <code>mixcr exportQc align</code>,  <code>chains</code> (<code>mixcr exportQc chainUsage</code>)</p> <code>'summary'</code> <code>count_type</code> <code>str</code> <p>possible values are: <code>percent</code>, <code>abs</code></p> <code>'percent'</code> <code>output_file</code> <code>str</code> <p>filename ending with '.png' to save an output plot to</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def show_report_images_new(folder, chart_type='summary', count_type='percent', output_file=None):\n    \"\"\"\n    Shows quality control reports in MiXCR-like style\n\n    Args:\n        folder (str): folder in which to look for QC images.\n        chart_type (str): Possible values are `summary` (corresponds to `mixcr exportQc align`, \n            `chains` (`mixcr exportQc chainUsage`)\n        count_type (str): possible values are: `percent`, `abs`\n        output_file (str): filename ending with '.png' to save an output plot to\n\n    Returns:\n        None\n\n    \"\"\"\n    CHAIN_VARIANTS = ['IGH', 'IGK', 'IGL', 'TRA', 'TRB', 'TRD', 'TRL']\n    files = []\n    try:\n        all_files = os.listdir(folder)\n        files = []\n        for f in all_files:\n            match = re.match(r\"(.+)\\.([a-zA-Z0-9_]+)\\.report\\.json\", f)\n            if match is not None:\n                sample_id = match.group(1)\n                report_type = match.group(2)\n                files.append([sample_id, report_type])\n            else:\n                continue\n    except FileNotFoundError:\n        print('No such file or directory')\n    df_list = []\n    for file in files:\n        report_type = file[1]\n        if report_type == 'align':\n            json_report_contents = read_json_report(file[0], folder, report_type=report_type)\n            align_data = json_report_contents['notAlignedReasons']\n            chain_usage_data = json_report_contents['chainUsage']['chains']\n\n            if chart_type == 'summary':\n                renaming_dict = {'NoHits': 'No hits (not TCR/IG?)',\n                                'NoCDR3Parts': 'No CDR3 parts',\n                                'NoVHits': 'No V hits',\n                                'NoJHits': 'No J hits',\n                                'VAndJOnDifferentTargets': 'No target with both V and J',\n                                'LowTotalScore': 'Low total score',\n                                'NoBarcode': 'Absent barcode',\n                                'SampleNotMatched': 'Sample not matched',\n                                }\n                align_df = {}\n                for old, new in renaming_dict.items():\n                    align_df[new] = [align_data.get(old, 0)]\n                align_df['Successfully aligned'] = json_report_contents['aligned']\n                df_list.append(pd.DataFrame(align_df, index=[file[0]])) \n\n            elif chart_type == 'chains':\n                align_df = {}\n                for chain, data in chain_usage_data.items(): \n                    align_df.update({chain: data['total'] - data['nonFunctional'],\n                                f'{chain} (stops)': data['hasStops'],\n                                f'{chain} (OOF)': data['isOOF']})\n                df_list.append(pd.DataFrame(align_df, index=[file[0]]))\n    results = pd.concat(df_list)\n    results = results.sort_index(ascending=False)\n    if count_type == 'percent':\n        results =  results.div(results.sum(axis=1), axis=0) * 100\n    if chart_type == 'summary':\n        order = ['Successfully aligned', \n                 'No hits (not TCR/IG?)', \n                 'No CDR3 parts', \n                 'No V hits', \n                 'No J hits', \n                 'No target with both V and J', \n                 'Low total score', \n                 'Absent barcode'] \n        colors = ['#3ecd8d', '#fed470', '#fda163', '#f36c5a', '#d64470', '#a03080', '#702084', '#451777']\n        colormap = ListedColormap(colors=colors,\n                                    name='mixcr')\n    elif chart_type == 'chains':\n        order = sorted(results.columns)\n        colors = ['#c26a27', '#ff9429', '#ffcb8f', '#a324b2', '#e553e5', '#faaafa', '#ad3757', '#f05670', '#ffadba', \n                                  '#105bcc', '#2d93fa', '#99ccff', '#198020', '#42b842', '#99e099', '#068a94', '#27c2c2', '#90e0e0', \n                                '#5f31cc', '#845cff', '#c1adff']\n        all_variants = ['IGH', 'IGH (stops)', 'IGH (OOF)', 'IGK', 'IGK (stops)', 'IGK (OOF)', 'IGL', 'IGL (stops)', 'IGL (OOF)', 'TRA', 'TRA (stops)', 'TRA (OOF)', 'TRB', 'TRB (stops)', 'TRB (OOF)', 'TRD', 'TRD (stops)', 'TRD (OOF)', 'TRG', 'TRG (stops)', 'TRG (OOF)']\n        # colormap = ListedColormap(colors=[colors[i] for i in range(len(colors)) if all_variants[i] in results.columns],\n                                    #   name='mixcr')        \n        colors = [colors[i] for i in range(len(colors)) if all_variants[i] in results.columns]\n    results = results[order]\n    size = results.shape[0]\n    # ax = results.plot.barh(width=0.85, figsize=(9, size * 0.5),  stacked=True, colormap=colormap)\n    bar_height = 0.85\n    min_size = 7\n    min_size_2 = 10\n    plot_rows = max(size, min_size)\n    if size &gt; min_size:\n            plot_rows = max(size, min_size_2)\n    fig, ax = plt.subplots(figsize=(9, plot_rows * bar_height * 0.5), dpi=100, constrained_layout=True)\n    ax.set_ylim(-0.5, results.shape[0] - 0.5)\n    bottom = np.zeros(len(results))\n    for i, column in enumerate(results.columns):\n        values = results[column].values\n        ax.barh(\n            y=results.index,\n            width=values,\n            height=bar_height,\n            left=bottom,\n            label=column,\n            color=colors[i],\n        )\n        bottom = [b + v for b, v in zip(bottom, values)]\n    if count_type == 'percent':\n        ax.set_xlabel('%')\n    else:\n        ax.set_xlabel('read count')\n    if chart_type == 'summary':\n        fig.legend(loc='outside upper center',  title='Alignments rate', ncol=3, frameon=False)\n    elif chart_type == 'chains':\n        fig.legend(loc='outside upper center',  title='Clonal chain usage', ncol=3, frameon=False)\n    # plt.tight_layout()\n    sns.despine(left=True, bottom=True)\n    plt.show()\n    if output_file is not None:\n        ax.get_figure().savefig(output_file, bbox_inches='tight')\n    return\n</code></pre>"},{"location":"functions/#pgen_calculation","title":"pgen_calculation","text":"<p>calculates probability of clonotype generation using OLGA model</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>An output of read_clonoset() function </p> required <code>overlap_type</code> <code>str</code> <p>Possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaVJ'</code> <code>mismatches</code> <code>int</code> <p>number of mismatches in heighbors CDR3 sequence, default=1</p> <code>1</code> <code>generation_model</code> <code>str</code> <p>generation model used by OLGA. Possible values are <code>human_B_heavy</code>, <code>human_B_kappa</code>,  <code>human_B_lambda</code>, <code>human_T_alpha</code>, <code>human_T_beta</code>, <code>mouse_T_alpha</code>, <code>mouse_T_beta</code>, default='human_T_beta'</p> <code>'human_T_beta'</code> <p>Returns:</p> Name Type Description <code>clonosets_df</code> <code>DataFrame</code> <p>a dataframe with calculated p_gens for each clonotype</p> Source code in <code>repseq/pgen_calculation.py</code> <pre><code>def calculate_clonotypes_pgen(clonosets_df, cl_filter=None, overlap_type='aaVJ', mismatches=1, generation_model='human_T_beta', olga_warnings=False):\n\n    '''\n    calculates probability of clonotype generation using OLGA model\n\n    Args:\n        clonosets_df (pd.DataFrame): An output of read_clonoset() function \n        overlap_type (str): Possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): number of mismatches in heighbors CDR3 sequence, default=1\n        generation_model (str): generation model used by OLGA. Possible values are `human_B_heavy`, `human_B_kappa`, \n            `human_B_lambda`, `human_T_alpha`, `human_T_beta`, `mouse_T_alpha`, `mouse_T_beta`, default='human_T_beta'\n\n\n    Returns: \n        clonosets_df (pd.DataFrame): a dataframe with calculated p_gens for each clonotype\n    '''\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    pgen_model = create_olga_model(generation_model)\n\n    if not cl_filter:\n        cl_filter = clf.Filter(functionality=\"a\")\n    clns_olga = cl_filter.apply(clonosets_df)\n\n    # creating tuples of size 1-3 each according to the overlap_type \n    overlap_type_vars = []\n    if aa:\n        overlap_type_vars.append('cdr3aa')\n    else:\n        overlap_type_vars.append('cdr3nt')\n    if check_v:\n        overlap_type_vars.append('v')\n    if check_j:\n        overlap_type_vars.append('j')\n\n    # creating neighbours, a {(cdr3, v if required, j if required): [neighbours]} dict, and a {neighbour: [(cdr3, v if required, j if required)]} dict\n    clonoset_seqs = list(clns_olga[overlap_type_vars].itertuples(index=False, name=None))\n    clonoset_seqs_for_calculation = []\n    if len(overlap_type_vars) != 3:\n        for cln in clonoset_seqs:\n            if len(cln) == 2:\n                seq, segment = cln\n                v, j = segment, segment\n            elif len(cln) == 1:\n                seq = cln[0]\n            if not check_v:\n                v = None\n            if not check_j:\n                j = None\n            clonoset_seqs_for_calculation.append((seq, v, j))\n    else:\n        clonoset_seqs_for_calculation = clonoset_seqs\n\n    # create a temporary column to merge p_gen values with respective clonotypes\n    clonoset_total_pgen = clns_olga.copy()\n    clonoset_total_pgen['temp'] = clonoset_seqs\n    neighbours, cln_neighbours_dict = create_neighbours(clonoset_seqs, mismatches, check_v, check_j)\n\n    clonotype_pgen_dict = {}\n    neighbours_pgen_dict = {}\n\n    n_chunks = 40\n    chunk_size = len(clonoset_seqs_for_calculation) // n_chunks + 1\n    tasks_clns = []\n    for i in range(0, len(clonoset_seqs_for_calculation), chunk_size):\n        if i + chunk_size &lt; len(clonoset_seqs_for_calculation):\n            task = (clonoset_seqs_for_calculation[i:i + chunk_size], aa, pgen_model, olga_warnings)\n        else:\n            task = (clonoset_seqs_for_calculation[i:], aa, pgen_model, olga_warnings)\n        tasks_clns.append(task)\n\n    clonotype_pgen = run_parallel_calculation(calculate_pgen_mp, \n                                              tasks_clns, \n                                              'Calculating p_gen using OLGA', \n                                              'chunks_clonoset')\n    for el in clonotype_pgen:\n        clonotype_pgen_dict.update(el)\n    if len(overlap_type_vars) != 3:\n        clonotype_pgen_dict = {tuple([x for x in ct if x is not None]):pgen for ct, pgen in clonotype_pgen_dict.items()}\n\n    n_chunks = 40\n    chunk_size = len(neighbours) // n_chunks + 1\n    tasks_nb = []\n    for i in range(0, len(neighbours), chunk_size):\n        if i + chunk_size &lt; len(neighbours):\n            task = (neighbours[i:i + chunk_size], aa, pgen_model, olga_warnings)\n        else:\n            task = (neighbours[i:], aa, pgen_model, olga_warnings)\n        tasks_nb.append(task)\n\n    neighbours_pgen = run_parallel_calculation(calculate_pgen_mp, \n                                                tasks_nb, \n                                                'Calculating p_gen using OLGA', \n                                                'chunks_neighbors')\n    for el in neighbours_pgen:\n        neighbours_pgen_dict.update(el)\n\n    # matching (neighbor, pgen) pairs with their respective clonotypes\n    clonotype_neighbours_pgen = {ct: [(n, neighbours_pgen_dict[n]) for n in ns] \n                                 for ct, ns in cln_neighbours_dict.items()}\n\n    # pgen = sum(neighbors_pgen) - (C(len(cdr3),mismatches) - 1) * pgen(clonotype)\n    clonoset_total_pgen_to_add = {'temp': [], 'p_gen': []}\n    for ct, ns in clonotype_neighbours_pgen.items():\n        ns_pgen_sum = sum([pgen[1] for pgen in ns])\n        n_overlaps = comb(len(ct[0]), mismatches) - 1\n        pgen_ct = clonotype_pgen_dict[ct]\n        clonoset_total_pgen_to_add['temp'].append(ct)\n        clonoset_total_pgen_to_add['p_gen'].append(ns_pgen_sum - n_overlaps * pgen_ct)\n\n    clonoset_total_pgen_to_add = pd.DataFrame(clonoset_total_pgen_to_add)\n\n    result_df = pd.merge(clonoset_total_pgen_to_add, clonoset_total_pgen, how='right', on=['temp'])\n    result_df.drop('temp', axis=1, inplace=True)\n    pgen = result_df.pop('p_gen')\n    result_df['pgen'] = pgen\n    return result_df\n</code></pre>"},{"location":"functions/#processing_stats","title":"processing_stats","text":""},{"location":"functions/#segment_usage","title":"segment_usage","text":""},{"location":"functions/#slurm","title":"slurm","text":""},{"location":"functions/#stats","title":"stats","text":""},{"location":"functions/#stats.calc_clonoset_stats","title":"<code>calc_clonoset_stats(clonosets_df, cl_filter=None, verbose=True)</code>","text":"<p>Calculates statistics for clonosets regarding clonotype, read and UMI counts. Also gives counts for functional clonotypes and non-singletons: clonotypes,  having only one count (UMI count if present, read count in other cases). Clonosets are given to the function in a form of pd.DataFrame</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>dataframe, containing two required columns:  <code>sample_id</code> and <code>filename</code>. Also recommended to have <code>chain</code> column in this DF.</p> required <code>cl_filter</code> <code>Filter</code> <p>clonoset filter - object from <code>clone_filter.py</code> module.</p> <code>None</code> <p>Returns:</p> Type Description <p>pd.DataFrame: dataframe with clonotype statistics for each sample in clonosets_df</p> Source code in <code>repseq/stats.py</code> <pre><code>def calc_clonoset_stats(clonosets_df, cl_filter=None, verbose=True):\n    \"\"\"\n    Calculates statistics for clonosets regarding clonotype, read and UMI counts.\n    Also gives counts for functional clonotypes and non-singletons: clonotypes, \n    having only one count (UMI count if present, read count in other cases).\n    Clonosets are given to the function in a form of pd.DataFrame\n\n    Args:\n        clonosets_df (pd.DataFrame): dataframe, containing two required columns: \n            `sample_id` and `filename`. Also recommended to have `chain` column in this DF.\n        cl_filter (Filter): clonoset filter - object from `clone_filter.py` module.\n\n    Returns:\n        pd.DataFrame: dataframe with clonotype statistics for each sample in clonosets_df\n    \"\"\"\n\n    df = generic_calculation(clonosets_df, calculate_clonoset_stats_cl, clonoset_filter=cl_filter, program_name=\"CalcClonosetStats\", verbose=verbose)\n    convert_dict = {\"clones\": int,\n                    \"clones_func\": int,\n                    \"clones_func_singletons\": int,\n                    \"clones_func_non_singletons\": int,\n                    \"clones_nonfunc\": int,\n                    \"reads\": int,\n                    \"reads_func\": int,\n                    \"reads_nonfunc\": int}\n    if not df[\"umi\"].isnull().values.any():\n        convert_dict.update({\"umi\": int,\n                        \"umi_func\": int,\n                        \"umi_nonfunc\": int})\n\n    df = df.astype(convert_dict)\n    return df\n</code></pre>"},{"location":"functions/#stats.calc_diversity_stats","title":"<code>calc_diversity_stats(clonosets_df, cl_filter=None, iterations=3, seed=None, drop_small_samples=True)</code>","text":"<p>Calculates <code>observed diversity</code>, <code>Shannon-Wiener</code> and <code>normalized Shannon-Wiener</code> index for each clonoset in <code>clonosets_df</code>. <code>observed diversity</code> - total number of unique clonotypes in a given clonoset <code>Shannon-Wiener</code> - mixed evenness and diversity metric <code>normalized Shannon-Wiener</code> - evenness metric. It is highly recommnded to use equal downsampling for all input clonosets for </p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>dataframe, containing two required columns:  <code>sample_id</code> and <code>filename</code>. Also recommended to have <code>chain</code> column in this DF.</p> required <code>segment</code> <code>str</code> <p>possible values are <code>v</code>, <code>j</code> or <code>c</code>. Defaults to \"v\".</p> required <code>cl_filter</code> <code>Filter</code> <p>clonoset filter - object from <code>clone_filter.py</code> module.</p> <code>None</code> <code>table</code> <code>str</code> <p>table type - <code>long</code> or <code>wide</code>. Defaults to \"long\".</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: 'long' or 'wide'. If 'long' it contains four columns, as stated in the function description. If 'wide' - then it has all possible segments in  column names, sample_id's - in rows and usage in each cell in the table.</p> Source code in <code>repseq/stats.py</code> <pre><code>def calc_diversity_stats(clonosets_df, cl_filter=None, iterations=3, seed=None, drop_small_samples=True):\n    \"\"\"\n    Calculates `observed diversity`, `Shannon-Wiener` and `normalized Shannon-Wiener` index for\n    each clonoset in `clonosets_df`.\n    `observed diversity` - total number of unique clonotypes in a given clonoset\n    `Shannon-Wiener` - mixed evenness and diversity metric\n    `normalized Shannon-Wiener` - evenness metric.\n    It is highly recommnded to use equal downsampling for all input clonosets for \n\n    Args:\n        clonosets_df (pd.DataFrame): dataframe, containing two required columns: \n            `sample_id` and `filename`. Also recommended to have `chain` column in this DF.\n        segment (str, optional): possible values are `v`, `j` or `c`. Defaults to \"v\".\n        cl_filter (Filter, optional): clonoset filter - object from `clone_filter.py` module.\n        table (str, optional): table type - `long` or `wide`. Defaults to \"long\".\n\n\n    Returns:\n        pd.DataFrame: 'long' or 'wide'. If 'long' it contains four columns, as stated in\n            the function description. If 'wide' - then it has all possible segments in \n            column names, sample_id's - in rows and usage in each cell in the table.\n    \"\"\"\n    df = generic_calculation(clonosets_df, calculate_diversity_stats_cl, clonoset_filter=cl_filter,\n                             program_name=\"CalcDiversityStats\", iterations=iterations, seed=seed, drop_small_samples=drop_small_samples)\n    return df\n</code></pre>"},{"location":"functions/#stats.calc_segment_usage","title":"<code>calc_segment_usage(clonosets_df, segment='v', cl_filter=None, table='long', by_count=False)</code>","text":"<p>Calculates segment (<code>V</code>, <code>J</code>, or <code>C</code>) usage for several samples. By default outputs 'long' table with four columns: segment name, <code>usage</code>, <code>sample_id</code> and <code>chain</code>. It also may take a clone_filter as input: <code>cl_filter</code> from <code>clone_filter</code> module.</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>dataframe, containing two required columns:  <code>sample_id</code> and <code>filename</code>. Also recommended to have <code>chain</code> column in this DF.</p> required <code>segment</code> <code>str</code> <p>possible values are <code>v</code>, <code>j</code> or <code>c</code>. Defaults to \"v\".</p> <code>'v'</code> <code>cl_filter</code> <code>Filter</code> <p>clonoset filter - object from <code>clone_filter.py</code> module.</p> <code>None</code> <code>table</code> <code>str</code> <p>table type - <code>long</code> or <code>wide</code>. Defaults to \"long\".</p> <code>'long'</code> <p>Returns:</p> Type Description <p>pd.DataFrame: 'long' or 'wide'. If 'long' it contains four columns, as stated in the function description. If 'wide' - then it has all possible segments in  column names, sample_id's - in rows and usage in each cell in the table.</p> Source code in <code>repseq/stats.py</code> <pre><code>def calc_segment_usage(clonosets_df, segment=\"v\", cl_filter=None, table=\"long\", by_count=False):\n    \"\"\"\n    Calculates segment (`V`, `J`, or `C`) usage for several samples. By default outputs\n    'long' table with four columns: segment name, `usage`, `sample_id` and `chain`.\n    It also may take a clone_filter as input: `cl_filter` from `clone_filter` module.\n\n\n    Args:\n        clonosets_df (pd.DataFrame): dataframe, containing two required columns: \n            `sample_id` and `filename`. Also recommended to have `chain` column in this DF.\n        segment (str, optional): possible values are `v`, `j` or `c`. Defaults to \"v\".\n        cl_filter (Filter, optional): clonoset filter - object from `clone_filter.py` module.\n        table (str, optional): table type - `long` or `wide`. Defaults to \"long\".\n\n\n    Returns:\n        pd.DataFrame: 'long' or 'wide'. If 'long' it contains four columns, as stated in\n            the function description. If 'wide' - then it has all possible segments in \n            column names, sample_id's - in rows and usage in each cell in the table.\n    \"\"\"\n\n\n    if cl_filter is None:\n        cl_filter = Filter()\n\n    table_options = [\"long\", \"wide\"]\n    if table not in table_options:\n        raise ValueError(f\"Unknown value for 'table' parameter. Possible options: {', '.join(table_options)}\")\n\n    possible_segments = [\"v\", \"j\", \"c\", \"vj\", \"vlen\", \"vjlen\"]\n    segment = segment.lower()\n    if segment not in possible_segments:\n        raise ValueError(f\"Wrong segment value. Possible values: {', '.join(possible_segments)}\")\n    df = generic_calculation(clonosets_df, calc_segment_usage_cl, clonoset_filter=cl_filter, program_name=\"CalcSegmentUsage\", segment=segment, by_count=by_count)\n    df = df.fillna(0)\n    if table == \"wide\":\n        return df\n    else:\n        return df.melt(id_vars=[\"sample_id\", \"chain\"]).rename(columns={\"value\":\"usage\", \"variable\":segment})\n</code></pre>"},{"location":"functions/#stats.generic_calculation","title":"<code>generic_calculation(clonosets_df_in, calc_function, clonoset_filter=None, program_name='Calculation', iterations=1, seed=None, drop_small_samples=False, verbose=True, skip_checks=False, **kwargs)</code>","text":"<p>Main function that applies batch calculations for multiple clonosets using a <code>calc_function</code>. It checks inputs, checks if clonotype counts are  coherent with downsample or top numbers in clonoset filter. All filters and calc_function are applied to each clonoset in parallel, if several cores are available. After all calculations are finished, this function combines the results into  one pd.DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>dataframe, containing two required columns:  <code>sample_id</code> and <code>filename</code>. Also recommended to have <code>chain</code> column in this DF. <code>sample_id</code>'s or <code>sample_id</code>+<code>chain</code> combinations must be unique in this DF.</p> required <code>calc_function</code> <code>function_name</code> <p>function, applicable to a single clonoset (pd.DataFrame) in VDJtools-like format that returns a dictionary of properties+values as output.</p> required <code>cl_filter</code> <code>Filter</code> <p>clonoset filter - object from <code>clone_filter.py</code> module.</p> required <code>program_name</code> <code>str</code> <p>the name of applied calculation, it is shown in the progress-bar</p> <code>'Calculation'</code> <code>iterations</code> <code>int</code> <p>number of iterations to obtain mean values for calculations when random processes (downsampling/mix_tails) in clonoset filter are applied. Recommended to use 3-5 iterations.</p> <code>1</code> <code>seed</code> <code>hashable</code> <p>seed for random events (downsampling/mix_tails). It overrides the  values, specified in <code>cl_filter</code>.</p> <code>None</code> <code>drop_small_samples</code> <code>bool</code> <p><code>True</code> - samples, that can't be downsampled or top-cropped because of lack of counts/clonotypes will be dropped before the calculation. <code>False</code> - small samples will be taken into account, but with fewer counts/clonotypes  than those with enough counts/clonotypes.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>resulting DataFrame, with <code>sample_id</code> and <code>chain</code> columns and properties columns for each clonoset.</p> Source code in <code>repseq/stats.py</code> <pre><code>def generic_calculation(clonosets_df_in, calc_function, clonoset_filter=None, program_name=\"Calculation\", iterations=1, seed=None, drop_small_samples=False, verbose=True, skip_checks=False, **kwargs):\n    '''\n    Main function that applies batch calculations for multiple clonosets\n    using a `calc_function`. It checks inputs, checks if clonotype counts are \n    coherent with downsample or top numbers in clonoset filter.\n    All filters and calc_function are applied to each clonoset in parallel, if\n    several cores are available.\n    After all calculations are finished, this function combines the results into \n    one pd.DataFrame.\n\n    Args:\n        clonosets_df (pd.DataFrame): dataframe, containing two required columns: \n            `sample_id` and `filename`. Also recommended to have `chain` column in this DF.\n            `sample_id`'s or `sample_id`+`chain` combinations must be unique in this DF.\n        calc_function (function_name): function, applicable to a single clonoset (pd.DataFrame)\n            in VDJtools-like format that returns a dictionary of properties+values as output.\n        cl_filter (Filter, optional): clonoset filter - object from `clone_filter.py` module.\n        program_name (str): the name of applied calculation, it is shown in the progress-bar\n        iterations (int): number of iterations to obtain mean values for calculations when\n            random processes (downsampling/mix_tails) in clonoset filter are applied.\n            Recommended to use 3-5 iterations.\n        seed (hashable): seed for random events (downsampling/mix_tails). It overrides the \n            values, specified in `cl_filter`.\n        drop_small_samples (bool): `True` - samples, that can't be downsampled or top-cropped\n            because of lack of counts/clonotypes will be dropped before the calculation.\n            `False` - small samples will be taken into account, but with fewer counts/clonotypes \n            than those with enough counts/clonotypes.\n\n    Returns:\n        df (pd.DataFrame): resulting DataFrame, with `sample_id` and `chain` columns and properties\n            columns for each clonoset.\n\n    '''\n\n\n    columns_retain = [\"sample_id\"]\n    clonosets_df = clonosets_df_in.copy()\n\n    if \"sample_id\" not in clonosets_df.columns:\n        raise ValueError(\"Clonoset_df does not contain required column 'sample_id'\")\n    if \"filename\" not in clonosets_df.columns:\n        raise ValueError(\"Clonoset_df does not contain required column 'filename'\")\n\n    split_chain_after_calculation = False\n    if \"chain\" not in clonosets_df.columns:\n        if len(clonosets_df) != len(clonosets_df.sample_id.unique()):\n            raise ValueError(\"Clonoset_df contains nonunique sample_ids\")\n    else:\n        columns_retain.append(\"chain\")\n        if len(clonosets_df[[\"sample_id\", \"chain\"]].drop_duplicates()) != len(clonosets_df):\n            raise ValueError(\"Clonoset_df contains nonunique sample_id+chain combinations\")\n        if len(clonosets_df) != len(clonosets_df.sample_id.unique()):\n            clonosets_df[\"sample_id\"] = clonosets_df[\"sample_id\"] + \"_\" + clonosets_df[\"chain\"]\n            split_chain_after_calculation = True\n\n    random_filter = False\n    need_downsample = False\n    need_top = False\n\n    count_column_by_umi_and_functionality = {\n            True: {\"a\": \"umi\",\n                   \"f\": \"umi_func\",\n                   \"n\": \"umi_nonfunc\"},\n            False: {\"a\": \"reads\",\n                    \"f\": \"reads_func\",\n                    \"n\": \"reads_nonfunc\"}\n        }\n\n    clone_column_by_functionality = {\n        \"a\": \"clones\",\n        \"f\": \"clones_func\",\n        \"n\": \"clones_nonfunc\"\n    }\n\n    exclude_samples = set()\n\n\n    if clonoset_filter is not None and not skip_checks:\n        if isinstance(clonoset_filter.downsample_size, int):\n            need_downsample = True\n        if isinstance(clonoset_filter.top, int):\n            need_top = True\n        if need_downsample or need_top:\n            print(\"Calcultating stats for original clonosets\\n\" + \"_\"*41)\n            stats = calc_clonoset_stats(clonosets_df, verbose=verbose)\n            downsample_column = count_column_by_umi_and_functionality[clonoset_filter.by_umi][clonoset_filter.functionality]\n            read_column = count_column_by_umi_and_functionality[False][clonoset_filter.functionality]\n            top_column = clone_column_by_functionality[clonoset_filter.functionality]\n\n        if need_downsample:\n            count_by_reads_samples = set()\n            if stats[downsample_column].isnull().any().any():\n                nan_downsample_samples = list(stats[stats[downsample_column].isna()].sample_id)\n                print(f\"WARNING! Following samples have NaN downsample counts ('{downsample_column}'): {', '.join(nan_downsample_samples)}\")\n                if clonoset_filter.by_umi:\n                    print(\"These samples will be counted by reads instead\")\n                    count_by_reads_samples = set(nan_downsample_samples)\n                else:\n                    print(\"These samples will be excluded from further calculations.\")\n                    exclude_samples.update(nan_downsample_samples)\n            not_enough_count_df = stats[(stats[downsample_column] &lt; clonoset_filter.downsample_size) &amp; (~stats.sample_id.isin(count_by_reads_samples)) |\n                                        (stats[read_column] &lt; clonoset_filter.downsample_size) &amp; (stats.sample_id.isin(count_by_reads_samples))]\n            if len(not_enough_count_df) &gt; 0:\n                not_enough_count_samples = list(not_enough_count_df.sample_id)\n                if not drop_small_samples:\n                    print(f\"WARNING! Following samples have not enough downsample counts ('{downsample_column}' &lt; {clonoset_filter.downsample_size}): {', '.join(not_enough_count_samples)}\")\n                    print(\"These samples will be excluded from further calculations.\")\n                    print(\"To suppress the warnings set drop_small_samples=True\")\n                exclude_samples.update(not_enough_count_samples)\n        if need_top:\n            if stats[downsample_column].isnull().any().any():\n                nan_downsample_samples = list(stats[stats[downsample_column].isna()].sample_id)\n                print(f\"WARNING! Following samples have NaN counts ('{downsample_column}'): {', '.join(nan_downsample_samples)}\")\n                if clonoset_filter.by_umi:\n                    print(\"These samples will be counted by reads instead\")\n                else:\n                    print(\"These samples will be excluded from further calculations.\")\n                    exclude_samples.update(nan_downsample_samples)\n            not_enough_clones_df = stats[stats[top_column] &lt; clonoset_filter.top]\n            if len(not_enough_clones_df) &gt; 0:\n                not_enough_clones_samples = list(not_enough_clones_df.sample_id)\n                if not drop_small_samples:\n                    print(f\"WARNING! Following samples have not enough clonotypes ('{top_column}' &lt; {clonoset_filter.top}): {', '.join(not_enough_clones_samples)}\")\n                    print(\"These samples will be excluded from further calculations.\")\n                    print(\"To suppress the warnings set drop_small_samples=True\")\n                exclude_samples.update(not_enough_clones_samples)\n\n\n        if isinstance(clonoset_filter.downsample_size, int) or (isinstance(clonoset_filter.top, int) and clonoset_filter.mix_tails):\n            random_filter = True\n\n\n    if random_filter and seed is None:\n        print(\"WARNING! Random filter is applied, but random seed is not set. This may lead to non-reproducible results.\")\n        print(\"You may set the seed (of any hashable type) by specifying 'seed='\")\n\n    tasks = []\n    for i,r in clonosets_df.iterrows():\n        sample_id = r[\"sample_id\"]\n        filename = r[\"filename\"]\n        if sample_id in exclude_samples:\n            continue\n        if clonoset_filter is not None:\n            task = (sample_id, filename, calc_function, clonoset_filter.spawn(), iterations, seed, program_name, random_filter, kwargs)\n        else:\n            task = (sample_id, filename, calc_function, None, iterations, seed, program_name, random_filter, kwargs)\n        tasks.append(task)\n\n    results = run_parallel_calculation(perform_generic_calculation_mp, tasks, program_name, object_name=\"calcultaion(s)\", verbose=verbose)\n    clonosets_df = clonosets_df[columns_retain]\n    df = clonosets_df.merge(pd.DataFrame(results), how=\"left\")\n    if split_chain_after_calculation:\n        df[\"sample_id\"] = df[\"sample_id\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n    return df\n</code></pre>"},{"location":"functions/#stats.perform_generic_calculation_mp","title":"<code>perform_generic_calculation_mp(args)</code>","text":"<p>A single-core \"worker\" for <code>generic_calculation</code> function. It applies clonoset filter (several times if <code>iterations</code> &gt; 1) and performes a calculation for a single filtered clonoset.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>tuple</code> <p>tuple, containing all required parameters</p> required <p>Returns:</p> Name Type Description <code>clonoset_result</code> <code>dict</code> <p>a dictionary of parameters, calculated for given clonoset and averaged for iterations (if &gt; 1)</p> Source code in <code>repseq/stats.py</code> <pre><code>def perform_generic_calculation_mp(args):\n    '''\n    A single-core \"worker\" for `generic_calculation` function.\n    It applies clonoset filter (several times if `iterations` &gt; 1) and performes a\n    calculation for a single filtered clonoset.\n\n\n    Args:\n        args (tuple): tuple, containing all required parameters\n\n    Returns:\n        clonoset_result (dict): a dictionary of parameters, calculated for given clonoset\n            and averaged for iterations (if &gt; 1)\n    '''\n\n\n    (sample_id, filename, calc_function, clonoset_filter, iterations, seed, program_name, random_filter, kwargs) = args\n    clonoset = read_clonoset(filename)\n    colnames = get_column_names_from_clonoset(clonoset)\n\n    if random_filter and isinstance(seed, int):\n        random.seed(seed)\n\n    clonoset_result = {\"sample_id\": sample_id}\n    clonoset_results = []\n    filtered_clonosets = []\n\n    for i in range(iterations):\n        if clonoset_filter is not None:\n            filtered_clonoset = clonoset_filter.apply(clonoset, colnames=colnames)\n        else:\n            filtered_clonoset = clonoset\n        filtered_clonosets.append(filtered_clonoset)\n\n    for filtered_clonoset in filtered_clonosets:\n        clonoset_results.append(calc_function(filtered_clonoset, **kwargs))\n    clonoset_result.update(pd.DataFrame(clonoset_results).mean().to_dict())\n\n    return clonoset_result\n</code></pre>"},{"location":"functions/#tcrdist_clustering","title":"tcrdist_clustering","text":""},{"location":"functions/#tcrdist_clusters_slurm","title":"tcrdist_clusters_slurm","text":""},{"location":"functions/#vdjtools","title":"vdjtools","text":""},{"location":"installation/","title":"Installation","text":"<p>PyPI</p> <p>Currently, a more straightforward installation using PyPI (The Python Package Index) isn't available, however, this feature is planned for future releases. </p>"},{"location":"installation/#library-installation","title":"Library installation","text":"<p>Clone the library into a designated directory: <code>git clone https://github.com/mmjmike/repseq</code></p>"},{"location":"installation/#library-update","title":"Library update","text":"<p>To update the library, use <code>git pull -p</code> inside its folder</p> <p>For instance, if <code>~/soft</code> directory is used:     <code>mkdir ~/soft</code> <code>cd ~/soft</code> <code>git clone https://github.com/mmjmike/repseq</code> <code>cd ~/soft/repseq</code> <code>git pull -p</code></p>"},{"location":"installation/#setting-up-the-environment","title":"Setting up the environment","text":"<p>To resolve all dependencies, you need to set up a Conda environment.</p> <ul> <li>The .yml environemnt file (<code>main_repseq.yml</code>) should be inside 'repseq' folder after the library installation or an update</li> <li>Enter this folder. For instance, if <code>~/soft</code> directory is used: <code>cd ~/soft/repseq</code></li> <li>Make sure Conda is working: You should see (base) or another environment name on the left side of the command prompt. If it\u2019s not active, run: <code>conda activate bash</code></li> <li>Create the environment and install dependencies from the .yml file: <code>conda env create -n main_repseq -f main_repseq.yml</code> (-n is used to specify the environment name)</li> <li>Wait for all dependencies to finish installing. Shortly after that, the new environment will appear in the environment list in Jupyter Hub.</li> </ul> <p>In Jupyter Hub, make sure to select the installed environment from the menu in the upper-right corner. For clustering and calculating statistics, it's recommended to run the Jupyter Hub server in a short session with 32 processors to fully utilize parallel computations during clustering.</p>"},{"location":"mixcr/","title":"MiXCR4 functions for batch analysis in Jupyter. Uses SLURM on Aldan3 server","text":""},{"location":"mixcr/#mixcr4_analyze_batch","title":"mixcr4_analyze_batch","text":"<p>Function for batch runs of MiXCR software using SLURM. For each record in the given <code>sample_df</code> this function creates a SLURM-script in <code>~/temp/slurm</code> folder and adds them to SLURM-queue. All the <code>stdout</code> logs are also  put to <code>~/temp/slurm</code> folder. In case of troubles check the latest logs in this folder.  By default this function uses <code>mixcr analyze</code> command for MiLab Hum RNA TCR Kit (with UMI).  To change the command template use <code>command_template</code> parameter</p> <p>Parameters:</p> Name Type Description Default <code>sample_df</code> <code>DataFrame</code> <p>DataFrame, containing 'sample_id' column and  'R1' and 'R2' columns, containing paths (recommended full paths) to raw read files</p> required <code>output_folder</code> <code>str</code> <p>path to output folder</p> required <code>command_template</code> <code>str</code> <p>MiXCR command template  (default: 'mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix'). May be used as an example. Note that <code>mixcr analyze</code> and <code>r1 r2 output_prefix</code> are  \"magical\" parts of the template that should be kept as-is in the template, so change  only the part in-between these parts.</p> <code>None</code> <code>mixcr_path</code> <code>str</code> <p>path to MiXCR binary</p> <code>'mixcr'</code> <code>memory</code> <code>int</code> <p>required OOM in GB</p> <code>32</code> <code>time_estimate</code> <code>numeric</code> <p>time estimate in hours for the calculation. It is the limit for SLURM task</p> <code>1.5</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def mixcr4_analyze_batch(sample_df, output_folder, command_template=None,\n                         mixcr_path=\"mixcr\", memory=32, time_estimate=1.5, custom_tag_pattern_column=None):\n\n    \"\"\"\n    Function for batch runs of MiXCR software using SLURM.\n    For each record in the given `sample_df` this function creates a SLURM-script in\n    `~/temp/slurm` folder and adds them to SLURM-queue. All the `stdout` logs are also \n    put to `~/temp/slurm` folder. In case of troubles check the latest logs in this folder. \n    By default this function uses `mixcr analyze` command for MiLab Hum RNA TCR Kit (with UMI). \n    To change the command template use `command_template` parameter\n\n    Args:\n        sample_df (pd.DataFrame): DataFrame, containing 'sample_id' column and \n            'R1' and 'R2' columns, containing paths (recommended full paths) to raw read files\n        output_folder (str): path to output folder\n        command_template (str): MiXCR command template \n            (default: 'mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix').\n            May be used as an example. Note that `mixcr analyze` and `r1 r2 output_prefix` are \n            \"magical\" parts of the template that should be kept as-is in the template, so change \n            only the part in-between these parts.\n        mixcr_path (str): path to MiXCR binary\n        memory (int): required OOM in GB\n        time_estimate (numeric): time estimate in hours for the calculation. It\n            is the limit for SLURM task\n\n    Returns:\n        None\n    \"\"\"\n    max_memory = 1500\n    min_memory = 16\n\n    program_name=\"MIXCR4 Analyze Batch\"\n    samples_num = sample_df.shape[0]\n\n    # by default use the most popular preset for MiLaboratory Human TCR UMI MULTIPLEX Kit\n    default_command_template = \"mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix\"\n    if command_template is None:\n        command_template = default_command_template\n\n    # cut placeholders from command template\n    remove_list = [\"mixcr\", \"r1\", \"r2\", \"output_prefix\"]\n    command_template = ' '.join([w for w in command_template.split() if w not in remove_list])\n\n    # check input for custom tag pattern\n    custom_tag_pattern = False\n    if isinstance(custom_tag_pattern_column, str):\n        if custom_tag_pattern_column not in sample_df.columns:\n            raise ValueError(f\"Specified tag-pattern columns '{custom_tag_pattern_column}' is not present in sample_df\")\n        if \"--tag-pattern\" in command_template.split():\n            raise ValueError(f\"Please, remove '--tag-pattern' option from command_template, when you use custom tag-pattern\")\n        custom_tag_pattern = True\n\n    # Create output dir if does not exist\n    os.makedirs(output_folder, exist_ok=True)\n\n    # default mixcr analyze slurm parameters. They are quite excessive, works fine.\n    # time_estimate=1.5\n    cpus=40\n    if not isinstance(memory, int):\n        raise TypeError(\"memory parameter must be an integer\")\n    if memory &lt; min_memory:\n        print(f\"{memory} &lt; than limit ({min_memory}), using {min_memory} GB\")\n        memory = min_memory\n    if memory &gt; max_memory:\n        print(f\"{memory} &gt; than limit ({max_memory}), using {max_memory} GB\")\n        memory = max_memory\n\n\n    # create slurm batch file for progress tracking\n    slurm_batch_filename = os.path.join(output_folder, \"mixcr_analyze_slurm_batch.log\")\n    create_slurm_batch_file(slurm_batch_filename, program_name, samples_num)\n\n    # main cycle by samples\n    for i,r in sample_df.iterrows():\n        sample_id = r[\"sample_id\"]\n        r1 = r[\"R1\"]\n        r2 = r[\"R2\"]\n    #   output_prefix = os.path.join(output_folder, sample_id)\n        output_prefix = sample_id\n        if custom_tag_pattern:\n            tag_pattern = r[custom_tag_pattern_column]\n            command = f'{mixcr_path} -Xmx{memory}g {command_template} --tag-pattern \"{tag_pattern}\" {r1} {r2} {output_prefix}'\n        else:\n            command = f'{mixcr_path} -Xmx{memory}g {command_template} {r1} {r2} {output_prefix}'\n        command = f\"cd {output_folder}; \" + command\n        jobname = f\"mixcr_analyze_{sample_id}\"\n\n        # for batch task finish tracking:\n        command += f'; echo \"{jobname} finished\" &gt;&gt; {slurm_batch_filename}'\n\n        # create slurm script and add job to queue, print stdout of sbatch\n        stdout, stderr = run_slurm_command_from_jupyter(command, jobname, cpus, time_estimate, memory)\n        print(stdout, stderr)\n\n    print(f\"{samples_num} tasks added to slurm queue\\n\")\n    print(f'To see running progress bar run this function in the next jupyter cell:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\", loop=True)')\n    print(f'To see current progress:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\")')\n</code></pre>"},{"location":"mixcr/#mixcr_7genes_run_batch","title":"mixcr_7genes_run_batch","text":"<p>Function for batch runs of the MiXCR software using the SLURM <code>mixcr analyze</code> command and the <code>Human 7GENES DNA Multiplex</code> MiXCR built-in preset.  Incomplete rearrangements obtained by this kit are also included. For each incomplete rearrangement, unaligned reads from the previous  step are iteratively processed. Each output is stored in a subdirectory named after the corresponding rearrangement. For each record in the given <code>sample_df</code>, this function creates a SLURM script in the <code>~/temp/slurm</code> folder and adds it to the SLURM queue.  All <code>stdout</code> logs are also saved to the <code>~/temp/slurm</code> folder. In case of troubles, check the latest logs in this folder. </p> <p>Parameters:</p> Name Type Description Default <code>sample_df</code> <code>DataFrame</code> <p>DataFrame containing a 'sample_id' column and  'R1' and 'R2' columns containing paths (recommended full paths) to raw read files.</p> required <code>output_folder</code> <code>str</code> <p>Path to the output folder.</p> required <code>mixcr_path</code> <code>str</code> <p>Path to the MiXCR binary.</p> <code>'mixcr'</code> <code>memory</code> <code>int</code> <p>Required OOM in GB.</p> <code>32</code> <code>time_estimate</code> <code>numeric</code> <p>Time estimate in hours for the calculation; it  is the limit for the SLURM task.</p> <code>1.5</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def mixcr_7genes_run_batch(sample_df, output_folder, mixcr_path=\"mixcr\", memory=32, time_estimate=1.5):\n    \"\"\"\n    Function for batch runs of the MiXCR software using the SLURM `mixcr analyze` command and the `Human 7GENES DNA Multiplex` MiXCR built-in preset. \n    Incomplete rearrangements obtained by this kit are also included. For each incomplete rearrangement, unaligned reads from the previous \n    step are iteratively processed. Each output is stored in a subdirectory named after the corresponding rearrangement.\n    For each record in the given `sample_df`, this function creates a SLURM script in the `~/temp/slurm` folder and adds it to the SLURM queue. \n    All `stdout` logs are also saved to the `~/temp/slurm` folder. In case of troubles, check the latest logs in this folder. \n\n    Args:\n        sample_df (pd.DataFrame): DataFrame containing a 'sample_id' column and \n            'R1' and 'R2' columns containing paths (recommended full paths) to raw read files.\n        output_folder (str): Path to the output folder.\n        mixcr_path (str): Path to the MiXCR binary.\n        memory (int): Required OOM in GB.\n        time_estimate (numeric): Time estimate in hours for the calculation; it \n            is the limit for the SLURM task.\n\n    Returns:\n        None\n    \"\"\"\n    # default mixcr analyze slurm parameters. They are quite excessive, works fine.\n    max_memory = 1500\n    min_memory = 16\n    cpus=40\n\n    program_name=\"MIXCR4 Analyze 7genes Batch\"\n    samples_num = sample_df.shape[0]\n\n    # Create output dir if does not exist\n    os.makedirs(output_folder, exist_ok=True)\n\n\n    if not isinstance(memory, int):\n        raise TypeError(\"memory parameter must be an integer\")\n    if memory &lt; min_memory:\n        print(f\"{memory} &lt; than limit ({min_memory}), using {min_memory} GB\")\n        memory = min_memory\n    if memory &gt; max_memory:\n        print(f\"{memory} &gt; than limit ({max_memory}), using {max_memory} GB\")\n        memory = max_memory\n\n    # create slurm batch file for progress tracking\n    slurm_batch_filename = os.path.join(output_folder, \"mixcr_analyze_slurm_batch.log\")\n    create_slurm_batch_file(slurm_batch_filename, program_name, samples_num)\n\n    list_of_incomplete_rearrangements = [\"DJ_TRB\", \"VDD_TRD\", \"DDJ_TRD\", \"DD_TRD\", \"DJ_IGH\", \"VKDE_IGK\", \"CINTRON_KDE_IGK\"]\n\n    # main cycle by samples\n    for i,r in sample_df.iterrows():\n        sample_id = r[\"sample_id\"]\n        r1 = r[\"R1\"]\n        r2 = r[\"R2\"]\n        output_prefix = sample_id\n\n        R1na = f\"{sample_id}_R1_not_aligned.fastq.gz\"\n        R2na = f\"{sample_id}_R2_not_aligned.fastq.gz\"\n\n        commands = [f\"cd {output_folder}\"]\n\n        first_command = f'{mixcr_path} -Xmx{memory}g analyze milab-human-dna-xcr-7genes-multiplex -f --not-aligned-R1 {R1na} --not-aligned-R2 {R2na} {r1} {r2} {output_prefix}'\n        commands.append(first_command)\n\n        for rearrangement in list_of_incomplete_rearrangements:\n\n            # swap r and Rna so we would not implement copy of R_na\n            r1, R1na = R1na, r1\n            r2, R2na = R2na, r2\n\n            output_prefix = os.path.join(rearrangement, sample_id)\n\n            R1na = f\"{output_prefix}_R1_not_aligned.fastq.gz\"\n            R2na = f\"{output_prefix}_R2_not_aligned.fastq.gz\"\n\n            i_r_command = f'{mixcr_path} -Xmx{memory}g analyze generic-amplicon -f --species hs --library {rearrangement} --assemble-clonotypes-by CDR3 --dna --floating-left-alignment-boundary --floating-right-alignment-boundary J -MexportClones.splitFilesBy=[] --not-aligned-R1 {R1na} --not-aligned-R2 {R2na} {r1} {r2} {output_prefix}'\n            commands.append(i_r_command)\n\n        jobname = f\"mixcr_analyze_{sample_id}\"\n\n        # for batch task finish tracking:\n        commands.append(f'echo \"{jobname} finished\" &gt;&gt; {slurm_batch_filename}')\n\n        # join commands by &amp;&amp; so that next command runs if previous was finished without error and add new lines to the script\n        command = \" &amp;&amp; \\\\ \\n\".join(commands)\n\n        # create slurm script and add job to queue, print stdout of sbatch\n        stdout, stderr = run_slurm_command_from_jupyter(command, jobname, cpus, time_estimate, memory)\n        print(stdout, stderr)\n        # print(command)\n    print(f\"{samples_num} tasks added to slurm queue\\n\")\n    print(f'To see running progress bar run this function in the next jupyter cell:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\", loop=True)')\n    print(f'To see current progress:\\nslurm.check_slurm_progress(\"{slurm_batch_filename}\")')\n</code></pre>"},{"location":"mixcr/#mixcr4_reports","title":"mixcr4_reports","text":"<p>runs <code>mixcr exportQc</code> commands - <code>align</code>, <code>chainUsage</code> and <code>tags</code> in a given folder  for all <code>.clns</code> filenames. <code>align</code> and <code>chainUsage</code> are run twice to create both  <code>svg</code> and <code>pdf</code> files.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>folder in which to run the <code>mixcr exportQc</code> commands</p> required <code>mixcr_path</code> <code>str</code> <p>path to MiXCR binary</p> <code>'mixcr'</code> <p>Returns:     None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def mixcr4_reports(folder, mixcr_path=\"mixcr\"):\n\n    \"\"\"\n    runs `mixcr exportQc` commands - `align`, `chainUsage` and `tags` in a given folder \n    for all `.clns` filenames. `align` and `chainUsage` are run twice to create both \n    `svg` and `pdf` files.\n\n    Args:\n        folder (str): folder in which to run the `mixcr exportQc` commands\n        mixcr_path (str): path to MiXCR binary\n    Returns:\n        None\n\n    \"\"\"\n\n\n    program_name=\"MIXCR4.3 Reports\"\n    time_estimate=1\n    cpus=40\n    memory=32\n\n    # clns_filenames = os.path.join(folder, \"*.clns\")\n    # align_filename = os.path.join(folder, \"alignQc.png\")\n    # chains_filename = os.path.join(folder, \"chainsQc.png\")\n    # tags_filename = os.path.join(folder, \"tagsQc.pdf\")\n    clns_filenames = \"*.clns\"\n    align_filename = \"alignQc.svg\"\n    chains_filename = \"chainsQc.svg\"\n    align_filename_pdf = \"alignQc.pdf\"\n    chains_filename_pdf = \"chainsQc.pdf\"\n    tags_filename = \"tagsQc.pdf\"\n    #tables_filename = os.path.join(folder, \"tables.tsv\")\n    #preproc_filename = os.path.join(folder, \"preproc_tables.tsv\")\n    #postanalysis_filename = os.path.join(folder, \"postanalysis.json\")\n\n\n\n    commands = {\"alignQc\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc align -f {clns_filenames} {align_filename}\",\n                \"chainUsage\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc chainUsage -f {clns_filenames} {chains_filename}\",\n                \"alignQcPDF\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc align -f {clns_filenames} {align_filename_pdf}\",\n                \"chainUsagePDF\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc chainUsage -f {clns_filenames} {chains_filename_pdf}\",\n                \"tagsQc\": f\"cd {folder}; {mixcr_path} -Xmx32g exportQc tags -f {clns_filenames} {tags_filename}\"#,\n                #\"postanalysis\": f\"{MIXCR} -Xmx32g postanalysis individual -f --default-downsampling none --default-weight-function umi --only-productive --tables {tables_filename} --preproc-tables {preproc_filename} {clns_filenames} {postanalysis_filename}\"\n               }\n\n\n    commands_num = len(commands)\n\n    slurm_batch_filename = os.path.join(folder, \"mixcr_reports_slurm_batch.log\")\n    create_slurm_batch_file(slurm_batch_filename, program_name, commands_num)\n\n    for jobname, command in commands.items():\n        command += f'; echo \"{jobname} finished\" &gt;&gt; {slurm_batch_filename}'\n        stdout, stderr = run_slurm_command_from_jupyter(command, jobname, cpus, time_estimate, memory)\n        print(stdout, stderr)\n</code></pre>"},{"location":"mixcr/#get_processing_table","title":"get_processing_table","text":"<p>Searches for clonosets in the the folder, extracts their sample_id's and shows main processing stats in a table format. By default does not show \"off-target\" clonosets -  those having less than 1% (default, may be overriden) of reads for the sample_id. For example, you have sequenced TRB sample, but there is found 0.5% (by read count)  of TRA chains for the same sample_id, then the clonoset will not be shown in the table. You can specify <code>show_offtarget=True</code> to display all found chains in the table or  outherwise set a higher value for <code>offtarget_chain_threshold</code> (<code>0.01</code> by default).</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str or list</code> <p>folder or list of folders in which to look for clonosets and processing stats</p> required <code>show_offtarget</code> <code>bool</code> <p>add offtarget chains to the stats</p> <code>False</code> <code>offtarget_chain_threshold</code> <code>float</code> <p>threshold for off-target chains</p> <code>0.01</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe, containing <code>sample_id</code>, <code>extracted_chain</code> and  different processing stats columns. There may be several rows with the same  <code>sample_id</code>, with each found <code>extracted_chain</code></p> Source code in <code>repseq/mixcr.py</code> <pre><code>def get_processing_table(folder, show_offtarget=False, offtarget_chain_threshold=0.01):\n    \"\"\"\n    Searches for clonosets in the the folder, extracts their sample_id's and shows main\n    processing stats in a table format. By default does not show \"off-target\" clonosets - \n    those having less than 1% (default, may be overriden) of reads for the sample_id.\n    For example, you have sequenced TRB sample, but there is found 0.5% (by read count) \n    of TRA chains for the same sample_id, then the clonoset will not be shown in the table.\n    You can specify `show_offtarget=True` to display all found chains in the table or \n    outherwise set a higher value for `offtarget_chain_threshold` (`0.01` by default).\n\n    Args:\n        folder (str or list): folder or list of folders in which to look for clonosets and\n            processing stats\n        show_offtarget (bool): add offtarget chains to the stats\n        offtarget_chain_threshold (float): threshold for off-target chains\n\n    Returns:\n        df (pd.DataFrame): dataframe, containing `sample_id`, `extracted_chain` and \n            different processing stats columns. There may be several rows with the same \n            `sample_id`, with each found `extracted_chain`\n    \"\"\"\n\n    if isinstance(folder, list):\n        tables = []\n        for f in folder:\n            table = get_processing_table(f, show_offtarget=show_offtarget)\n            tables.append(table)\n        return pd.concat(tables).sort_values(by=\"sample_id\").reset_index(drop=True)\n\n    results = []\n    clonosets = find_all_exported_clonosets_in_folder(folder, chain=None)\n\n    for i, r in clonosets.iterrows():\n        sample_id = r[\"sample_id\"]\n        chain = r[\"chain\"]\n        align_report = read_json_report(sample_id, folder, \"align\")\n\n        try:\n            refine_report = read_json_report(sample_id, folder, \"refine\")\n            umi = True\n        except FileNotFoundError:\n            umi = False\n\n        assemble_report = read_json_report(sample_id, folder, \"assemble\")\n\n        # print(sample_id, chain)\n        clonoset = read_clonoset(r.filename)\n        clonoset_f = filter_nonfunctional_clones(clonoset)\n\n        # align report\n        Rt=align_report[\"totalReadsProcessed\"]\n        Ru=align_report[\"totalReadsProcessed\"]-align_report[\"notAlignedReasons\"][\"NoBarcode\"]\n        Ru_pc = round(Ru/Rt*100, 2)\n        Ra=align_report[\"aligned\"]\n        Ra_pc = round(Ra/Rt*100, 2)\n        Roa = align_report[\"overlappedAligned\"]\n        Roa_pc = round(Roa/Ra*100, 2)\n\n        if umi:\n        #Ra2=refine_report[\"correctionReport\"][\"inputRecords\"] ##### differs from Ra, but D.Bolotin did not explain why\n\n            UMIa=refine_report[\"correctionReport\"][\"steps\"][0][\"inputDiversity\"]\n            UMIc=refine_report[\"correctionReport\"][\"steps\"][0][\"outputDiversity\"]\n            try:\n                UMIf=refine_report[\"correctionReport\"][\"filterReport\"][\"numberOfGroupsAccepted\"]\n            except TypeError:\n                UMIf=UMIc\n            Rf=refine_report[\"correctionReport\"][\"outputRecords\"]\n            try:\n                overseq_threshold = int(refine_report[\"correctionReport\"][\"filterReport\"][\"operatorReports\"][0][\"operatorReport\"][\"threshold\"])\n            except TypeError:\n                overseq_threshold = None\n            reads_per_umi = round(Rf/UMIf, 2)\n        else:\n            UMIa = np.nan\n            UMIc = np.nan\n            UMIf = np.nan\n            Rf = np.nan\n            overseq_threshold = np.nan\n            reads_per_umi = np.nan\n\n        Ct=assemble_report[\"clones\"]\n        Rcl=assemble_report[\"readsInClones\"]\n\n        Ctc=len(clonoset)\n        Rclc=int(clonoset.readCount.sum())\n\n        Cfunc=len(clonoset_f)\n        Rfunc=int(clonoset_f.readCount.sum())\n        if umi:\n            UMIcl=clonoset.uniqueMoleculeCount.sum()\n            UMIfunc=clonoset_f.uniqueMoleculeCount.sum()\n        else:\n            UMIcl=np.nan\n            UMIfunc=np.nan\n        if umi and overseq_threshold is None:\n            reads_per_umi = round(Rclc/UMIcl, 2)\n\n        results.append([sample_id, chain, Rt, Ru_pc, Ra_pc, Roa_pc, UMIa, UMIc, overseq_threshold, Rf, UMIf, reads_per_umi, Ct, Rcl, Ctc, Rclc, Cfunc, Rfunc, UMIcl, UMIfunc])\n    result_df = pd.DataFrame(results, columns=[\"sample_id\", \"extracted_chain\", \"reads_total\", \"reads_with_umi_pc\", \"reads_aligned_pc\", \"reads_overlapped_aln_pc\",\n                                               \"total_umi\", \"umi_after_correction\", \"overseq_threshold\", \"reads_after_filter\", \"umi_after_filter\",\n                                               \"reads_per_umi\", \"clones_total\", \"reads_in_clones_total\", \"clones\", \"reads_in_clones\", \"clones_func\", \"reads_in_func_clones\", \"umi_in_clones\", \"umi_in_func_clones\"])\n    if not show_offtarget:\n        result_df = result_df.loc[result_df.reads_in_clones/result_df.reads_in_clones_total &gt; offtarget_chain_threshold]\n    return result_df.sort_values(by=\"sample_id\").reset_index(drop=True)\n</code></pre>"},{"location":"mixcr/#show_report_images","title":"show_report_images","text":"<p>This function displays QC images <code>alignQc.svg</code> and <code>chainsQc.svg</code> in Jupyter Notebook. This pictures may be generated by <code>mixcr4_reports</code> function. In case there are no <code>.svg</code> images, the <code>.png</code> images are shown.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>folder in which to look for QC images.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def show_report_images(folder):\n    \"\"\"\n    This function displays QC images `alignQc.svg` and `chainsQc.svg` in Jupyter Notebook.\n    This pictures may be generated by `mixcr4_reports` function.\n    In case there are no `.svg` images, the `.png` images are shown.\n\n    Args:\n        folder (str): folder in which to look for QC images.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    svg_align_filename = os.path.join(folder, \"alignQc.svg\")\n    svg_chain_filename = os.path.join(folder, \"chainsQc.svg\")\n    png_align_filename = os.path.join(folder, \"alignQc.png\")\n    png_chain_filename = os.path.join(folder, \"chainsQc.png\")\n\n    if os.path.exists(svg_align_filename):\n        print(svg_align_filename)\n        display(SVG(filename=svg_align_filename))\n    elif os.path.exists(png_align_filename):\n        print(png_align_filename)\n        display(Image(filename=png_align_filename))\n    else:\n        print(\"No alignQc image found (svg or png)\")\n\n    if os.path.exists(svg_chain_filename):\n        print(svg_chain_filename)\n        display(SVG(filename=svg_chain_filename))\n    elif os.path.exists(png_chain_filename):\n        print(png_chain_filename)\n        display(Image(filename=png_chain_filename))\n    else:\n        print(\"No chainQc image found (svg or png)\")\n</code></pre>"},{"location":"mixcr/#show_report_images_1","title":"show_report_images","text":"<p>This function displays QC images <code>alignQc.svg</code> and <code>chainsQc.svg</code> in Jupyter Notebook. This pictures may be generated by <code>mixcr4_reports</code> function. In case there are no <code>.svg</code> images, the <code>.png</code> images are shown.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>folder in which to look for QC images.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>repseq/mixcr.py</code> <pre><code>def show_report_images(folder):\n    \"\"\"\n    This function displays QC images `alignQc.svg` and `chainsQc.svg` in Jupyter Notebook.\n    This pictures may be generated by `mixcr4_reports` function.\n    In case there are no `.svg` images, the `.png` images are shown.\n\n    Args:\n        folder (str): folder in which to look for QC images.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    svg_align_filename = os.path.join(folder, \"alignQc.svg\")\n    svg_chain_filename = os.path.join(folder, \"chainsQc.svg\")\n    png_align_filename = os.path.join(folder, \"alignQc.png\")\n    png_chain_filename = os.path.join(folder, \"chainsQc.png\")\n\n    if os.path.exists(svg_align_filename):\n        print(svg_align_filename)\n        display(SVG(filename=svg_align_filename))\n    elif os.path.exists(png_align_filename):\n        print(png_align_filename)\n        display(Image(filename=png_align_filename))\n    else:\n        print(\"No alignQc image found (svg or png)\")\n\n    if os.path.exists(svg_chain_filename):\n        print(svg_chain_filename)\n        display(SVG(filename=svg_chain_filename))\n    elif os.path.exists(png_chain_filename):\n        print(png_chain_filename)\n        display(Image(filename=png_chain_filename))\n    else:\n        print(\"No chainQc image found (svg or png)\")\n</code></pre>"},{"location":"modules/","title":"Modules","text":""},{"location":"modules/#clonosets","title":"Clonosets","text":"<p>Manipulations with groups of clonosets:</p> <ul> <li>Creates clonosets DataFrames by searching for typical filenames in directories</li> <li>Detects clonoset formats</li> <li>Filters out non-target clonosets (same sample_id but low % of reads)</li> <li>Pools clonosets into one</li> </ul>"},{"location":"modules/#stats","title":"Stats","text":"<ul> <li>Basic clonoset properties, like clone/read/umi counts functional or with OOF/Stops</li> <li>CDR3 amino acid properties: N-counts, physico-chemical properties, Kidera Factors</li> <li>Diversity statistics: observed diversity, (normalized) Shannon-Wiener, chao1</li> <li>Convergence estimate</li> <li>V/D/J/C-gene frequencies or VJ-combinations</li> <li>All calculations are parallelized</li> </ul>"},{"location":"modules/#intersections","title":"Intersections","text":"<ul> <li>Finds parwise intersecting clonotypes between clonosets</li> <li>Intersection metrics: F, F2, D</li> <li>Count tables for clonotypes (similarity groups of clonotypes)</li> <li>Intersect clusters with clonosets</li> <li>TCRnet integration</li> </ul>"},{"location":"modules/#clustering","title":"Clustering","text":"<p>This module implements different immune repertoire clustering analyses:</p> <ul> <li>basic clustering: using hamming distance similarity in CDR3 regions and/or same V/J-segments</li> <li>tcr-dist clustering: using distance metrics from tcrdist software</li> <li>ALICE: find expanded clonotypes by analyzing the probability of neighbour generation with OLGA algrorithm</li> <li>split clusters with community detection algorithms (Louvain, Leiden)</li> <li>easy and modular customisation of cluster analysis</li> <li>output graphs and metadata to Cytoscape format</li> </ul> <p>Graph representation with NetworkX library.</p>"},{"location":"modules/#diffexp","title":"Diffexp","text":"<p>Finds differentially expressing clonotypes/clusters of clonotypes in CFSE-assays or similar experiments.</p>"},{"location":"modules/#clone-filter","title":"Clone Filter","text":"<p>Easy filtering of clonosets by one Filter object, integrated with other analysis procedures. Filtering includes following features:</p> <ul> <li>counting by reads/UMIs/clonotypes</li> <li>use top N clonotypes (tails mixing included for randomly mixing clonotypes with same counts)</li> <li>randomly downsample to N UMIs/reads (you can specify seed, highly recommended for reproducibility)</li> <li>remove low count clonotypes</li> <li>filter out non-functional(OOF,Stop in CDR3)/functional clonotypes</li> <li>white/black list of clonotypes</li> <li>recount frequencies (by reads/UMIs)</li> <li>convert to vdjtools-like format</li> <li>combine (pool) clonotypes with similar features: CDR3/V/J</li> </ul>"},{"location":"modules/#io-module","title":"IO module","text":"<ul> <li>reads and understands clonosets of following formats: MiXCR 3/4, vdjtools, Adaptive Biosciences</li> <li>tsv, .gz, .zip</li> </ul>"},{"location":"modules/#mixcr-module","title":"MiXCR module","text":"<p>As MiXCR is the leading software for generating clonoset tables from raw FastQ files this module helps to run MiXCR 4.3+ batch analyses with SLURM queue manager. Easy accumulation of most sensible processing data from json-reports of MiXCR into one table.</p>"},{"location":"page2/","title":"Page 2","text":""},{"location":"page2/#code-annotation-examples","title":"Code annotation examples","text":""},{"location":"page2/#code-blocks","title":"Code blocks","text":"<p>Some <code>code</code> goes here</p>"},{"location":"page2/#plain-code-blocks","title":"Plain code blocks","text":"<pre><code>def read_json_report(sample_id, folder, report_type):\n    filename = os.path.join(folder, f\"{sample_id}.{report_type}.report.json\")\n    with open(filename) as data_file:    \n        for jsonObj in data_file:\n            report = json.loads(jsonObj)\n    return report\n// some comment\n</code></pre>"},{"location":"page2/#code-for-a-specific-language","title":"Code for a specific language","text":"<p>Some more code with the <code>py</code> at the start</p> <pre><code>import sys\nREPSEQ_PATH = '/home/mmyshkin/soft/repseq'\nsys.path.append(REPSEQ_PATH)\nfrom repseq import slurm\nfrom repseq import io\nfrom repseq import common_functions as cf\nfrom repseq import clonosets as cl\nfrom repseq import clustering\nfrom repseq import mixcr as mx\nfrom repseq import segment_usage as su\nfrom repseq import stats\n</code></pre>"},{"location":"page2/#code-with-a-title","title":"Code with a title","text":"import useful packages<pre><code>import os\nimport pandas as pd\nfrom IPython.display import Image, display\nimport json\nimport re\nimport math\nimport random\nimport numpy as np\n</code></pre>"},{"location":"page2/#add-line-numbers","title":"Add line numbers","text":"<pre><code>def shannon_wiener(list_of_numbers):\n    list_of_numbers = list(list_of_numbers)\n    total_size = sum(list_of_numbers)\n    freqs = [s/total_size for s in list_of_numbers]\n    diversity = len(list_of_numbers)\n    sw = -sum([f*np.log(f) for f in freqs])\n    sw_norm = sw/np.log(diversity)\n    return sw, sw_norm, diversity\n</code></pre>"},{"location":"page2/#highlighting-lines","title":"Highlighting lines","text":"<pre><code>def shannon_wiener(list_of_numbers):\n    list_of_numbers = list(list_of_numbers)\n    total_size = sum(list_of_numbers)\n    freqs = [s/total_size for s in list_of_numbers]\n    diversity = len(list_of_numbers)\n    sw = -sum([f*np.log(f) for f in freqs])\n    sw_norm = sw/np.log(diversity)\n    return sw, sw_norm, diversity\n</code></pre>"},{"location":"page2/#icons-and-emojis","title":"Icons and Emojis","text":""},{"location":"page2/#try-to-show-all-the-functions-of-a-module","title":"Try to show all the functions of a module","text":""},{"location":"page2/#intersections.count_table","title":"<code>count_table(clonosets_df, cl_filter=None, overlap_type='aaV', mismatches=0, strict_presence=False, by_freq=False)</code>","text":"<p>Creates a table that shows how many times each unique clonotype appears across different clonosets. It processes a given dataset of clonotypes (clonosets_df)  and generates a frequency/count table based on a specified overlap type.</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, <code>filename</code> - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>mismatches</code> <code>int</code> <p>Max number of single-letter mismatches in clonotypes sequences  for them to be treated similar, i.e. hamming distance.</p> <code>0</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required <code>strict_presence (bool, default</code> <p>): if set to <code>True</code> and the clonotype is not found in the clonoset, it will not be counted, even when the <code>mismatches</code> option is not set to 0. If <code>False</code>, mismatched sequences are counted even if the exact match does not exist.     by_freq (bool): default is <code>True</code> - this means that the intersect metric is frequency of clonotype,  but not its count</p> required <code>by_freq</code> <code>bool</code> <p>default is <code>True</code> - this means that the intersect metric is frequency of clonotype,  but not its count</p> <code>False</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe containing clonotype counts with sample names as columns and all possible clonotypes (given the overlap_type) as rows.</p> Source code in <code>repseq/intersections.py</code> <pre><code>def count_table(clonosets_df, cl_filter=None, overlap_type=\"aaV\", mismatches=0, strict_presence=False, by_freq=False):\n    \"\"\"\n    Creates a table that shows how many times each unique clonotype appears across different clonosets. It processes a given dataset of clonotypes (clonosets_df) \n    and generates a frequency/count table based on a specified overlap type.\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            `filename` - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): Max number of single-letter mismatches in clonotypes sequences \n            for them to be treated similar, i.e. hamming distance.\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n        strict_presence (bool, default:): if set to `True` and the clonotype is not found in the clonoset, it will not be counted, even when the `mismatches` option is not set to 0.\n            If `False`, mismatched sequences are counted even if the exact match does not exist.\n                by_freq (bool): default is `True` - this means that the intersect metric is frequency of clonotype, \n            but not its count\n        by_freq (bool): default is `True` - this means that the intersect metric is frequency of clonotype, \n            but not its count\n\n    Returns:\n        df (pd.DataFrame): dataframe containing clonotype counts with sample names as columns and all possible clonotypes (given the overlap_type) as rows. \n    \"\"\"\n\n\n    print(\"Creating clonotypes count table\\n\"+\"-\"*50)\n    print(f\"Overlap type: {overlap_type}\")\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    clonoset_dicts = convert_clonosets_to_compact_dicts(clonosets_df, cl_filter=cl_filter,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=not bool(mismatches))\n    unique_clonotypes = find_unique_clonotypes_in_clonoset_dicts(clonoset_dicts)\n\n    tasks = []\n    for sample_id in clonoset_dicts:\n        task = [unique_clonotypes, sample_id, clonoset_dicts[sample_id], mismatches, strict_presence]\n        tasks.append(task)\n\n    results = run_parallel_calculation(count_table_mp, tasks, \"Counting features\", object_name=\"clonosets\")\n    result_dict = dict()\n    for result in results:\n        result_dict.update(result)\n    count_table = pd.DataFrame(result_dict)\n    count_table.index = unique_clonotypes\n    return count_table\n</code></pre>"},{"location":"page2/#intersections.count_table_by_cluster","title":"<code>count_table_by_cluster(clonosets_df, clusters_list, cl_filter=None, overlap_type='aaV', mismatches=0, by_freq=True)</code>","text":"<p>This function creates a table that shows the presence of clonotypes grouped into user-provided clusters across different clonosets. Instead of counting individual clonotypes, it  calculates how many clonotypes from each cluster appear in each clonoset.</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>cluster_list</code> <code>?</code> <p>description</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>mismatches</code> <code>int</code> <p>Max number of single-letter mismatches in clonotypes sequences  for them to be treated similar, i.e. hamming distance.</p> <code>0</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with the following columns: description</p> Source code in <code>repseq/intersections.py</code> <pre><code>def count_table_by_cluster(clonosets_df, clusters_list, cl_filter=None, overlap_type=\"aaV\", mismatches=0, by_freq=True):\n\n    \"\"\"\n    This function creates a table that shows the presence of clonotypes grouped into user-provided clusters across different clonosets. Instead of counting individual clonotypes, it \n    calculates how many clonotypes from each cluster appear in each clonoset.\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        cluster_list (?): description\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): Max number of single-letter mismatches in clonotypes sequences \n            for them to be treated similar, i.e. hamming distance.\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n\n    Returns:\n        df (pd.DataFrame): dataframe with the following columns: description\n    \"\"\"\n\n    print(\"Creating clonotypes count table\\n\"+\"-\"*50)\n    print(f\"Overlap type: {overlap_type}\")\n\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n\n    clonoset_dicts = convert_clonosets_to_compact_dicts(clonosets_df, cl_filter=cl_filter,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=not bool(mismatches))\n\n    clonotypes_by_cluster = convert_clusters_to_clonotype_list(clusters_list, aa, check_v, check_j, mismatches)\n\n    tasks = []\n    for sample_id in clonoset_dicts:\n        task = [clonotypes_by_cluster, sample_id, clonoset_dicts[sample_id], mismatches]\n        tasks.append(task)\n\n    results = run_parallel_calculation(count_table_by_cluster_mp, tasks, \"Counting cluster presence\", object_name=\"clonosets\")\n    result_dict = dict()\n    for result in results:\n        result_dict.update(result)\n    count_table = pd.DataFrame(result_dict).reset_index().rename(columns = {\"index\":\"feature_id\"})\n    count_table[\"feature_id\"] = count_table[\"feature_id\"].apply(lambda x: f\"cluster_{x}\")\n\n    return count_table\n</code></pre>"},{"location":"page2/#intersections.count_table_with_custom_clonotypes","title":"<code>count_table_with_custom_clonotypes(clonosets_df, clones_df=None, cl_filter=None, overlap_type='aaV', mismatches=0, strict_presence=False, by_freq=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>clones_df</code> <code>DataFrame</code> <p>a dataframe with clonotypes to look for in <code>clonosets_df</code>. Should have the following columns:</p> <code>None</code> Source code in <code>repseq/intersections.py</code> <pre><code>def count_table_with_custom_clonotypes(clonosets_df, clones_df=None, cl_filter=None, overlap_type=\"aaV\", mismatches=0, strict_presence=False, by_freq=False):\n    \"\"\"\n    Args:\n        clones_df (pd.DataFrame): a dataframe with clonotypes to look for in `clonosets_df`. Should have the following columns:\n        `cdr3aa`, `cdr3nt` (at least one of these two), `v`, `j` (could be none of those). For instance, your input might have only `cdr3aa` column.\n        If `overlap_type` isn't specified, it is determined individually for each clonotype in `clones_df`. In this case, if a row corresponding to\n        a particular clonotype has non-empty values in both `cdr3aa` and `cdr3nt` columns, `cdr3aa` is chosen.\n    \"\"\"\n\n    print(\"Creating clonotypes count table\\n\"+\"-\"*50)\n    print(f\"Overlap type: {overlap_type}\")\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    clonoset_dicts = convert_clonosets_to_compact_dicts(clonosets_df, cl_filter=cl_filter,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=not bool(mismatches))\n\n    if clones_df is not None:\n        # creating dummy counts\n        clones_df_copy = clones_df.copy()\n        clones_df_copy['count'] = [0 for _ in range(clones_df.shape[0])]\n        clones_df_copy['freq'] = [0 for _ in range(clones_df.shape[0])]\n        clonotype_list_dict = {'clonotype_list': None}\n\n        clonotype_list_dict['clonotype_list'] = prepare_clonoset_for_intersection(clones_df_copy, \n                                                                                overlap_type=overlap_type,\n                                                                                len_vj_format=bool(mismatches))  \n\n        unique_clonotypes = find_unique_clonotypes_in_clonoset_dicts(clonotype_list_dict)\n    else: \n        unique_clonotypes = find_unique_clonotypes_in_clonoset_dicts(clonoset_dicts)\n\n    tasks = []\n    for sample_id in clonoset_dicts:\n        task = [unique_clonotypes, sample_id, clonoset_dicts[sample_id], mismatches, strict_presence]\n        tasks.append(task)\n\n    results = run_parallel_calculation(count_table_mp, tasks, \"Counting features\", object_name=\"clonosets\")\n    result_dict = dict()\n    for result in results:\n        result_dict.update(result)\n    count_table = pd.DataFrame(result_dict)\n    if aa:\n        count_table.insert(0, 'cdr3aa', [ct[0] for ct in unique_clonotypes])\n    else:\n        count_table.insert(0, 'cdr3nt', [ct[0] for ct in unique_clonotypes])\n    if check_v:\n        count_table.insert(1, 'v', [ct[1] for ct in unique_clonotypes])\n    if check_j:\n        count_table.insert(1, 'j', [ct[2] for ct in unique_clonotypes])\n    count_table.index = unique_clonotypes\n    return count_table\n</code></pre>"},{"location":"page2/#intersections.find_intersecting_clonotypes","title":"<code>find_intersecting_clonotypes(clonosets_df, cl_filter=None, overlap_type='aaV', mismatches=0, metric='F2', clonosets_df2=None, cl_filter2=None)</code>","text":"<p>Calculating overlap distances between multiple repseq samples using F2 of F metrics The result of this function may be used for heatmap+clusterization of samples or for MDS plots</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>cl_filter</code> <code>Filter</code> <p>clonoset filter - object from <code>clone_filter.py</code> module. It is applied to clonosets in <code>clonosets_df</code>.</p> <code>None</code> <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>mismatches</code> <code>int</code> <p>Max number of single-letter mismatches in clonotypes sequences  for them to be treated similar, i.e. hamming distance.</p> <code>0</code> <code>by_umi</code> <code>bool</code> <p>set =True for MiXCR4 clonosets to select count/frequency of clonotypes  in UMI's if they exist in implemented protocol</p> required <code>metric</code> <code>str</code> <p>possible values - <code>F</code>, <code>F2</code> or <code>C</code>. Default <code>F2</code>. <code>F2</code> - sum of sqrt of product of  similar clonotype frequencies in two clonosets. <code>F</code> - sqrt of the sum of frequency products. <code>C</code> - total frequency of clonotypes in <code>sample1</code>, that are similar to clonotypes in <code>sample2</code></p> <code>'F2'</code> <code>clonosets_df2</code> <code>DataFrame</code> <p>If <code>clonosets_df2</code> is None (default), samples within <code>clonosets_df</code> are compared with each other;  Otherwise, the comparison is performed exclusively between samples from <code>clonosets_df</code> and <code>clonosets_df2</code>. </p> <code>None</code> <code>cl_filter2</code> <code>Filter</code> <p>clonoset filter - object from <code>clone_filter.py</code> module. It is applied to clonosets in <code>clonosets_df2</code>.  If there are samples with non-unique sample_ids between the two dataframes, both filters will be applied to those samples.</p> <code>None</code> <p>Important: similar clonotypes by <code>overlap_type</code> in one particular clonoset are NOT combined into one and are treated as different clonotypes.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>clone</code>, <code>sample1_count</code>, <code>sample2_count</code>, <code>sample1</code>, <code>sample2</code>, <code>pair</code>.  <code>clone</code> - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric count columns contain freq/count of the clone in sample pair column is made for easy separation of possibly huge DataFrame into overlapping pairs</p> Source code in <code>repseq/intersections.py</code> <pre><code>def find_intersecting_clonotypes(clonosets_df, cl_filter=None, overlap_type=\"aaV\", mismatches=0, metric=\"F2\", clonosets_df2=None, cl_filter2=None):\n    \"\"\"\n    Calculating overlap distances between multiple repseq samples using F2 of F metrics\n    The result of this function may be used for heatmap+clusterization of samples or for MDS plots\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        cl_filter (Filter): clonoset filter - object from `clone_filter.py` module. It is applied to clonosets in `clonosets_df`.\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): Max number of single-letter mismatches in clonotypes sequences \n            for them to be treated similar, i.e. hamming distance.\n        by_umi (bool): set =True for MiXCR4 clonosets to select count/frequency of clonotypes \n            in UMI's if they exist in implemented protocol\n        metric (str): possible values - `F`, `F2` or `C`. Default `F2`. `F2` - sum of sqrt of product of \n            similar clonotype frequencies in two clonosets. `F` - sqrt of the sum of frequency products.\n            `C` - total frequency of clonotypes in `sample1`, that are similar to clonotypes in `sample2`\n        clonosets_df2 (pd.DataFrame): If `clonosets_df2` is None (default), samples within `clonosets_df` are compared with each other; \n            Otherwise, the comparison is performed exclusively between samples from `clonosets_df` and `clonosets_df2`. \n        cl_filter2 (Filter): clonoset filter - object from `clone_filter.py` module. It is applied to clonosets in `clonosets_df2`. \n            If there are samples with non-unique sample_ids between the two dataframes, both filters will be applied to those samples.\n\n\n    Important: similar clonotypes by `overlap_type` in one particular clonoset are NOT combined into one\n    and are treated as different clonotypes.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `clone`, `sample1_count`, `sample2_count`, `sample1`, `sample2`, `pair`. \n            `clone` - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric\n            count columns contain freq/count of the clone in sample\n            pair column is made for easy separation of possibly huge DataFrame into overlapping pairs\n    \"\"\"\n\n\n    print(\"Intersecting clones in clonosets\\n\"+\"-\"*50)\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    print(f\"Overlap type: {overlap_type}\")\n\n    clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2 = prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2,\n                                                                                                                        cl_filter, cl_filter2,\n                                                                                                                        overlap_type, strict=not bool(mismatches))\n\n    # generating a set of tasks\n\n    tasks = []\n\n    if two_dataframes:\n        for sample1 in sample_list:\n            for sample2 in sample_list2:\n                tasks.append((sample1, sample2, clonoset_lists, check_v, check_j, mismatches))        \n    else:\n        for i in range(samples_total):\n            for j in range(samples_total):\n                sample1 = sample_list[i]\n                sample2 = sample_list[j]\n                if sample1 != sample2:\n                    tasks.append((sample1, sample2, clonoset_lists, check_v, check_j, mismatches))\n\n\n    # run calculation in parallel\n    result_list = run_parallel_calculation(find_overlapping_clones_in_two_clone_dicts, tasks, \"Intersecting clonosets\", object_name=\"pairs\")\n\n    return pd.concat(result_list).reset_index(drop=True)\n</code></pre>"},{"location":"page2/#intersections.intersect_clones_in_samples_batch","title":"<code>intersect_clones_in_samples_batch(clonosets_df, cl_filter=None, overlap_type='aaV', by_freq=True, clonosets_df2=None, cl_filter2=None)</code>","text":"<p>Calculating frequencies of intersecting clonotypes between multiple repseq samples. The result of this function may be used for scatterplots of frequencies/counts of  overlapping clonotypes</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, <code>filename</code> - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>by_umi</code> <code>bool</code> <p>set <code>=True</code> for MiXCR4 clonosets to select count/frequency of clonotypes  in UMI's if they exist in implemented protocol</p> required <code>by_freq</code> <code>bool</code> <p>default is <code>True</code> - this means that the intersect metric is frequency of clonotype,  but not its count</p> <code>True</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required <p>Important: when using particular overlap type, similar clonotypes in one particular clonoset are combined into one with summation of counts/frequencies.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>clone</code>, <code>sample1_count</code>, <code>sample2_count</code>, <code>sample1</code>, <code>sample2</code>, <code>pair</code> clone - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric count columns contain freq/count of the clone in sample pair column is made for easy separation of possibly huge DataFrame into overlapping pairs</p> Source code in <code>repseq/intersections.py</code> <pre><code>def intersect_clones_in_samples_batch(clonosets_df, cl_filter=None, overlap_type=\"aaV\", by_freq=True, clonosets_df2=None, cl_filter2=None):\n    \"\"\"\n    Calculating frequencies of intersecting clonotypes between multiple repseq samples.\n    The result of this function may be used for scatterplots of frequencies/counts of \n    overlapping clonotypes\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            `filename` - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        by_umi (bool): set `=True` for MiXCR4 clonosets to select count/frequency of clonotypes \n            in UMI's if they exist in implemented protocol\n        by_freq (bool): default is `True` - this means that the intersect metric is frequency of clonotype, \n            but not its count\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n\n    Important: when using particular overlap type, similar clonotypes in one particular clonoset are\n    combined into one with summation of counts/frequencies.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `clone`, `sample1_count`, `sample2_count`, `sample1`, `sample2`, `pair`\n            clone - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric\n            count columns contain freq/count of the clone in sample\n            pair column is made for easy separation of possibly huge DataFrame into overlapping pairs\n    \"\"\"\n\n\n    print(\"Intersecting clones in clonosets\\n\"+\"-\"*50)\n    print(f\"Overlap type: {overlap_type}\")\n\n    clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2 = prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2,\n                                                                                                                        cl_filter, cl_filter2,\n                                                                                                                        overlap_type, by_freq=by_freq,\n                                                                                                                        strict=True)\n    # generating a set of tasks\n\n    tasks = []\n\n    if two_dataframes:\n        for sample1 in sample_list:\n            for sample2 in sample_list2:\n                tasks.append((sample1, sample2, clonoset_lists))\n    else:\n        for i in range(samples_total):\n            sample1 = sample_list[i]\n            for j in range(samples_total-i-1):\n                sample2 = sample_list[j+i+1]\n                tasks.append((sample1, sample2, clonoset_lists))\n\n    results = run_parallel_calculation(intersect_two_clone_dicts, tasks, \"Intersecting clonosets\", object_name=\"pairs\")\n\n    # df = pd.concat(results).index.set_names()\n    df = pd.concat(results).reset_index(drop=True)\n    df = split_tuple_clone_column(df, overlap_type)\n\n    return df\n</code></pre>"},{"location":"page2/#intersections.overlap_distances","title":"<code>overlap_distances(clonosets_df, cl_filter=None, overlap_type='aaV', mismatches=0, metric='F2', clonosets_df2=None, cl_filter2=None)</code>","text":"<p>Calculating overlap distances between multiple repseq samples using F2 of F metrics The result of this function may be used for heatmap+clusterization of samples or for MDS plots</p> <p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>contains three columns - <code>sample_id</code> and <code>filename</code> columns, filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format sample_id's should be all unique in this DF</p> required <code>overlap_type</code> <code>str</code> <p>possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments to decide if clonotypes are equal</p> <code>'aaV'</code> <code>mismatches</code> <code>int</code> <p>Max number of single-letter mismatches in clonotypes sequences  for them to be treated similar, i.e. hamming distance.</p> <code>0</code> <code>by_umi</code> <code>bool</code> <p>set =True for MiXCR4 clonosets to select count/frequency of clonotypes  in UMI's if they exist in implemented protocol</p> required <code>metric</code> <code>str</code> <p>possible values - <code>F</code>, <code>F2</code> or <code>C</code>. Default <code>F2</code>. <code>F2</code> - sum of sqrt of product of  similar clonotype frequencies in two clonosets. <code>F</code> - sqrt of the sum of frequency products. <code>C</code> - total frequency of clonotypes in <code>sample1</code>, that are similar to clonotypes in <code>sample2</code></p> <code>'F2'</code> <code>only_functional</code> <code>bool</code> <p>use only functional clonotypes (do not contain stop codons or frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to 1 after filtering of non-functional clonotypes</p> required similar clonotypes by <code>overlap_type</code> in one particular clonoset will be combined into one <p>clonotype with sum for count.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>clone</code>, <code>sample1_count</code>, <code>sample2_count</code>, <code>sample1</code>, <code>sample2</code>, <code>pair</code>.  <code>clone</code> - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric count columns contain freq/count of the clone in sample pair column is made for easy separation of possibly huge DataFrame into overlapping pairs</p> Source code in <code>repseq/intersections.py</code> <pre><code>def overlap_distances(clonosets_df, cl_filter=None, overlap_type=\"aaV\", mismatches=0, metric=\"F2\", clonosets_df2=None, cl_filter2=None):\n    \"\"\"\n    Calculating overlap distances between multiple repseq samples using F2 of F metrics\n    The result of this function may be used for heatmap+clusterization of samples or for MDS plots\n\n    Args:\n        clonosets_df (pd.DataFrame): contains three columns - `sample_id` and `filename` columns,\n            filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n            sample_id's should be all unique in this DF\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): Max number of single-letter mismatches in clonotypes sequences \n            for them to be treated similar, i.e. hamming distance.\n        by_umi (bool): set =True for MiXCR4 clonosets to select count/frequency of clonotypes \n            in UMI's if they exist in implemented protocol\n        metric (str): possible values - `F`, `F2` or `C`. Default `F2`. `F2` - sum of sqrt of product of \n            similar clonotype frequencies in two clonosets. `F` - sqrt of the sum of frequency products.\n            `C` - total frequency of clonotypes in `sample1`, that are similar to clonotypes in `sample2`\n        only_functional (bool): use only functional clonotypes (do not contain stop codons or\n            frameshifts in CDR3 sequences: * or _ symbol in CDR3aa sequence). The frequences are recounted to\n            1 after filtering of non-functional clonotypes\n\n    Important: similar clonotypes by `overlap_type` in one particular clonoset will be combined into one\n        clonotype with sum for count.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `clone`, `sample1_count`, `sample2_count`, `sample1`, `sample2`, `pair`. \n            `clone` - is tuple, containing sequence (aa or nt), plus V or J if they are required by the metric\n            count columns contain freq/count of the clone in sample\n            pair column is made for easy separation of possibly huge DataFrame into overlapping pairs\n    \"\"\"\n\n\n    print(\"Intersecting clones in clonosets\\n\"+\"-\"*50)\n    aa, check_v, check_j = overlap_type_to_flags(overlap_type)\n    print(f\"Overlap type: {overlap_type}\")\n\n    metric = metric.upper()\n    metrics = [\"F\", \"F2\", \"C\", \"BC\", \"J\", \"JSD\"]\n    mismatch_metrics = [\"F\", \"C\"]\n    non_symmetry_metrics = [\"C\"]\n    frequency_metrics = [\"F\", \"F2\", \"C\"]\n\n\n    if metric not in metrics:\n        raise ValueError(f\"Metric {metric} is not supported. Possible values: {', '.join(metrics)}\")\n\n    if mismatches and metric not in mismatch_metrics:\n        raise ValueError(f\"Metric {metric} does not allow mismatches. Mismatches only possible for: {', '.join(mismatch_metrics)}\")\n\n    by_freq = metric in frequency_metrics\n\n    clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2 = prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2,\n                                                                                                                        cl_filter, cl_filter2,\n                                                                                                                        overlap_type, by_freq=by_freq)\n\n    # generating a set of tasks\n\n    tasks = []\n\n    if two_dataframes:\n        for sample1 in sample_list:\n            for sample2 in sample_list2:\n                tasks.append((sample1, sample2, clonoset_lists, mismatches, metric))        \n    else:\n        if metric not in non_symmetry_metrics and not two_dataframes:\n            for i in range(samples_total):\n                sample1 = sample_list[i]\n                for j in range(samples_total-i-1):\n                    sample2 = sample_list[j+i+1]\n                    tasks.append((sample1, sample2, clonoset_lists, mismatches, metric))\n                if metric == \"F2\":\n                    tasks.append((sample1, sample1, clonoset_lists, mismatches, metric))\n        else:\n            for i in range(samples_total):\n                for j in range(samples_total):\n                    sample1 = sample_list[i]\n                    sample2 = sample_list[j]\n                    if sample1 != sample2:\n                        tasks.append((sample1, sample2, clonoset_lists, mismatches, metric))\n\n\n    # run calculation in parallel\n    result_list = run_parallel_calculation(overlap_metric_two_clone_dicts, tasks, \"Intersecting clonosets\", object_name=\"pairs\")\n\n    if not two_dataframes and metric != \"C\":\n        result_list = result_list + [(result[1], result[0], result[2]) for result in result_list]\n    overlap_df = pd.DataFrame(result_list, columns=[\"sample1\", \"sample2\", metric.lower()]).pivot_table(index=\"sample1\", columns=[\"sample2\"], values=metric.lower()).reset_index().set_index(\"sample1\").fillna(1)\n    return overlap_df\n</code></pre>"},{"location":"page2/#intersections.prepare_clonotypes_dfs_for_intersections","title":"<code>prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2, cl_filter, cl_filter2, overlap_type, by_freq=True, strict=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>clonosets_df</code> <code>DataFrame</code> <p>description</p> required <code>clonosets_df2</code> <code>DataFrame</code> <p>description</p> required <code>cl_filter</code> <code>Filter</code> <p>description</p> required <code>cl_filter2</code> <code>Filter</code> <p>description</p> required <code>overlap_type</code> <code>str</code> <p>description</p> required <code>by_freq</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> <code>ValueError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>clonoset_lists</code> <code>dict</code> <p>dict of </p> <code>samples_total</code> <code>int</code> <code>two_dataframes</code> <code>bool</code> <code>sample_list</code> <code>list</code> <code>sample_list2</code> <code>list</code> Source code in <code>repseq/intersections.py</code> <pre><code>def prepare_clonotypes_dfs_for_intersections(clonosets_df, clonosets_df2, cl_filter, cl_filter2, overlap_type, by_freq=True, strict=False):\n    \"\"\"\n    Args:\n        clonosets_df (pd.DataFrame): _description_\n        clonosets_df2 (pd.DataFrame): _description_\n        cl_filter (Filter): _description_\n        cl_filter2 (Filter): _description_\n        overlap_type (str): _description_\n        by_freq (bool, optional): _description_. Defaults to True.\n\n    Raises:\n        ValueError: _description_\n        ValueError: _description_\n\n    Returns:\n        clonoset_lists (dict): dict of \n        samples_total (int): \n        two_dataframes (bool):\n        sample_list (list):\n        sample_list2 (list):\n    \"\"\"\n    # output:\n    ### clonoset_lists\n\n    if len(clonosets_df.sample_id.unique()) &lt; len(clonosets_df):\n        raise ValueError(\"Input clonosets in DataFrame have non-unique sample_id's\")\n    clonosets_df_1 = clonosets_df[[\"sample_id\", \"filename\"]]\n    two_dataframes = False\n    if isinstance(clonosets_df2, pd.DataFrame):\n        two_dataframes = True\n        if len(clonosets_df2.sample_id.unique()) &lt; len(clonosets_df2):\n            raise ValueError(\"Input clonosets in DataFrame2 have non-unique sample_id's\")\n        clonosets_df_2 = clonosets_df2[[\"sample_id\", \"filename\"]]\n        intersecting_sample_ids = set(clonosets_df2.sample_id.unique()).intersection(set(clonosets_df.sample_id.unique()))\n        if len(intersecting_sample_ids) &gt; 0 and cl_filter2 is not None:\n            print(\"WARNING! Some samples have the same sample_id in two sample_df's. The second filter will be applied to common samples\")\n\n\n    # converting clonosets to compact lists of clonotypes separated by CDR3 lengths to dictionary based on overlap type and count/freq/umi\n    clonoset_lists = convert_clonosets_to_compact_dicts(clonosets_df_1, cl_filter=cl_filter,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=strict)\n    if two_dataframes:\n        if cl_filter2 is None:\n            cl_filter2 = cl_filter\n        clonoset_lists_2 = convert_clonosets_to_compact_dicts(clonosets_df_2, cl_filter=cl_filter2,\n                                                        overlap_type=overlap_type, by_freq=by_freq, strict=strict)\n        clonoset_lists.update(clonoset_lists_2)\n\n    samples_total = len(clonosets_df_1)\n    if two_dataframes:\n        samples_total = len(pd.concat([clonosets_df_1, clonosets_df_2]))\n\n    sample_list = list(clonosets_df_1.sort_values(by=\"sample_id\").sample_id)\n    sample_list2 = None\n    if two_dataframes:\n        sample_list2 = list(clonosets_df_2.sort_values(by=\"sample_id\").sample_id)\n\n    return clonoset_lists, samples_total, two_dataframes, sample_list, sample_list2\n</code></pre>"},{"location":"page2/#intersections.tcrnet","title":"<code>tcrnet(clonosets_df_exp, clonosets_df_control, cl_filter=None, cl_filter_c=None, overlap_type='aaVJ', mismatches=1)</code>","text":"<p>This is an implementation of TCRnet (TCR neighbour enrichment test) algorithm.  It identifies similar clonotypes for the experimental dataset and the control one based on sequence  similarity (allowing up to <code>mismatches</code> differences).   Args:     clonosets_df_exp (pd.DataFrame): a DataFrame with experimental clonosets containing three columns - <code>sample_id</code> and <code>filename</code> columns,         filename - full path to a clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format     clonosets_df_control (pd.DataFrame): a DataFrame with control clonosets containing three columns - <code>sample_id</code> and <code>filename</code> columns,         filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format     cl_filter (repseq.clone_filter.Filter): A filter applied to the experimental dataset before processing     cl_filter_c (repseq.clone_filter.Filter): A filter applied to the control dataset before processing     overlap_type (str): possible values are <code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>. aa/nt define which CDR3 sequence         to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments         to decide if clonotypes are equal     mismatches (int): Max number of single-letter mismatches in clonotype sequences          for them to be treated similar, i.e. hamming distance.</p> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>dataframe with following columns: <code>fold</code>, <code>p_value_b</code>, <code>p_value_p</code>, <code>p_value_b_adj</code>, <code>p_value_p_adj</code>, <code>log10_b_adj</code>, <code>log10_p_adj</code>, <code>log2_fc</code>. <code>p</code> in <code>p_value</code> </p> <p>stands for <code>poisson</code>, <code>b</code> for <code>binomial</code>, <code>adj</code> for multiple testing correction, and <code>log2_fc</code>for log2 fold change</p> Source code in <code>repseq/intersections.py</code> <pre><code>def tcrnet(clonosets_df_exp, clonosets_df_control, cl_filter=None, cl_filter_c=None, overlap_type=\"aaVJ\", mismatches=1):\n\n    \"\"\"\n    This is an implementation of TCRnet (TCR neighbour enrichment test) algorithm.  It identifies similar clonotypes for the experimental dataset and the control one based on sequence \n    similarity (allowing up to `mismatches` differences).    \n    Args:\n        clonosets_df_exp (pd.DataFrame): a DataFrame with experimental clonosets containing three columns - `sample_id` and `filename` columns,\n            filename - full path to a clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n        clonosets_df_control (pd.DataFrame): a DataFrame with control clonosets containing three columns - `sample_id` and `filename` columns,\n            filename - full path to clonoset file. Clonoset file may be of MiXCR3/MiXCR4 or VDJtools format\n        cl_filter (repseq.clone_filter.Filter): A filter applied to the experimental dataset before processing\n        cl_filter_c (repseq.clone_filter.Filter): A filter applied to the control dataset before processing\n        overlap_type (str): possible values are `aa`, `aaV`, `aaVJ`, `nt`, `ntV`, `ntVJ`. aa/nt define which CDR3 sequence\n            to use (amino acid or nucleotide). V/J in the overlap_type define whether to check V or J segments\n            to decide if clonotypes are equal\n        mismatches (int): Max number of single-letter mismatches in clonotype sequences \n            for them to be treated similar, i.e. hamming distance.\n\n    Returns:\n        df (pd.DataFrame): dataframe with following columns: `fold`, `p_value_b`, `p_value_p`, `p_value_b_adj`, `p_value_p_adj`, `log10_b_adj`, `log10_p_adj`, `log2_fc`. `p` in `p_value` \n        stands for `poisson`, `b` for `binomial`, `adj` for multiple testing correction, and `log2_fc`for log2 fold change \n    \"\"\"\n\n    print(\"Running TCRnet neighbour count\\n\"+\"-\"*50)\n    print(f\"Overlap type: {overlap_type}\")\n\n    clonoset_exp = pool_clonotypes_from_clonosets_df(clonosets_df_exp, cl_filter=cl_filter)\n    clonoset_exp_dict = prepare_clonoset_for_intersection(clonoset_exp, overlap_type=overlap_type, by_freq=False, len_vj_format=True)\n\n    unique_clonotypes = [(seq_count[0], *len_vj[1:]) for len_vj, seq_counts in clonoset_exp_dict.items() for seq_count in seq_counts]\n\n    clonoset_control = pool_clonotypes_from_clonosets_df(clonosets_df_control, cl_filter=cl_filter_c)\n    clonoset_control_dict = prepare_clonoset_for_intersection(clonoset_control, overlap_type=overlap_type, by_freq=False, len_vj_format=True)\n\n\n    tasks = []\n    chunks = 40\n    chunk_size = len(unique_clonotypes)//chunks+1\n    for i in range(chunks):\n        first = i*chunk_size\n        last = (i+1)*chunk_size\n        task = (unique_clonotypes[first:last], clonoset_exp_dict, clonoset_control_dict, mismatches)\n        tasks.append(task)\n\n    results = run_parallel_calculation(tcrnet_mp, tasks, \"Calc neighbours (TCRnet)\", object_name=\"parts\")\n    results = list(itertools.chain.from_iterable(results)) # unpack results from several workers\n\n    df = pd.DataFrame(results, columns=[\"clone\", \"count_exp\", \"count_control\", \"group_count_exp\", \"group_count_control\"])\n    df = tcrnet_stats_calc(df)\n    return df\n</code></pre>"},{"location":"supporting_modules/","title":"IO module","text":"<p>This module contains functions for input-output procedures</p>"},{"location":"supporting_modules/#read_yaml_metadata","title":"read_yaml_metadata","text":"<p>Reads NGSiK metadata from a given folder and converts to <code>pd.DataFrame</code>. By default  it searches for <code>metadata.yaml</code> file in this folder and extracts the table.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>path to NGSiK folder</p> required <code>filename</code> <code>str</code> <p>NGSiK metadata filename</p> <code>'metadata.yaml'</code> <code>verbose</code> <code>bool</code> <p>verbosity for </p> <code>True</code> <p>Returns:</p> Name Type Description <code>sample_df</code> <code>DataFrame</code> <p>extracted DataFrame from metadata</p> Source code in <code>repseq/io.py</code> <pre><code>def read_yaml_metadata(folder, filename=\"metadata.yaml\", verbose=True):\n\n    \"\"\"\n    Reads NGSiK metadata from a given folder and converts to `pd.DataFrame`. By default \n    it searches for `metadata.yaml` file in this folder and extracts the table.\n\n    Args:\n        folder (str): path to NGSiK folder\n        filename (str): NGSiK metadata filename\n        verbose (bool): verbosity for \n\n    Returns:\n        sample_df (pd.DataFrame): extracted DataFrame from metadata\n\n    \"\"\"\n\n\n    most_important_columns = [\"sample_id\", \"R1\", \"R2\",\"libraryPerson\", \"projectPerson\", \"projectName\", \"species\", \"miNNNPattern\", \"SMPL\", \"mix_id\", \"preset\", \"startingMaterial\", \"libraryType\"]\n    yaml_filename = os.path.join(folder, filename)\n\n    if os.path.isfile(yaml_filename):\n        with open(yaml_filename, \"r\") as stream:\n            try:\n                metadata_dict =yaml.safe_load(stream)\n        #         pd.io.json.json_normalize(metadata_dict, \"file\", \"samples\", errors='ignore')\n            except yaml.YAMLError as exc:\n                print(exc)\n\n        df = pd.json_normalize(metadata_dict)\n        df = df.explode(\"file\")\n        df = pd.concat([df.drop(['file'], axis=1), df['file'].apply(pd.Series)], axis=1)\n        df = df.explode(\"samples\")\n        df = pd.concat([df.drop(['samples'], axis=1), df['samples'].apply(pd.Series)], axis=1)\n        if 'patternGroupValues' in df.columns:\n            df = pd.concat([df.drop(['patternGroupValues'], axis=1), df['patternGroupValues'].apply(pd.Series)], axis=1)\n        df[\"R1\"] = df[\"R1\"].apply(lambda x: os.path.join(folder, x))\n        df[\"R2\"] = df[\"R2\"].apply(lambda x: os.path.join(folder, x))\n        df = df.rename(columns={\"name\": \"sample_id\"})\n\n        for col_name in most_important_columns[::-1]:\n            if col_name in df.columns:\n                first_column = df.pop(col_name) \n                df.insert(0, col_name, first_column)\n\n        return df.reset_index(drop=True)\n    else:\n        if verbose:\n            print(f\"Metadata file '{yaml_filename}' not found. Nothing to return\")\n        return pd.DataFrame()\n</code></pre>"},{"location":"supporting_modules/#read_clonoset","title":"read_clonoset","text":"<p>Reads generic clonoset files.  Easyly reads <code>csv</code>, <code>tsv</code>, <code>txt</code> or <code>gz</code> files. Reads first found file inside <code>zip</code> files. Clonosets should be in tab-separated format: MiXCR (v3 or v4), vdjtools, Bioadaptive</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>path to clonoset file</p> required <p>Returns:</p> Name Type Description <code>clonoset</code> <code>DataFrame</code> <p>DataFrame representation of clonoset in given file. Bioadaptive clonosets are converted to vdjtools-like format.</p> Source code in <code>repseq/io.py</code> <pre><code>def read_clonoset(filename):\n    \"\"\"\n    Reads generic clonoset files. \n    Easyly reads `csv`, `tsv`, `txt` or `gz` files.\n    Reads first found file inside `zip` files.\n    Clonosets should be in tab-separated format: MiXCR (v3 or v4), vdjtools, Bioadaptive\n\n    Args:\n        filename (str): path to clonoset file\n\n    Returns:\n        clonoset (pd.DataFrame): DataFrame representation of clonoset in given file.\n            Bioadaptive clonosets are converted to vdjtools-like format.\n    \"\"\"\n\n\n    file_name, file_extension = os.path.splitext(filename)\n\n    d_types_mixcr = {'cloneId': int, 'readCount': int, 'readFraction': float,\n                    'uniqueUMICount': int, 'uniqueUMIFraction': float,\n                    'uniqueMoleculeCount': int, 'uniqueMoleculeFraction': float,\n                    'cloneCount': int, 'cloneFraction': float,\n                    'targetSequences': str, 'targetQualities': str,\n                    'allVHitsWithScore': str, 'allDHitsWithScore': str,\n                    'allJHitsWithScore': str, 'allCHitsWithScore': str,\n                    'allVAlignments': str, 'allDAlignments': str,\n                    'allJAlignments': str, 'allCAlignments': str,\n                    'nSeqFR1': str, 'minQualFR1': str,\n                    'nSeqCDR1': str, 'minQualCDR1': str,\n                    'nSeqFR2': str, 'minQualFR2': str,\n                    'nSeqCDR2': str, 'minQualCDR2': str,\n                    'nSeqFR3': str, 'minQualFR3': str,\n                    'nSeqCDR3': str, 'minQualCDR3': str,\n                    'nSeqFR4': str, 'minQualFR4': str,\n                    'aaSeqFR1': str, 'aaSeqCDR1': str,\n                    'aaSeqFR2': str, 'aaSeqCDR2': str,\n                    'aaSeqFR3': str, 'aaSeqCDR3': str,\n                    'aaSeqFR4': str, 'refPoints': str\n                    }\n\n    d_types_vdjtools = {'cdr3aa': str, 'cdr3nt': str,\n                        'v': str, 'd': str, 'j': str,\n                        'CDR3aa': str, 'CDR3nt': str,\n                        'V': str, 'D': str, 'J': str,\n                        'C': str, \"frequency\": float#,\n                        #'count': int, 'freq': float#,\n                        #'VEnd':int, 'DStart':int, 'DEnd':int, \"JStart\":int\n                        }\n\n    d_types_bioadaptive = {'nucleotide': str, 'aminoAcid': str,\n                            'count (templates/reads)': int,\n                            'frequencyCount (%)': float,\n                            'count': int,\n                            'frequencyCount': float,\n                            'vGeneName': str, 'dGeneName': str,\n                            'jGeneName': str, 'cdr3Length': int,\n                            'n1Index': int,'dIndex': int,\n                            'n2Index': int,'jIndex': int\n                            }\n\n\n    datatypes = {**d_types_mixcr,**d_types_vdjtools, **d_types_bioadaptive}\n    if file_extension == \".zip\":\n        archive = zipfile.ZipFile(filename, 'r')\n        inner_filename = zipfile.ZipFile.namelist(archive)[0]\n        filename = archive.open(inner_filename)\n    clonoset = pd.read_csv(filename, sep=\"\\t\", dtype=datatypes)\n    if \"nucleotide\" in clonoset.columns and \"aminoAcid\" in clonoset.columns:\n        clonoset = convert_bioadaptive_clonoset(clonoset)\n    return clonoset\n</code></pre>"},{"location":"supporting_modules/#read_json_report","title":"read_json_report","text":"<p>Reads MiXCR4 json reports into a Python mixed data structure. This function takes the last json record, if for example MiXCR adds up several records  to json file (it happens, when the program is rerun several times on the same data). Program also includes cases when Sample-barcodes are used.</p> <p>Parameters:</p> Name Type Description Default <code>sample_id</code> <code>str</code> <p>sample_id used when running the MiXCR program</p> required <code>folder</code> <code>str</code> <p>folder in which the MiXCR output is stored</p> required <code>report_type</code> <code>str</code> <p>align, refine, assemble</p> required <p>Returns:</p> Name Type Description <code>report</code> <code>dict</code> <p>mixed dict/list python structure, representing the json report</p> Source code in <code>repseq/io.py</code> <pre><code>def read_json_report(sample_id, folder, report_type):\n    \"\"\"\n    Reads MiXCR4 json reports into a Python mixed data structure.\n    This function takes the last json record, if for example MiXCR adds up several records \n    to json file (it happens, when the program is rerun several times on the same data).\n    Program also includes cases when Sample-barcodes are used.\n\n    Args:\n        sample_id (str): sample_id used when running the MiXCR program\n        folder (str): folder in which the MiXCR output is stored\n        report_type (str): align, refine, assemble\n\n    Returns:\n        report (dict): mixed dict/list python structure, representing the json report\n    \"\"\"\n\n\n    filename = os.path.join(folder, f\"{sample_id}.{report_type}.report.json\")\n    if \".\" in sample_id:\n        sample_id2 = \".\".join(sample_id.split(\".\")[:-1])\n        filename2 = os.path.join(folder, f\"{sample_id2}.{report_type}.report.json\")\n        try:\n            report = open_json_report(filename)\n        except FileNotFoundError:\n            report = open_json_report(filename2)\n    else:\n        report = open_json_report(filename)\n    return report\n</code></pre>"},{"location":"usage_clustering/","title":"Usage: clustering","text":"<p>Clustering finds clusters in given clonosets.</p>"},{"location":"usage_clustering/#how-to-create-clusters","title":"How to create clusters","text":"<p><code>create_clusters</code> function args:</p> <ul> <li><code>cl_filter</code>: see the previous page and Filter for further explanations.</li> <li><code>mismatches</code>: specifies the maximum number of mismatches allowed for clonotypes to qualify as neighbours (adjacent).</li> <li><code>overlap_type</code>: Possible overlap types are [<code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>], <code>aa</code>/<code>nt</code> stands for an amino acid or nucleotide sequence, and <code>V</code>/<code>J</code>/<code>VJ</code> denote a segment type.</li> <li>If <code>igh</code>=True, the constant (C) segment is kept.</li> <li>If <code>tcrdist_radius</code> is not None, only edges between clones with a tsrdist metric less than or equal to the specified radius are built. It overrides <code>overlap_type</code> and <code>mismatches</code>. A TCRdist modification is introduced: no gaps are allowed in CDR3, weights are in 3:1 ratio for CDR3 compared to other regions. Also, V-segment distances are pre-calculated and currently are available only for Homo sapiens.</li> <li>If <code>by_freq</code>=True (default), clonotype frequencies are used instead of counts.</li> </ul> <pre><code>from repseq import clustering\n\nclusters = clustering.create_clusters(clonosets_df, cl_filter=top_filter, mismatches=1, overlap_type=\"aaV\", igh=False, tcrdist_radius=None, count_by_freq=True)\n</code></pre> <p>Output is a list of NetworkX Graph() objects \u2014 separate clusters and single nodes. The list is sorted by cluster size.</p> <pre><code>clusters[:3]\n[&lt;networkx.classes.graph.Graph at 0x7fa133b1bd00&gt;,\n &lt;networkx.classes.graph.Graph at 0x7fa15200cb80&gt;,\n &lt;networkx.classes.graph.Graph at 0x7fa154aa3c10&gt;]\n</code></pre> <p></p>"},{"location":"usage_clustering/#clusters-from-a-pooled-dataframe","title":"Clusters from a pooled DataFrame","text":"<p>Alternatively, one can create clusters from a dataframe with clonotypes. Mandatory columns are [<code>freq</code>, <code>count</code>, <code>v</code>, <code>j</code>, <code>cdr3aa</code>, <code>cdr3nt</code>, <code>sample_id</code>].</p> <p><code>pooled_df</code> example:</p> count freq cdr3nt cdr3aa v d j c VEnd DStart DEnd JStart sample_id 0 117 7.46674e-05 TGTGCCAGCAGTCGCCACAGTTACAGGGATGGCTACACCTTC CASSRHSYRDGYTF TRBV12-3 TRBD1 TRBJ1-2 TRBC1 11 22 27 28 sample1_nCD4_1_TRB 1 109 6.95619e-05 TGTGCCAGCAGTTTAGCGCATCAGGGAGGCAGCTATGGCTACACCTTC CASSLAHQGGSYGYTF TRBV12-4 TRBD2 TRBJ1-2 TRBC1 18 23 28 32 sample1_nCD4_1_TRB 2 105 6.70092e-05 TGTGCCAGCAGCCCGGGACTGGCCTACAATGAGCAGTTCTTC CASSPGLAYNEQFF TRBV12-3 TRBD2 TRBJ2-1 TRBC2 10 11 19 22 sample1_nCD4_1_TRB <pre><code>create_clusters_from_pooled_df(pooled_df, mismatches=1, overlap_type=\"aaV\", igh=False, tcrdist_radius=None,\n                                   count_by_freq=True, _run_from_create_clusters=False)\n</code></pre>"},{"location":"usage_clustering/#adding-metadata-to-clusters","title":"Adding metadata to clusters","text":"<p>Metadata, if present, could also be added to node properties prior to saving to Cytoscape. The info will be added to <code>node.additional_properties</code> dictionary. Note that the metadata should contain the same <code>sample_id</code>s that were used in the <code>clonosets_df</code> when creating the clusters. </p> <p>Metadata example:</p> sample_id group type 0 sample1_nCD4_1_TRB 1 nCD4 1 sample1_nCD8_1_TRB 1 nCD8 3 sample2_nCD4_1_TRB 2 nCD4 4 sample2_nCD8_1_TRB 2 nCD8 <pre><code>clustering.add_metadata(clusters, metadata)\n</code></pre> <p></p>"},{"location":"usage_clustering/#save-clusters-in-cytoscape-format","title":"Save clusters in Cytoscape format","text":"<p>Here, we filter out single-node clusters. Clusters are exported in two forms: edges are saved in a .sif file and cluster properties are in a tab-separated .csv file.  In the case of TCRdist, the edges are also assigned a length (radius).</p> <pre><code>clusters_output_prefix = os.path.join(output_dir, \"clusters\")\n# here, one-node clusters are filtered out\nclusters_filtered = clustering.filter_one_node_clusters(clusters)\nclustering.save_clusters_for_cytoscape(clusters_filtered, clusters_output_prefix, sample_metadata=metadata)\n</code></pre> <p></p>"},{"location":"usage_clustering/#cluster-properties","title":"Cluster properties","text":"<p>node size</p> <p>If <code>weighed</code> is set to True, the weight of a node is determined by its size. The size of the node is defined by the <code>by_freq</code> parameter in the <code>create_clusters</code> function, which indicates whether the size is calculated based on counts or frequencies.</p> <p>Cluster properties include consensus CDR3, v- and j-segment sequences, as well as some properties of clusters as graphs:</p> <ul> <li>diameter: the maximum eccentricity in a graph.</li> <li>density: The density is 0 for a graph without edges and 1 for a complete graph. The density of multigraphs can be higher than 1.</li> <li>eccentricity: for a node v, it is the maximum distance from v to all other nodes in a graph. Cluster-wise, it is the average eccentricity of all the nodes within the cluster.</li> </ul> <pre><code>cluster_properties = clustering.cluster_properties(clusters_filtered, weighed=True)\ncluster_properties.to_csv('clusters.tsv', sep='\\t')\n</code></pre> cluster_id nodes edges diameter density eccentricity concensus_cdr3aa concensus_cdr3nt concensus_v concensus_j 0 cluster_0 58 164 10 0.0992136 7.96552 CASSLTGSYEQYF TGCGCCAGCAGCTTGGCAGGGTCCTACGAGCAGTACTTC TRBV5-1 TRBJ2-7 1 cluster_1 47 188 7 0.173913 5.40426 CASSLGGNTEAFF TGCGCCAGCAGCTTGGCAGGGAACACTGAAGCTTTCTTT TRBV5-1 TRBJ1-1 2 cluster_2 38 154 7 0.219061 5.5 CASSLDTYEQYF TGCGCCAGCAGCTTGGACACCTACGAGCAGTACTTC TRBV5-1 TRBJ2-7 3 cluster_3 35 167 6 0.280672 4.45714 CASSLSYEQYF TGTGCCAGCAGTTTAGCCTACGAGCAGTACTTC TRBV12-3 TRBJ2-7 4 cluster_4 33 79 7 0.149621 5.42424 CASSLGTDTQYF TGCGCCAGCAGCTTGGGCACAGATACGCAGTATTTT TRBV5-1 TRBJ2-3 <p>For creating a table with counts or frequencies by cluster, see the previous page.</p> <p></p>"},{"location":"usage_clustering/#sequence-logo","title":"Sequence logo","text":"<p>To visualize cluster's CDR3 consensus sequence, use <code>plot_cluster_logo</code>. Possible <code>seq_type</code> are <code>prot</code> and <code>dna</code>.</p> <pre><code>clustering.plot_cluster_logo(clusters[0])\n</code></pre> <p></p> <p><pre><code>clustering.plot_cluster_logo(clusters[0], seq_type='dna', weighed=True)\n</code></pre> </p> <p></p>"},{"location":"usage_clustering/#custom-cluster-metric-example","title":"Custom cluster metric example","text":"<pre><code>top_filter = clf.Filter(functionality=\"f\", top=4000, by_umi=True, mix_tails=True, seed=100)\ncd4_clusters = clustering.create_clusters(clonosets_df, cl_filter=top_filter, mismatches=1, overlap_type=\"aaV\", igh=False, tcrdist_radius=None, count_by_freq=True)\nclustering.add_metadata(clonosets_df, metadata)\n</code></pre> <p>Metadata in this example:</p> sample_id experimental_group subset 0 UCB4_nCD4_1_TRB late nCD4 4 UCB11_nCD4_1_TRB preterm nCD4 7 UCB10_nCD4_1_TRB term nCD4 8 UCB2_nCD4_1_TRB term nCD4 <p>This function calculates the total frequency of all clonotypes within a cluster, as well as the percentage of clonotypes from different experimental groups present in the cluster.</p> <pre><code>def calc_custom_clusters_properties(clusters):\n    results = []\n    properties = [\"nodes\", \"sum_frequency\", \"preterm_percent\", \"term_percent\", \"late_percent\"]\n\n    for cluster in clusters:\n\n        prop1_nodes = len(cluster)\n        prop2_sum_size = sum([node.size for node in cluster])\n        prop3_sum_size = calc_cluster_preterm_clones_percent(cluster)\n\n        result = [prop1_nodes, prop2_sum_size, *prop3_sum_size]\n\n        results.append(result)\n    df = pd.DataFrame(results, columns = properties)\n    return df\n\ndef calc_cluster_clones_percent(cluster):\n    total_size = sum([node.size for node in cluster])\n    preterm = sum([node.size for node in cluster if node.additional_properties[\"experimental_group\"] == \"preterm\"])\n    term = sum([node.size for node in cluster if node.additional_properties[\"experimental_group\"] == \"term\"])\n    late = sum([node.size for node in cluster if node.additional_properties[\"experimental_group\"] == \"late\"])\n    percent_preterm = round(preterm/total_size*100, 2)\n    percent_term = round(term/total_size*100, 2)\n    percent_late = round(late/total_size*100, 2)\n    return percent_preterm, percent_term, percent_late\n</code></pre> nodes sum_frequency preterm_percent term_percent late_percent 0 73 0.0210043 24.1 62.04 13.86 1 41 0.0115223 44.21 41.43 14.36 2 39 0.0114498 25.18 65.35 9.47 3 38 0.0119551 32.43 55.64 11.93 4 29 0.00817272 36.06 55.56 8.38 <p>To see another example of a custom function for stats calculation, visit stats page.</p> <p></p>"},{"location":"usage_clustering/#community-detection","title":"Community detection","text":"<pre><code>communities_louvain = clustering.find_cluster_communities_louvain(clusters, resolution=1, threshold=1e-07, seed=1)\n</code></pre>"},{"location":"usage_clustering/#alice-antigen-specific-lymphocyte-identification-by-clustering-of-expanded-sequences","title":"ALICE (Antigen-specific Lymphocyte Identification by Clustering of Expanded sequences)","text":"<p>ALICE works by treating clonotypes as graph vertices, with edges connecting sequences which differ by at most 1 CDR3 amino acid. It identifies clonotypes with a higher numbers of neighbors than expected by a null model of recombination, separating clusters of antigen-responding clonotypes from clusters arising from recombination statistics.Currently, it is implemented for H.sapiens only.</p> <pre><code>alice(clusters, overlap_type='aaVJ', mismatches=1, species=\"hs\", olga_warnings=False)\n</code></pre>"},{"location":"usage_diffexp/","title":"Usage: differential expression analysis","text":"<p>Setting up the environment</p> <p>Before getting started, make sure that main_repseq environment is chosen. Otherwise, check the installation guide</p> <pre><code>import sys\nREPSEQ_PATH = 'your_path_to_repseq'\nsys.path.append(REPSEQ_PATH)\nfrom repseq import io as repseqio\nfrom repseq import common_functions as cf\nfrom repseq import mixcr as mx\nfrom repseq import slurm\nfrom repseq import clonosets as cl\nfrom repseq import stats\nfrom repseq import clone_filter as clf\nfrom repseq import intersections\nfrom repseq import clustering\nfrom repseq import diffexp\nimport os\n\nimport pandas as pd\nimport random\nimport json\nimport re\n</code></pre>"},{"location":"usage_diffexp/#input-data","title":"Input data","text":"<p>To run differential expression analysis, two dataframes are required:</p> <ul> <li><code>metadata</code> dataframe. Note that sample_id (must be unique and match <code>count_table</code> column names) and group columns are nessesary for this pipeline. Example:</li> </ul> sample_id Subset Replica Number_of_cells group 0 Luk_TRB_AdV_CD4_1 CD4 1 1376 PepTivator_AdV 1 Luk_TRB_AdV_CD4_2 CD4 2 1504 PepTivator_AdV 2 Luk_TRB_AdV_CD4_3 CD4 3 1623 PepTivator_AdV 3 Luk_TRB_AdV_CD8_1 CD8 1 37698 PepTivator_AdV 4 Luk_TRB_AdV_CD8_2 CD8 2 21359 PepTivator_AdV 5 Luk_TRB_AdV_CD8_3 CD8 3 21031 PepTivator_AdV 6 Luk_TRB_CMV_CD4_1 CD4 1 1018 PepTivator_CMV 7 Luk_TRB_CMV_CD4_2 CD4 2 1032 PepTivator_CMV 8 Luk_TRB_CMV_CD4_3 CD4 3 2788 PepTivator_CMV 9 Luk_TRB_CMV_CD8_1 CD8 1 19617 PepTivator_CMV 10 Luk_TRB_CMV_CD8_2 CD8 2 17490 PepTivator_CMV 11 Luk_TRB_CMV_CD8_3 CD8 3 16662 PepTivator_CMV 12 Luk_TRB_EBV_CD4_1 CD4 1 453 PepTivator_EBV 13 Luk_TRB_EBV_CD4_2 CD4 2 840 PepTivator_EBV 14 Luk_TRB_EBV_CD4_3 CD4 3 770 PepTivator_EBV 15 Luk_TRB_EBV_CD8_1 CD8 1 41274 PepTivator_EBV 16 Luk_TRB_EBV_CD8_2 CD8 2 22020 PepTivator_EBV 17 Luk_TRB_EBV_CD8_3 CD8 3 39014 PepTivator_EBV <ul> <li><code>count_table</code> which can be created with Intersections <code>count_table</code> (for clonotypes of clonotype groups) or <code>count_table_by_cluster</code> (for clusters) functions. Example:</li> </ul> feature_id Luk_TRB_AdV_CD8_1 Luk_TRB_AdV_CD8_2 Luk_TRB_AdV_CD8_3 Luk_TRB_CMV_CD8_1 Luk_TRB_CMV_CD8_2 Luk_TRB_CMV_CD8_3 Luk_TRB_EBV_CD8_1 Luk_TRB_EBV_CD8_2 Luk_TRB_EBV_CD8_3 0 cluster_0 0.329744 0.323807 0.31638 0.178582 0.190506 0.183408 0.194283 0.314896 0.266949 1 cluster_1 0.0875153 0.0826597 0.12148 0.0544798 0.0355812 0.0637625 0.0852074 0.0706873 0.075124 2 cluster_2 0.0126884 0.0273287 0.0147922 0.386493 0.449984 0.361615 0.0247404 0.0136364 0.0211886 3 cluster_3 0.20035 0.172857 0.192064 0.0404682 0.0345952 0.0217206 0.214885 0.180312 0.163982 4 cluster_4 0.00825616 0.0143059 0.0245092 0.00313302 0.0107374 0.00292042 0.00648297 0.00469606 0.00751628 <p>Tips</p> <p>For the <code>count_table</code>, it\u2019s best to use UMI counts. If deep sequencing samples (&gt;30 000 UMIs per sample) are available, it is recommended to downsample them to the same number of UMIs for normalization and faster computation. For details on downsampling filter usage, visit stats usage page.</p> <p>Also, <code>clonosets_df</code> is required for creating a count table. Example (here, a subset of CD8 cells is selected):</p> <pre><code>count_table_by_cluster_cd8\n</code></pre> sample_id chain filename Subset Replica Number_of_cells Peptid 0 Luk_TRB_AdV_CD8_3 TRB path_to_sample_0 CD8 3 21031 PepTivator_AdV 2 Luk_TRB_CMV_CD8_2 TRB path_to_sample_1 CD8 2 17490 PepTivator_CMV 3 Luk_TRB_AdV_CD8_1 TRB path_to_sample_2 CD8 1 37698 PepTivator_AdV 5 Luk_TRB_CMV_CD8_3 TRB path_to_sample_3 CD8 3 16662 PepTivator_CMV 6 Luk_TRB_EBV_CD8_2 TRB path_to_sample_4 CD8 2 22020 PepTivator_EBV 12 Luk_TRB_EBV_CD8_1 TRB path_to_sample_5 CD8 1 41274 PepTivator_EBV 14 Luk_TRB_EBV_CD8_3 TRB path_to_sample_6 CD8 3 39014 PepTivator_EBV 16 Luk_TRB_AdV_CD8_2 TRB path_to_sample_7 CD8 2 21359 PepTivator_AdV 17 Luk_TRB_CMV_CD8_1 TRB path_to_sample_8 CD8 1 19617 PepTivator_CMV <p></p>"},{"location":"usage_diffexp/#count-table-by-cluster","title":"Count table by cluster","text":""},{"location":"usage_diffexp/#creating-clusters","title":"Creating clusters","text":"<p>Here, only functional clonotypes are used (no frameshifts or premature stop codons) and one-node clusters are filtered out. </p> <pre><code>func_filter = clf.Filter(functionality=\"f\", by_umi=True)\n\nclusters_cd8 = clustering.create_clusters(cd8_clonosets, cl_filter=func_filter, mismatches=1, overlap_type=\"aaV\", igh=False, tcrdist_radius=None, count_by_freq=False)\n\nclusters_cd8_nonsingle = clustering.filter_one_node_clusters(clusters_cd8)\n</code></pre>"},{"location":"usage_diffexp/#count_table_by_cluster","title":"<code>count_table_by_cluster</code>","text":"<p>Number of mismatches</p> <p>It is recommended to set <code>mismatches</code> to 0, as the results with either 0 or 1 <code>mismatches</code> will be identical, however, using the former will result in faster computation. When the same clonoset is used for both clustering and intersections, the clone will be already present in the cluster.</p> <pre><code>count_table_by_cluster_cd8 = intersections.count_table_by_cluster(cd8_clonosets, clusters_cd8_nonsingle, cl_filter=func_filter, overlap_type=\"aaV\", mismatches=0, by_freq=True)\n\ncount_table_by_cluster_cd8.head()\n</code></pre> feature_id Luk_TRB_AdV_CD8_1 Luk_TRB_AdV_CD8_2 Luk_TRB_AdV_CD8_3 Luk_TRB_CMV_CD8_1 Luk_TRB_CMV_CD8_2 Luk_TRB_CMV_CD8_3 Luk_TRB_EBV_CD8_1 Luk_TRB_EBV_CD8_2 Luk_TRB_EBV_CD8_3 0 cluster_0 0.329744 0.323807 0.31638 0.178582 0.190506 0.183408 0.194283 0.314896 0.266949 1 cluster_1 0.0875153 0.0826597 0.12148 0.0544798 0.0355812 0.0637625 0.0852074 0.0706873 0.075124 2 cluster_2 0.0126884 0.0273287 0.0147922 0.386493 0.449984 0.361615 0.0247404 0.0136364 0.0211886 3 cluster_3 0.20035 0.172857 0.192064 0.0404682 0.0345952 0.0217206 0.214885 0.180312 0.163982 4 cluster_4 0.00825616 0.0143059 0.0245092 0.00313302 0.0107374 0.00292042 0.00648297 0.00469606 0.00751628 <p></p>"},{"location":"usage_diffexp/#differential-expression-calculation","title":"Differential expression calculation","text":"<p><code>wilcox_diff_expression</code> has two operating modes: </p> <ul> <li>If there are 2 group present, a pairwise comparison is done;</li> <li>If the number of groups is higher than 2, each group is compared to all others, with additional columns for pairwise comparison between groups.</li> </ul> <p>Features (clonotypes or clusters) that appear in <code>min_samples</code> or more samples and have a <code>count_threshold</code> or higher counts or frequencies. This filters out unreliable differences, as the final results undergo FDR correction by the BH method. The default values for <code>min_samples</code> and <code>count_threshold</code> are <code>2</code> and <code>5e-04</code>, respectively. </p>"},{"location":"usage_diffexp/#for-clusters","title":"For clusters","text":"<pre><code>diff_expression_by_cluster = diffexp.wilcox_diff_expression(count_table_by_cluster_cd8, sample_metadata, min_samples=2, count_threshold=0.00005)\n</code></pre> feature_id logFC U p p_adj pair Luk_TRB_AdV_CD8_1 Luk_TRB_AdV_CD8_2 Luk_TRB_AdV_CD8_3 Luk_TRB_CMV_CD8_1 Luk_TRB_CMV_CD8_2 Luk_TRB_CMV_CD8_3 Luk_TRB_EBV_CD8_1 Luk_TRB_EBV_CD8_2 Luk_TRB_EBV_CD8_3 959 cluster_71 100 36 0.000304934 0.0227611 PepTivator_EBV_vs_all 0 0 0 0 0 0 0.00337752 0.00498357 0.000248729 1007 cluster_76 100 36 0.000304934 0.0227611 PepTivator_EBV_vs_all 0 0 0 0 0 0 0.000309606 0.000533954 0.000287593 1072 cluster_83 100 36 0.000304934 0.0227611 PepTivator_CMV_vs_all 0 0 0 0.0018276 0.00372521 0.00638842 0 0 0"},{"location":"usage_diffexp/#for-clonotype-groups","title":"For clonotype groups","text":"<p>Here, a clonotype group is defined based on the <code>overlap_type</code> used during <code>count_table</code> calculation. Possible overlap types are [<code>aa</code>, <code>aaV</code>, <code>aaVJ</code>, <code>nt</code>, <code>ntV</code>, <code>ntVJ</code>], <code>aa</code>/<code>nt</code> stands for an amino acid or nucleotide sequence, and <code>V</code>/<code>J</code>/<code>VJ</code> denote a segment type. </p> <pre><code>count_table_by_clonotypes_aaV_cd8 = intersections.count_table(cd8_clonosets, cl_filter=func_filter, overlap_type=\"aaV\", mismatches=0, by_freq=False))\n</code></pre> feature_id Luk_TRB_AdV_CD8_1 Luk_TRB_AdV_CD8_2 Luk_TRB_AdV_CD8_3 Luk_TRB_CMV_CD8_1 Luk_TRB_CMV_CD8_2 Luk_TRB_CMV_CD8_3 Luk_TRB_EBV_CD8_1 Luk_TRB_EBV_CD8_2 Luk_TRB_EBV_CD8_3 0 ('CASSPGLAAPYSEQFF', 'TRBV12-3') 1 0 0 0 0 0 0 0 0 1 ('CASRQGAGEQYF', 'TRBV19') 1 0 0 0 0 0 0 0 0 2 ('CSAGTTYGTDIISQHF', 'TRBV20-1') 0 4 0 0 0 0 10 0 0 <pre><code>diff_expression_by_clonotypes_aaV = diffexp.wilcox_diff_expression(count_table_by_clonotypes_aaV_cd8, sample_metadata, min_samples=2, count_threshold=2)\n</code></pre> feature_id logFC U p p_adj pair Luk_TRB_AdV_CD8_1 Luk_TRB_AdV_CD8_2 Luk_TRB_AdV_CD8_3 Luk_TRB_CMV_CD8_1 Luk_TRB_CMV_CD8_2 Luk_TRB_CMV_CD8_3 Luk_TRB_EBV_CD8_1 Luk_TRB_EBV_CD8_2 Luk_TRB_EBV_CD8_3 2 ('CASSSGLLGEQFF', 'TRBV6-5') 9.62006 36 0.00118822 0.0481036 PepTivator_EBV_vs_all 0 1 0 0 0 0 421 204 3141 111 ('CASSSGLLDTQYF', 'TRBV6-5') 7.93852 36 0.00118822 0.0481036 PepTivator_EBV_vs_all 0 0 0 0 0 0 1428 1462 614 118 ('CASSRTSGSFLFEQYF', 'TRBV14') 100 36 0.000304934 0.0201417 PepTivator_AdV_vs_all 223 3 7 0 0 0 0 0 0 <p>Output has the following columns:</p> <ul> <li> <p>LogFC: Log fold change of counts / frequencies between two groups. If all the samples in one group have zero counts or frequencies, logFC is set to 100 or -100.</p> </li> <li> <p>U: Mann-Whitney U statistic</p> </li> <li> <p>p: p-value</p> </li> <li> <p>p_adj: p-value after multiple testing correction using Benjamini-Hochberg (BH) procedure</p> </li> <li> <p>Pairwise group comparisons and their raw p-values (_p in a column name) and log fold changes (_logFC in a column name).</p> </li> </ul> <p></p>"},{"location":"usage_diffexp/#merge-cluster-properties-by-feature_id","title":"Merge cluster properties by <code>feature_id</code>","text":"<ul> <li>Create cluster properties</li> </ul> <pre><code>cd8_cluster_props = clustering.cluster_properties(clusters_cd8_nonsingle, weighed=True).rename(columns = {\"cluster_id\": \"feature_id\"})\n\ndiff_expression_by_cluster.merge(cd8_cluster_props)\n</code></pre> feature_id logFC U p p_adj pair Luk_TRB_AdV_CD8_1 Luk_TRB_AdV_CD8_2 Luk_TRB_AdV_CD8_3 Luk_TRB_CMV_CD8_1 Luk_TRB_CMV_CD8_2 Luk_TRB_CMV_CD8_3 Luk_TRB_EBV_CD8_1 Luk_TRB_EBV_CD8_2 Luk_TRB_EBV_CD8_3 nodes edges diameter density eccentricity concensus_cdr3aa concensus_cdr3nt concensus_v concensus_j 0 cluster_71 100 36 0.000304934 0.0227611 PepTivator_EBV_vs_all 0 0 0 0 0 0 0.00337752 0.00498357 0.000248729 7 21 1 1 1 CASSGASGSFNEQFF TGTGCCAGTAGTGGGGCTAGCGGGAGTTTTAATGAGCAGTTCTTC TRBV19 TRBJ2-1 1 cluster_76 100 36 0.000304934 0.0227611 PepTivator_EBV_vs_all 0 0 0 0 0 0 0.000309606 0.000533954 0.000287593 7 21 1 1 1 CASTSHGTSKDNEQFF TGTGCCAGCACGTCTCACGGGACTAGCAAAGACAATGAGCAGTTCTTC TRBV7-6 TRBJ2-1 2 cluster_83 100 36 0.000304934 0.0227611 PepTivator_CMV_vs_all 0 0 0 0.0018276 0.00372521 0.00638842 0 0 0 6 14 2 0.933333 1.33333 CASSFLLGQGADYEQYF TGTGCCAGCAGCTTCCTCCTGGGACAGGGGGCCGACTACGAGCAGTACTTC TRBV11-2 TRBJ2-7 Visualization"},{"location":"usage_intersections/","title":"Usage: intersections between clonosets","text":"<p>To see further details, check the Intersections module.</p>"},{"location":"usage_intersections/#clonoset-intersection","title":"Clonoset intersection","text":"<p><code>intersect_clones_in_samples_batch</code> function performs pairwise clonotype overlapping for all clonosets. Possible overlap types are [aa, aaV, aaVJ, nt, ntV, ntVJ], aa/nt stands for an amino acid or nucleotide sequence, and V/J/VJ denote a segment type.  An output table contains clonotype sequence and V/J segments if required, overlapping clonotypes for each pair, and clonosets they belong to.    </p> <p>clonosets_df and clonosets_df2</p> <p>If <code>clonosets_df2</code> is None (default), samples within <code>clonosets_df</code> are compared with each other;  Otherwise, the comparison is performed exclusively between samples from <code>clonosets_df</code> and <code>clonosets_df2</code>.  If <code>cl_filter2</code> is set, it is applied to clonosets in <code>clonosets_df2</code>. If there are samples with non-unique sample_ids between the two dataframes, both filters will be applied to those samples.</p> <pre><code>from repseq import intersections\nfrom repseq import clone_filter as clf\nfrom repseq import clustering\n\ndownsample_filter = clf.Filter(functionality=\"f\", downsample=15000, by_umi=True, seed=100)\nintersect_df = intersections.intersect_clones_in_samples_batch(clonosets_df, cl_filter=downsample_filter, overlap_type=\"aaV\", by_freq=True)\n</code></pre> cdr3aa v sample1_count sample2_count sample1 sample2 pair 0 CASSLGQVNTEAFF TRBV12-3 6.66667e-05 0 sample1_nCD4_1_TRB sample2_nCD4_1_TRB sample1_nCD4_1_TRB_vs_sample2_nCD4_1_TRB 1 CSARDPASGRVDTQYF TRBV20-1 0 6.66667e-05 sample1_nCD4_1_TRB sample2_nCD4_1_TRB sample1_nCD4_1_TRB_vs_sample2_nCD4_1_TRB 2 CASSPKQGNPYEQYF TRBV18 0 6.66667e-05 sample1_nCD4_1_TRB sample2_nCD4_1_TRB sample1_nCD4_1_TRB_vs_sample2_nCD4_1_TRB 3 CASSWNPTGGTEAFF TRBV5-6 0 6.66667e-05 sample1_nCD4_1_TRB sample2_nCD4_1_TRB sample1_nCD4_1_TRB_vs_sample2_nCD4_1_TRB 4 CASSLLAGGTDTQYF TRBV7-2 0 6.66667e-05 sample1_nCD4_1_TRB sample2_nCD4_1_TRB sample1_nCD4_1_TRB_vs_sample2_nCD4_1_TRB <p></p>"},{"location":"usage_intersections/#overlap-distances-between-clonosets","title":"Overlap distances between clonosets","text":"<p>Calculate overlap distances between clonosets. F, F2, C, J, BC or JCD metric can be used. The mismatches option specifies the maximum number of mismatches allowed for clonotypes to be considered similar. </p> <ul> <li>F2 - clonotype-wise sum of geometric mean frequencies</li> <li>F -  geometric mean of relative overlap frequencies</li> <li>C - total frequency of clonotypes in sample1 that are similar to clonotypes in sample2</li> <li>BC (Bray-Curtis dissimilarity) - sum of differences between clonotype frequencies or counts (<code>by_freq</code>=False) in sample1 and sample2 divided by the total counts in sample1 and sample2  </li> <li>J (Jaccard index) - size of sample1 and sample2 intersection divided by the size of their union</li> <li>JCD (Jensen-Shannon divergence)</li> </ul> <pre><code>f2_ntVJ = intersections.overlap_distances(clonosets, cl_filter=downsample_filter, overlap_type=\"ntVJ\", mismatches=0, metric=\"F2\")\nf_cd4_aaV = intersections.overlap_distances(clonosets_df.query(\"subset=='nCD4'\"), cl_filter=downsample_filter, overlap_type=\"aaV\", mismatches=0, metric=\"F\")\n</code></pre> <p></p>"},{"location":"usage_intersections/#count_table","title":"<code>count_table</code>","text":"<p>Create a table containing the number of times each clonotype appears in each clonoset in <code>clonosets_df</code>.  For <code>overlap_type</code>, possible overlap types are [aa, aaV, aaVJ, nt, ntV, ntVJ], aa/nt stands for an amino acid or nucleotide sequence, and V/J/VJ denote a segment type. </p> <pre><code>count_table = intersections.count_table(clonosets, cl_filter=downsample_filter, overlap_type=\"aaV\", mismatches=0)\n</code></pre> sample1_nCD4_1_TRB sample1_nCD8_1_TRB sample1_nTreg_1_TRB sample2_nCD4_1_TRB sample2_nCD8_1_TRB sample2_nTreg_1_TRB ('CASSLGQVNTEAFF', 'TRBV12-3') 1 0 0 0 0 0 ('CASSPKQGNPYEQYF', 'TRBV18') 0 1 0 0 0 0 ('CASSLLAGGTDTQYF', 'TRBV7-2') 0 1 0 1 1 0 ('CASSHGEGTQYF', 'TRBV3-1') 2 0 0 0 0 0 ('CASSDREGYTEAFF', 'TRBV6-5') 0 1 0 0 0 0"},{"location":"usage_intersections/#count_table_by_cluster","title":"<code>count_table_by_cluster</code>","text":"<p>Create a count table for clusters as opposed to single clonotypes (clusters are provided by the user). They can be created with <code>create_clusters</code> function from clustering module. </p> <pre><code>clusters = clustering.create_clusters(clonosets, cl_filter=top_filter, mismatches=1, overlap_type=\"aaV\", igh=False, tcrdist_radius=None, count_by_freq=True)\nclusters_filtered = clustering.filter_one_node_clusters(clusters)\n</code></pre> <pre><code>count_table_by_cluster = intersections.count_table_by_cluster(clonosets_df, clusters_list, cl_filter=downsample_filter, overlap_type=\"aaV\", mismatches=1)\n</code></pre> feature_id sample1_nCD4_1_TRB sample1_nCD8_1_TRB sample2_nCD4_1_TRB sample2_nCD8_1_TRB 0 cluster_0 0.000133333 0.0008 0.000866667 0.0014 1 cluster_1 0.000333333 0.000333333 0.000666667 0.000866667 2 cluster_2 0.000133333 0.000333333 0.000666667 0.000666667 3 cluster_3 6.66667e-05 0.000333333 0.0008 0.0008 4 cluster_4 0.000333333 0.000133333 0.00106667 6.66667e-05 <p></p>"},{"location":"usage_intersections/#tcrnet","title":"TCRnet","text":"<p>TCRnet compares two datasets with their respective clonosets, typically an experimental dataset and a control one. </p> <pre><code>clonoset_df_exp = ...\nclonoset_df_control = ...\ntcrnet_compared_clns = intersections.tcrnet(clonosets_df_exp, clonoset_df_control, cl_filter=downsampling, overlap_type=\"aaVJ\", mismatches=1)\n</code></pre> clone count_exp count_control group_count_exp group_count_control fold p_value_b p_value_p p_value_b_adj p_value_p_adj log10_b_adj log10_p_adj log2_fc 0 ('CASSPGVGFVEKLFF', 'TRBV11-2', 'TRBJ1-4') 1 1 7 29 4.14286 0.211254 0.20811 0.250473 0.247205 0.601239 0.606943 2.05063 1 ('CASSLMKTENEKLFF', 'TRBV11-2', 'TRBJ1-4') 1 0 7 29 8.28571 0 0 0 0 inf inf 3.05063 2 ('CASSLGGHPNEKLFF', 'TRBV11-2', 'TRBJ1-4') 1 0 7 29 8.28571 0 0 0 0 inf inf 3.05063 Visualization <p>Properties from proc_table can be visualized in Jupyter notebook using %%R cell magic.  </p> <pre><code>%%R -i intersect_df -h 600 -w 700\nintersect_df %&gt;% \n    ggplot(aes(x=sample1_count, y=sample2_count)) +\n        geom_point()+\n        theme_bw()+\n        facet_wrap(vars(pair)) +\n        scale_x_log10(limits = c(1e-5, 3.5e-03)) +\n        scale_y_log10(limits = c(1e-5, 3.5e-03))\n</code></pre>"},{"location":"usage_mixcr/","title":"Usage: working with MiXCR","text":"<p>MiXCR is the leading software for generating clonoset tables from raw FastQ files. MiXCR module allows to run MiXCR 4.3+ batch analyses with SLURM queue manager.</p> <p>Setting up the environment</p> <p>Before getting started, make sure that main_repseq environment is chosen. Otherwise, check the installation guide</p>"},{"location":"usage_mixcr/#working-with-metadata-and-creating-a-dataframe-with-samples","title":"Working with metadata and creating a dataframe with samples","text":"<p>Create <code>sample_df</code> from dataset metadata in <code>.yaml</code> format (if it's in a tabular format, use external libraries such as Pandas). Remove unnesessary columns if needed. Note If METADATA_FILENAME is absent, it is set to <code>metadata.yaml</code> by default. Note that this is relevant only for metadata created with NGSiK in CDR3.net group.  If your dataset does not have metadata, create the dataframe manually. The neccessary columns are: <code>R1</code>, <code>R2</code>, <code>sample_id</code>, where <code>R1</code> and <code>R2</code> contain paths (using full paths is strongly advised) to respective raw files, and sample_id are arbitrary unique identificators.</p> <pre><code>from repseq import mixcr as mx\nfrom repseq import slurm\nfrom repseq import io as repseqio\n\nsample_df = repseqio.read_yaml_metadata(RAW_DATA_DIR, filename=METADATA_FILENAME)\nmetadata = sample_df.prop(columns=['R1', 'R2'])\noutput_dir = ...\npath_to_mixcr_binary = ...\n</code></pre> <p><code>sample_df</code> example:</p> sample_id R1 R2 0 sample_1_nCD4 /home/user/samples/sample1_nCD4_1_TRB_L001_R1_001.fastq.gz /home/user/samples/sample1_nCD4_1_TRB_L001_R2_001.fastq.gz 1 sample_2_nCD4 /home/user/samples/sample2_nCD4_1_TRB_L001_R1_001.fastq.gz /home/user/samples/sample2_nCD4_1_TRB_L001_R2_001.fastq.gz 2 sample_3_nCD4 /home/user/samples/sample3_nCD4_1_TRB_L001_R1_001.fastq.gz /home/user/samples/sample3_nCD4_1_TRB_L001_R2_001.fastq.gz <p></p>"},{"location":"usage_mixcr/#command-template-for-mixcr-analyze","title":"Command template for <code>mixcr analyze</code>","text":"<p>Create a command template for mixcr analyze. The default template is <code>mixcr analyze milab-human-rna-tcr-umi-multiplex -f r1 r2 output_prefix</code> for Milab Hum TCR RNA multiplex kit. The default values are 32 GB for <code>memory</code> (required OOM in GB),  1.5 hours for <code>time_estimate</code> and 40 for <code>cpus</code> (in case of Aldan3 server, it is the size of a smallest node). Note that <code>mixcr analyze</code> and <code>r1 r2 output_prefix</code> are \"magical\" parts of the template that should be kept as-is in the template, so change only the part in-between these parts. For more detailed information on MiXCR presets, visit the MiXCR website.</p> <p><pre><code>mixcr_race_command_template = \"mixcr analyze milab-mouse-rna-tcr-umi-race -f r1 r2 output_prefix\"\n</code></pre> </p>"},{"location":"usage_mixcr/#running-mixcr-analyze-in-batches-using-slurm","title":"Running <code>mixcr analyze</code> in batches using SLURM","text":"<p>Run mixcr analyze in batches (Relevant only for servers using SLURM). This function generates a set of commands for each sample by creating a SLURM script for each command and submitting them to the SLURM queue. Scripts itself and .log files (contain std.out and std.error outputs) will be saved in <code>~/temp/SLURM</code>. </p> <pre><code>mx.mixcr4_analyze_batch(sample_df, output_dir, command_template=mixcr_race_command_template,\n                        path_to_mixcr_binary)\n</code></pre> <p></p> <p>To check the progress, use <code>check_slurm_progress</code>. <code>loop</code> set to <code>True</code> gives real-time updates with 0.5 s interval while <code>loop</code>=<code>False</code> shows current progress and runs in the background without blocking other cells.  Currently, <code>check_slurm_progress</code> might be unreliable in some cases, thus it's recommended to check SLURM queue and .log files in <code>~/temp/SLURM</code> folder manually.</p> <p><pre><code>slurm.check_slurm_progress(os.path.join(output_dir, \"mixcr_analyze_slurm_batch.log\"), loop=True)\n</code></pre> </p>"},{"location":"usage_mixcr/#making-report-images","title":"Making report images","text":"<p>Make reports (combines <code>mixcr exportQc align</code>, <code>chainUsage</code> and <code>tags</code>) and get report images (both .pdf and .svg for <code>align</code> and <code>chainUsage</code>, only .pdf for <code>tags</code>). To see report images examples, visit the MiXCR website.</p> <ul> <li>align \u2014 exports various quality control metrics</li> <li>chainUsage \u2014 calculates chain usage across all clonotypes</li> <li>tags \u2014 for samples with barcodes, provides barcode coverage statistics for every sample</li> </ul> <p>To see progress, use <code>check_slurm_progress</code> as shown below</p> <pre><code>mx.mixcr4_reports(output_dir, mixcr_path=path_to_mixcr_binary)\nslurm.check_slurm_progress(os.path.join(output_dir, \"mixcr_reports_slurm_batch.log\"), loop=True)\nmx.show_report_images(output_dir)\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"usage_mixcr/#creating-a-table-containing-clonosets-stats","title":"Creating a table containing clonosets stats","text":"<p>Get a tabular report using <code>get_processing_table</code> function. It searches for clonosets in the the folder, extracts their sample_id's and shows main processing stats in a table format. By default does not show \"off-target\" clonosets -  those having less than 1% (default, may be overriden) of reads for the sample_id. For example, you have sequenced TRB sample, but there is found 0.5% (by read count)  of TRA chains for the same sample_id, then the clonoset will not be shown in the table. You can specify <code>show_offtarget=True</code> to display all found chains in the table or  outherwise set a higher value for <code>offtarget_chain_threshold</code> (<code>0.01</code> by default).</p> <pre><code>proc_table = mx.get_processing_table(output_dir)\n</code></pre> <p>A full processing table example:</p> sample_id extracted_chain reads_total reads_with_umi_pc reads_aligned_pc reads_overlapped_aln_pc total_umi umi_after_correction overseq_threshold reads_after_filter umi_after_filter reads_per_umi clones_total reads_in_clones_total clones reads_in_clones clones_func reads_in_func_clones umi_in_clones umi_in_func_clones 0 sample1_nCD4_1_TRB TRB 2120957 98.63 86.38 4.97 597401 564176 2 1612478 344972 4.67 145019 1566962 145012 1566949 135644 1509856 349587 337223 3 sample2_nCD4_1_TRB TRB 1484339 98.64 85.43 3.49 242845 223565 3 1183345 155008 7.63 69771 1167512 69770 1167509 66237 1133350 156334 151913 6 sample3_nCD4_1_TRB TRB 940861 96.49 86.42 2.84 279362 236455 2 734134 157646 4.66 68466 715176 68465 715174 64208 690533 160301 154995 <p>Some columns may be omitted for better readability:</p> <pre><code>small_proc_table = proc_table[[\"sample_id\", \"extracted_chain\",\"reads_total\", \"reads_with_umi_pc\", \"reads_aligned_pc\", \"reads_per_umi\", \"overseq_threshold\",\"clones_func\", \"umi_in_func_clones\"]]\n</code></pre> <p>Columns:</p> Parameters Description sample_id sample_id specified in the clonoset filename extracted_chain clonoset \u0441hain specified in the clonoset filename (e.g., TRA, IGH). One sample (particular <code>sample_id</code>) might include multiple chains depending on the protocol used and the values of the <code>show_offtarget</code> and the <code>offtarget_chain_threshold</code> parameters reads_total a total number of raw reads for the whole sample reads_with_umi_pc Percentage of reads with barcodes if tag-pattern was used reads_aligned_pc Percentage of successfully aligned reads for the whole sample reads_overlapped_aln_pc Percentage of overlapping reads of all aligned reads total_umi Total number of UMIs before any filtering umi_after_correction Number of UMIs after PCR and sequencing error correction overseq_threshold Reads per group threshold used for filtering and selected by MiXCR reads_after_filter Number of reads after filtering umi_after_filter Number of UMIs after filtering reads_per_umi Average number of reads per UMI clones_total Total number of clonotypes according to MiXCR assemble report reads_in_clones_total Number of reads assigned to clonotypes for the whole sample clones Number of clonotypes in a clonoset for a given chain reads_in_clones Number of reads in a clonoset for a given chain clones_func Number of functional clonotypes (no frameshifts and stops) reads_in_func_clones Number of reads in functional clonotypes umi_in_clones Number of UMIs in a clonoset for a given chain umi_in_func_clones Number of UMIs in functional clonotypes <p></p> Visualization <p>Properties from proc_table can be visualized in Jupyter notebook using %%R cell magic.  </p> <pre><code>%load_ext rpy2.ipython\n%%R -i proc_table -w 900 -h 500\n\nparams_order &lt;- c(\"reads_with_umi_pc\", \"reads_aligned_pc\",\"reads_per_umi\",\"umi_in_func_clones\",\"clones_func\")\n\nproc_table %&gt;%\n    select(sample_id, experimental_group, subset, reads_per_umi, reads_with_umi_pc, reads_aligned_pc, clones_func, umi_in_func_clones) %&gt;%\n    pivot_longer(-c(sample_id, experimental_group, subset), names_to=\"parameter\", values_to=\"value\") %&gt;%\n    mutate(experimental_group=factor(experimental_group, group_order)) %&gt;%\n    mutate(parameter=factor(parameter, params_order)) %&gt;%\n    ggplot(aes(x=experimental_group, y=value, color=experimental_group)) +\n        geom_boxplot(outlier.shape=NA)+\n        geom_jitter()+\n        facet_wrap(vars(parameter), scales=\"free_y\")+\n        scale_color_manual(values=colors_6_groups) + \n        boxplot_theme+\n        theme(legend.position=\"none\")\n</code></pre>"},{"location":"usage_stats/","title":"Usage: calculating basic stats for a clonoset","text":"<p>Stats module allows to calculate various stats for all clonosets and for individual clonosets.</p>"},{"location":"usage_stats/#working-with-clonosets","title":"Working with clonosets","text":"<p>To read all clonosets (.tsv format, MiXCR3/4 typical output names, VDJtools or Bioadaptive formats) in a directory or several directories, use <code>find_all_exported_clonosets</code>. </p> <pre><code>from repseq import clonosets as cl\nfrom repseq import stats\nfrom repseq import clone_filter as clf\nfrom repseq import io as repseqio\nfrom repseq import vdjtools\n\nclonosets_dir_or_dirs = '/home/user/sample/mixcr'\nclonosets = cl.find_all_exported_clonosets(clonosets_dir_or_dirs).sort_values(by=\"sample_id\").reset_index(drop=True)\n</code></pre> <p>Output table example:</p> sample_id chain filename 0 sample_1_nCD4_1_TRB TRB /home/user/samples/mixcr/sample_1_nCD4_1_TRB.clones_TRB.tsv 1 sample_2_nCD4_1_TRB TRB /home/user/samples/mixcr/sample_2_nCD4_1_TRB.clones_TRB.tsv 2 sample_3_nCD4_1_TRB TRB /home/user/samples/mixcr/sample_3_nCD4_1_TRB.clones_TRB.tsv"},{"location":"usage_stats/#vdjtools","title":"VDJtools","text":"<p>To convert clonosets (in a form of a dataframe) to VDJtools format, use:</p> <pre><code>vdjtools.save_to_vdjtools(clonosets, \"/home/user/samples/vdjtools_folder/\")\n</code></pre>"},{"location":"usage_stats/#reading-a-single-clonoset-into-pddataframe","title":"Reading a single clonoset into pd.DataFrame","text":"<p>To read a single clonoset in a tab-separated format (.tsv, .txt, .tsv.gz or .zip (reads the first file)) format, use <code>read_clonoset</code> function from <code>io</code> module: <pre><code>clonoset_df = repseqio.read_clonoset(path_to_clonoset)\n</code></pre></p>"},{"location":"usage_stats/#filtering-clonosets","title":"Filtering clonosets","text":"<p>Filter is a special object, that may be used as a setup for Postanalysis. You can easily create or change it and put as an argument to other functions for individual clonoset or multi-clonoset metrics.  For <code>functionality</code>, possible values are: </p> <ul> <li><code>a</code> - any (default). No clones are filtered out.</li> <li><code>f</code> - only functional. Those not having stop codons and frameshifts in CDR3 regions, or having non-empty values in CDR3 amino-acid sequence.</li> <li><code>n</code> - only-nonfunctional - opposite to <code>f</code> - functional.</li> </ul> <p> Other commonly used parameters:</p> <ul> <li>Using <code>seed</code> is highly advised when using top and downsample filters. Setting a specific value ensures the reproducibility as these filters use pseudorandom number generation.</li> <li>With <code>by_umi</code> = True, Filter() uses counts based on UMI if the corresponding columns are present; otherwise, counts are based on reads.</li> <li><code>mix_tails</code> = True is recommended for top filter, as sorting with identical counts may not be random.</li> </ul> <p>To see all possible parameters and their description, visit clone_filter module description.</p> <p>Most commonly used filters:</p> <ul> <li> <p>by functionality: only functional clones (no frameshifts and stops), counts by UMI <pre><code>func_filter = clf.Filter(functionality=\"f\", by_umi=True)\n</code></pre></p> </li> <li> <p>by functionality, takes top clonotypes by UMI count. <code>Seed</code> parameter is used for reproducibility <pre><code>top_filter = clf.Filter(functionality=\"f\", top=4000, by_umi=True, mix_tails=True, seed=100)\n</code></pre></p> </li> <li> <p>by functionality, count by UMI, randomly samples a clonoset down to 15000 UMI  <pre><code>downsample_filter = clf.Filter(functionality=\"f\", downsample=15000, by_umi=True, seed=100)\n</code></pre></p> </li> <li> <p>by functionality, all clonotypes with UMI count less than <code>count_threshold</code> will be filtered out <pre><code>count_threshold_filter = clf.Filter(functionality=\"f\", count_threshold=3, by_umi=True)\n</code></pre></p> </li> </ul> <p> Filtering a clonoset: <pre><code>filtered_clonoset_df = top_filter.apply(clonoset_df)\n</code></pre></p>"},{"location":"usage_stats/#clonoset-stats","title":"Clonoset stats","text":"<p>Calc stats for clonoset size in clones, reads and UMIs</p> <pre><code>clonoset_stats = stats.calc_clonoset_stats(clonosets)\n</code></pre> sample_id chain clones clones_func clones_func_singletons clones_func_non_singletons clones_nonfunc clones_nonfunc_freq reads reads_func reads_nonfunc reads_nonfunc_freq umi umi_func umi_nonfunc umi_nonfunc_freq 0 sample1_nCD4_1_TRB TRB 145012 135644 49523 86121 9368 0.0646016 1566949 1509856 57093 0.0364358 349587 337223 12364 0.0353674 1 sample2_nCD4_1_TRB TRB 134150 126556 48485 78071 7594 0.0566083 772217 746989 25228 0.0326696 312575 302754 9821 0.0314197 2 sample3_nCD4_1_TRB TRB 68965 64585 24802 39783 4380 0.0635105 793340 766721 26619 0.0335531 163789 158403 5386 0.0328838 <p>Calculating CDR3 properties. In this example, only functional clonotypes (=no frameshifts or stops) are used.</p> <p>basic stats for CDR3 regions. CDR3 amino acid sequence properties (both full sequence and central 5-residue sequence (closer to N-term in case of even length))</p> <pre><code>func_filter = clf.Filter(functionality=\"f\", by_umi=True)\ncdr3_properties = stats.calc_cdr3_properties(clonosets, cl_filter=func_filter)\n</code></pre> sample_id chain mean_cdr3nt_len mean_insert_size zero_insert_freq mean_frequency cdr3_5_hydropathy cdr3_full_hydropathy cdr3_5_charge cdr3_full_charge cdr3_5_polarity cdr3_full_polarity cdr3_5_volume cdr3_full_volume cdr3_5_strength cdr3_full_strength cdr3_5_mjenergy cdr3_full_mjenergy cdr3_5_kf1 cdr3_full_kf1 cdr3_5_kf2 cdr3_full_kf2 cdr3_5_kf3 cdr3_full_kf3 cdr3_5_kf4 cdr3_full_kf4 cdr3_5_kf5 cdr3_full_kf5 cdr3_5_kf6 cdr3_full_kf6 cdr3_5_kf7 cdr3_full_kf7 cdr3_5_kf8 cdr3_full_kf8 cdr3_5_kf9 cdr3_full_kf9 cdr3_5_kf10 cdr3_full_kf10 cdr3_5_rim cdr3_full_rim cdr3_5_surface cdr3_full_surface cdr3_5_turn cdr3_full_turn cdr3_5_alpha cdr3_full_alpha cdr3_5_beta cdr3_full_beta cdr3_5_core cdr3_full_core cdr3_5_disorder cdr3_full_disorder 0 sample1_nCD4_1_TRB TRB 43.1311 5.91418 0.0669883 7.37224e-06 -3.6533 -3.49631 0.145473 -0.25535 2.47028 7.38906 457.085 1393.35 0.929842 4.54233 -14.1225 -44.0132 1.87336 1.88425 -2.91408 -5.49689 0.0863277 -0.583377 0.899116 -0.293989 -1.00823 -2.86921 -1.18772 -1.7715 0.94773 0.770087 1.27514 0.193953 -1.9049 -0.106618 0.368452 -1.06328 0.306982 0.767234 0.310197 0.797021 5.81511 14.9261 4.4949 14.1549 4.83404 14.1038 0.290491 0.776406 2.31293 2.60787 1 sample2_nCD4_1_TRB TRB 42.6894 5.45495 0.0937428 7.90164e-06 -3.63854 -3.7497 -0.070658 -0.65764 2.52354 7.43611 453.382 1376.69 0.954769 4.53017 -14.1263 -43.5042 1.95697 1.73002 -2.96411 -5.37987 -0.189644 -1.12346 0.76061 -0.362696 -0.959487 -2.87697 -1.19542 -1.66914 0.654523 0.440107 1.30386 0.341753 -1.90955 -0.0793168 0.297878 -1.35161 0.306715 0.760597 0.310945 0.791057 5.87189 14.8052 4.49732 14.0927 4.81489 13.9093 0.289834 0.766072 2.2879 2.64048 2 sample3_nCD4_1_TRB TRB 43.1306 5.53264 0.109455 1.54835e-05 -3.5389 -3.36705 0.226643 -0.13198 2.45753 7.38413 463.875 1401.24 1.01215 4.64907 -14.3082 -44.2336 1.88087 1.95313 -2.68089 -5.22143 0.141751 -0.455098 0.772542 -0.446082 -1.0089 -2.85593 -1.24996 -1.84142 0.968762 0.771571 1.23777 0.206734 -1.78501 0.0142436 0.431144 -1.00567 0.302192 0.761093 0.305975 0.791293 5.76219 14.8705 4.50163 14.148 4.8573 14.1363 0.29094 0.777235 2.11711 2.3694 <p>Calculating diversity stats. It includes observed diversity, Shannon-Wiener, normalized Shannon-Wiener and chao1 index for each clonoset in clonosets_df. Here, a top_n filter is applied.</p> <pre><code>diversity_stats = stats.calc_diversity_stats(clonosets, cl_filter=downsample_filter, seed=123)\n</code></pre> sample_id chain shannon_wiener norm_shannon_wiener diversity clonality chao1 0 sample1_nCD4_1_TRB TRB 9.53229 0.997617 14116 0.00238349 125744 1 sample2_nCD4_1_TRB TRB 9.52557 0.997336 14059 0.00266397 121758 2 sample3_nCD4_1_TRB TRB 9.43915 0.994989 13183 0.005011 61315.5 <p>Calculating convergence (=the number of unique CDR3 nucleotide sequences that code for the same amino acid sequence) for each clonoset in clonosets_df.  <pre><code>convergence = stats.calc_convergence(clonosets, cl_filter=top_filter)\n</code></pre></p> sample_id chain convergence 0 sample1_nCD4_1_TRB TRB 1.01114 1 sample2_nCD4_1_TRB TRB 1.03426 2 sample3_nCD4_1_TRB TRB 1.02362 <p>Segment usage (combined frequency of segments) can be calculated for V/J/C-segments. All possible options are [\"v\", \"j\", \"c\", \"vj\", \"vlen\", \"vjlen\"]. <code>vj</code> - usage of combinations of <code>v</code> and <code>j</code> segments. <code>vlen</code> and <code>vjlen</code> options also take the length of amimo acid CDR3 length into account and calculate usage for particular combination.</p> <p>The resulting dataframe can be in either <code>long</code> or <code>wide</code> format:</p> <ul> <li><code>long</code> - four columns: <code>sample_id</code>, <code>chain</code>, <code>&lt;segment_type&gt;</code>, <code>usage</code></li> <li><code>wide</code> - num of rows equals to the number of input clonosets, and all segments are the columns and usage is in each cell.</li> </ul> <pre><code>v_usage = stats.calc_segment_usage(clonosets, segment=\"v\", cl_filter=func_filter, table=\"long\")\n</code></pre>"},{"location":"usage_stats/#custom-stats","title":"Custom stats","text":"<p>Stats module has a special function <code>generic_calculation</code> which performs multiple individual clonoset statistic calculation. It runs all individual clonoset calculations in parallel.</p> <p>Instruction:</p> <ul> <li>First, create a <code>function_name_cl</code> which performs individual clonoset calculation;</li> <li>Create a main wrapper function named <code>function_name</code>, which passes <code>function_name_cl</code> and <code>clonosets_df</code> to <code>generic_calculation</code>.</li> </ul> <p>In this example, Crohn's-associated invariant T cells (CAITs) are identified across clonosets.</p> <pre><code>import re\n\ndef find_caits(clonosets_df, cl_filter=None):\n    df = stats.generic_calculation(clonosets_df, find_caits_cl, clonoset_filter=cl_filter, program_name=\"Find CAITs\")\n    return df\n\ndef find_caits_cl(clonoset_in, colnames=None):\n    clonoset = clonoset_in.copy()\n\n    # find colnames for freq, count, v, j and so on\n    if colnames is None:\n        colnames = cl.get_column_names_from_clonoset(clonoset)\n\n    # create a motif to search for\n    cait_motif = re.compile(r'CVV[A-Z]{2}A[A-Z]{1}GGSYIPTF')\n    trav = \"TRAV12-1\"\n    traj = \"TRAJ6\"\n\n    # perform calculation of motif abundance\n    clonoset[\"cait_cdr3aa\"] = clonoset[colnames[\"cdr3aa_column\"]].apply(lambda x: cait_motif.search(x) is not None)\n\n    clonoset = clonoset.loc[clonoset[colnames[\"v_column\"]] == trav]\n    trav_freq = clonoset[colnames[\"fraction_column\"]].sum()\n\n    clonoset = clonoset.loc[clonoset[colnames[\"j_column\"]] == traj]\n    trav_traj_freq = clonoset[colnames[\"fraction_column\"]].sum()\n\n    clonoset = clonoset.loc[clonoset[\"cait_cdr3aa\"]]\n    cait_freq = clonoset[colnames[\"fraction_column\"]].sum()\n    cait_clonotypes = len(clonoset)\n\n    # save it to dictionary\n    result_dict = {\"cait_clonotypes\": cait_clonotypes,\n                   \"cait_freq\": cait_freq,\n                   \"trav12_1_freq\": trav_freq,\n                   \"v12_1_j6_freq\": trav_traj_freq}\n\n    return result_dict\n\ntra_clonosets = cl.find_all_exported_clonosets(\"/projects/cdr3_common/repseq_demo/custom_stats_clonosets/\")\nclonoset_caits = find_caits(tra_clonosets, cl_filter=func_filter)\n</code></pre> sample_id chain cait_clonotypes cait_freq trav12_1_freq v12_1_j6_freq 0 sample1_nCD4_1_TRB TRA 4 7.93676e-05 0.0382234 0.00144449 1 sample2_nCD4_1_TRB TRA 1 3.55821e-05 0.0360803 0.00170794 2 sample3_nCD4_1_TRB TRA 5 0.000124894 0.0386671 0.002348 Convergence visualization <p>Calculated stats can be visualized in Jupyter notebook using %%R cell magic.  </p> <pre><code>%load_ext rpy2.ipython\n%%R -i convergence,metadata -w 400 -h 300\n\nparams_order &lt;- c(\"convergence\")\n\nconvergence %&gt;%\n    merge(metadata) %&gt;%\n    select(sample_id, experimental_group, subset, convergence) %&gt;%\n    pivot_longer(-c(sample_id, experimental_group, subset), names_to=\"parameter\", values_to=\"value\") %&gt;%\n    mutate(experimental_group=factor(experimental_group, group_order)) %&gt;%\n    mutate(parameter=factor(parameter, params_order)) %&gt;%\n    ggplot(aes(x=experimental_group, y=value, color=experimental_group)) +\n        geom_boxplot(outlier.shape=NA)+\n        geom_jitter()+\n        facet_wrap(vars(parameter), scales=\"free_y\")+\n        scale_color_manual(values=colors_6_groups) + \n        boxplot_theme+\n        theme(legend.position=\"none\")\n</code></pre> Diversity visualization <p>Calculated stats can be visualized in Jupyter notebook using %%R cell magic.  </p> <pre><code>%load_ext rpy2.ipython\n%%R -i diversity_stats,metadata -w 900 -h 300\n\nparams_order &lt;- c(\"diversity\", \"norm_shannon_wiener\", \"chao1\")\n\ndiversity_stats %&gt;%\n    merge(metadata) %&gt;%\n    select(sample_id, experimental_group, subset, diversity, norm_shannon_wiener, chao1) %&gt;%\n    pivot_longer(-c(sample_id, experimental_group, subset), names_to=\"parameter\", values_to=\"value\") %&gt;%\n    mutate(experimental_group=factor(experimental_group, group_order)) %&gt;%\n    mutate(parameter=factor(parameter, params_order)) %&gt;%\n    ggplot(aes(x=experimental_group, y=value, color=experimental_group)) +\n        geom_boxplot(outlier.shape=NA)+\n        geom_jitter()+\n        facet_wrap(vars(parameter), scales=\"free_y\")+\n        scale_color_manual(values=colors_6_groups) + \n        boxplot_theme+\n        theme(legend.position=\"none\")\n</code></pre> CDR3 properties visualization <p>Calculated stats can be visualized in Jupyter notebook using %%R cell magic.  </p> <pre><code>%load_ext rpy2.ipython\n%%R -i cdr3_properties,metadata -w 900 -h 500\n\nparams_order &lt;- c(\"mean_cdr3nt_len\", \"mean_insert_size\", \"zero_insert_freq\", \"cdr3_5_charge\", \"cdr3_5_kf4\")\n\ncdr3_properties %&gt;%\n    merge(metadata) %&gt;%\n    select(sample_id, experimental_group, subset, mean_cdr3nt_len, mean_insert_size, zero_insert_freq, cdr3_5_charge, cdr3_5_kf4) %&gt;%\n    pivot_longer(-c(sample_id, experimental_group, subset), names_to=\"parameter\", values_to=\"value\") %&gt;%\n    mutate(experimental_group=factor(experimental_group, group_order)) %&gt;%\n    mutate(parameter=factor(parameter, params_order)) %&gt;%\n    ggplot(aes(x=experimental_group, y=value, color=experimental_group)) +\n        geom_boxplot(outlier.shape=NA)+\n        geom_jitter()+\n        facet_wrap(vars(parameter), scales=\"free_y\")+\n        scale_color_manual(values=colors_6_groups) + \n        boxplot_theme+\n        theme(legend.position=\"none\")\n</code></pre> V-segment usage visualization <p>Calculated stats can be visualized in Jupyter notebook using %%R cell magic.  </p> <pre><code>%load_ext rpy2.ipython\n%%R -i v_usage,metadata -w 1200 -h 600\n\nv_usage_order &lt;- v_usage %&gt;% select(v) %&gt;% distinct() %&gt;%\n  separate(v, into = c(\"f\", \"l\"), remove = F, convert = T) %&gt;%\n  separate(f, into = c(\"s\", \"n\"), remove = T, convert = T, sep=\"V\") %&gt;%\n  arrange(n,l) %&gt;% pull(v)\n\nv_usage %&gt;%\n    merge(metadata) %&gt;%\n    mutate(v=factor(v, v_usage_order)) %&gt;%\n    ggplot(aes(x=v, y=usage, fill=experimental_group)) +\n        stat_summary(fun.data=mean_se, geom=\"errorbar\", position=\"dodge\")+\n        stat_summary(fun=mean, geom=\"col\", position=\"dodge\")+\n        theme_bw()+\n        theme(\n            text = element_text(size=14),\n          axis.text.x = element_text(size=14,angle=90, vjust=0.5, hjust=1),\n          axis.text.y = element_text(size=14),\n          axis.title.x = element_blank(),\n          plot.title = element_text(hjust = 0.5),\n          panel.grid = element_blank(),\n          legend.position = \"bottom\"\n        )+\n        ggtitle(\"nCD4 V-usage\")+\n        scale_fill_manual(values = colors_6_groups)\n</code></pre>"}]}